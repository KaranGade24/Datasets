{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.1457883770926953,
  "eval_steps": 500,
  "global_step": 285000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00010050766421193448,
      "grad_norm": 6.679025173187256,
      "learning_rate": 4.999834162354051e-05,
      "loss": 1.9356,
      "step": 100
    },
    {
      "epoch": 0.00020101532842386896,
      "grad_norm": 7.100730895996094,
      "learning_rate": 4.999666649580364e-05,
      "loss": 2.0136,
      "step": 200
    },
    {
      "epoch": 0.00030152299263580345,
      "grad_norm": 6.799045085906982,
      "learning_rate": 4.999499136806678e-05,
      "loss": 1.9474,
      "step": 300
    },
    {
      "epoch": 0.00040203065684773793,
      "grad_norm": 6.547858238220215,
      "learning_rate": 4.999331624032991e-05,
      "loss": 1.9154,
      "step": 400
    },
    {
      "epoch": 0.0005025383210596724,
      "grad_norm": 8.692085266113281,
      "learning_rate": 4.9991641112593043e-05,
      "loss": 1.8681,
      "step": 500
    },
    {
      "epoch": 0.0006030459852716069,
      "grad_norm": 7.229135990142822,
      "learning_rate": 4.998996598485618e-05,
      "loss": 1.9807,
      "step": 600
    },
    {
      "epoch": 0.0007035536494835414,
      "grad_norm": 7.319532871246338,
      "learning_rate": 4.998829085711931e-05,
      "loss": 1.916,
      "step": 700
    },
    {
      "epoch": 0.0008040613136954759,
      "grad_norm": 8.072954177856445,
      "learning_rate": 4.9986615729382444e-05,
      "loss": 1.9107,
      "step": 800
    },
    {
      "epoch": 0.0009045689779074103,
      "grad_norm": 4.841886043548584,
      "learning_rate": 4.998494060164558e-05,
      "loss": 1.965,
      "step": 900
    },
    {
      "epoch": 0.0010050766421193448,
      "grad_norm": 6.891130447387695,
      "learning_rate": 4.998326547390871e-05,
      "loss": 1.8562,
      "step": 1000
    },
    {
      "epoch": 0.0011055843063312793,
      "grad_norm": 6.640909194946289,
      "learning_rate": 4.9981590346171844e-05,
      "loss": 1.9073,
      "step": 1100
    },
    {
      "epoch": 0.0012060919705432138,
      "grad_norm": 7.564633369445801,
      "learning_rate": 4.9979915218434984e-05,
      "loss": 1.8435,
      "step": 1200
    },
    {
      "epoch": 0.0013065996347551483,
      "grad_norm": 11.860538482666016,
      "learning_rate": 4.997824009069812e-05,
      "loss": 1.965,
      "step": 1300
    },
    {
      "epoch": 0.0014071072989670828,
      "grad_norm": 6.825024604797363,
      "learning_rate": 4.997656496296125e-05,
      "loss": 1.9644,
      "step": 1400
    },
    {
      "epoch": 0.0015076149631790172,
      "grad_norm": 6.785588264465332,
      "learning_rate": 4.9974889835224384e-05,
      "loss": 1.7692,
      "step": 1500
    },
    {
      "epoch": 0.0016081226273909517,
      "grad_norm": 9.296548843383789,
      "learning_rate": 4.997321470748752e-05,
      "loss": 2.0364,
      "step": 1600
    },
    {
      "epoch": 0.0017086302916028862,
      "grad_norm": 8.525436401367188,
      "learning_rate": 4.997153957975065e-05,
      "loss": 1.9007,
      "step": 1700
    },
    {
      "epoch": 0.0018091379558148207,
      "grad_norm": 8.90234661102295,
      "learning_rate": 4.996986445201379e-05,
      "loss": 1.8744,
      "step": 1800
    },
    {
      "epoch": 0.0019096456200267552,
      "grad_norm": 2.9031546115875244,
      "learning_rate": 4.9968189324276925e-05,
      "loss": 1.8842,
      "step": 1900
    },
    {
      "epoch": 0.0020101532842386896,
      "grad_norm": 6.411301612854004,
      "learning_rate": 4.996651419654006e-05,
      "loss": 1.808,
      "step": 2000
    },
    {
      "epoch": 0.002110660948450624,
      "grad_norm": 5.378444671630859,
      "learning_rate": 4.996483906880319e-05,
      "loss": 1.8459,
      "step": 2100
    },
    {
      "epoch": 0.0022111686126625586,
      "grad_norm": 6.388241767883301,
      "learning_rate": 4.9963163941066325e-05,
      "loss": 1.8822,
      "step": 2200
    },
    {
      "epoch": 0.002311676276874493,
      "grad_norm": 7.366267204284668,
      "learning_rate": 4.996148881332946e-05,
      "loss": 2.0941,
      "step": 2300
    },
    {
      "epoch": 0.0024121839410864276,
      "grad_norm": 7.457080841064453,
      "learning_rate": 4.99598136855926e-05,
      "loss": 1.911,
      "step": 2400
    },
    {
      "epoch": 0.002512691605298362,
      "grad_norm": 8.185558319091797,
      "learning_rate": 4.995813855785573e-05,
      "loss": 1.96,
      "step": 2500
    },
    {
      "epoch": 0.0026131992695102965,
      "grad_norm": 17.301502227783203,
      "learning_rate": 4.9956463430118866e-05,
      "loss": 1.8861,
      "step": 2600
    },
    {
      "epoch": 0.002713706933722231,
      "grad_norm": 8.140386581420898,
      "learning_rate": 4.9954788302382e-05,
      "loss": 1.7922,
      "step": 2700
    },
    {
      "epoch": 0.0028142145979341655,
      "grad_norm": 7.738512992858887,
      "learning_rate": 4.995311317464513e-05,
      "loss": 1.7912,
      "step": 2800
    },
    {
      "epoch": 0.0029147222621460998,
      "grad_norm": 8.09174919128418,
      "learning_rate": 4.995143804690827e-05,
      "loss": 1.8614,
      "step": 2900
    },
    {
      "epoch": 0.0030152299263580345,
      "grad_norm": 6.653111934661865,
      "learning_rate": 4.9949762919171406e-05,
      "loss": 1.7911,
      "step": 3000
    },
    {
      "epoch": 0.0031157375905699687,
      "grad_norm": 5.75560998916626,
      "learning_rate": 4.994808779143454e-05,
      "loss": 1.9692,
      "step": 3100
    },
    {
      "epoch": 0.0032162452547819034,
      "grad_norm": 6.897823333740234,
      "learning_rate": 4.994641266369767e-05,
      "loss": 1.7702,
      "step": 3200
    },
    {
      "epoch": 0.0033167529189938377,
      "grad_norm": 6.91762113571167,
      "learning_rate": 4.9944737535960806e-05,
      "loss": 1.9675,
      "step": 3300
    },
    {
      "epoch": 0.0034172605832057724,
      "grad_norm": 6.931949138641357,
      "learning_rate": 4.994306240822394e-05,
      "loss": 1.8405,
      "step": 3400
    },
    {
      "epoch": 0.0035177682474177067,
      "grad_norm": 6.7602081298828125,
      "learning_rate": 4.994138728048708e-05,
      "loss": 1.8166,
      "step": 3500
    },
    {
      "epoch": 0.0036182759116296414,
      "grad_norm": 7.053390026092529,
      "learning_rate": 4.9939712152750214e-05,
      "loss": 1.795,
      "step": 3600
    },
    {
      "epoch": 0.0037187835758415756,
      "grad_norm": 6.521948337554932,
      "learning_rate": 4.993803702501335e-05,
      "loss": 1.8188,
      "step": 3700
    },
    {
      "epoch": 0.0038192912400535103,
      "grad_norm": 7.2544779777526855,
      "learning_rate": 4.993636189727648e-05,
      "loss": 1.8294,
      "step": 3800
    },
    {
      "epoch": 0.003919798904265445,
      "grad_norm": 7.098494529724121,
      "learning_rate": 4.9934686769539614e-05,
      "loss": 1.8604,
      "step": 3900
    },
    {
      "epoch": 0.004020306568477379,
      "grad_norm": 7.170236110687256,
      "learning_rate": 4.993301164180275e-05,
      "loss": 1.9047,
      "step": 4000
    },
    {
      "epoch": 0.0041208142326893136,
      "grad_norm": 5.667891502380371,
      "learning_rate": 4.993133651406589e-05,
      "loss": 1.9239,
      "step": 4100
    },
    {
      "epoch": 0.004221321896901248,
      "grad_norm": 8.017497062683105,
      "learning_rate": 4.992966138632902e-05,
      "loss": 1.8726,
      "step": 4200
    },
    {
      "epoch": 0.004321829561113183,
      "grad_norm": 8.006534576416016,
      "learning_rate": 4.9927986258592154e-05,
      "loss": 1.8718,
      "step": 4300
    },
    {
      "epoch": 0.004422337225325117,
      "grad_norm": 7.279261589050293,
      "learning_rate": 4.992631113085529e-05,
      "loss": 1.8064,
      "step": 4400
    },
    {
      "epoch": 0.0045228448895370515,
      "grad_norm": 6.3965840339660645,
      "learning_rate": 4.992463600311842e-05,
      "loss": 1.884,
      "step": 4500
    },
    {
      "epoch": 0.004623352553748986,
      "grad_norm": 12.207391738891602,
      "learning_rate": 4.9922960875381555e-05,
      "loss": 1.9493,
      "step": 4600
    },
    {
      "epoch": 0.004723860217960921,
      "grad_norm": 6.6144022941589355,
      "learning_rate": 4.9921285747644695e-05,
      "loss": 1.8302,
      "step": 4700
    },
    {
      "epoch": 0.004824367882172855,
      "grad_norm": 9.16118335723877,
      "learning_rate": 4.991961061990783e-05,
      "loss": 1.8878,
      "step": 4800
    },
    {
      "epoch": 0.004924875546384789,
      "grad_norm": 4.866172790527344,
      "learning_rate": 4.991793549217096e-05,
      "loss": 1.8359,
      "step": 4900
    },
    {
      "epoch": 0.005025383210596724,
      "grad_norm": 8.8894681930542,
      "learning_rate": 4.9916260364434095e-05,
      "loss": 1.8267,
      "step": 5000
    },
    {
      "epoch": 0.005125890874808659,
      "grad_norm": 7.74812650680542,
      "learning_rate": 4.991458523669723e-05,
      "loss": 1.8276,
      "step": 5100
    },
    {
      "epoch": 0.005226398539020593,
      "grad_norm": 4.848573207855225,
      "learning_rate": 4.991291010896036e-05,
      "loss": 1.8554,
      "step": 5200
    },
    {
      "epoch": 0.005326906203232527,
      "grad_norm": 7.241760730743408,
      "learning_rate": 4.9911234981223495e-05,
      "loss": 1.8746,
      "step": 5300
    },
    {
      "epoch": 0.005427413867444462,
      "grad_norm": 8.01929759979248,
      "learning_rate": 4.990955985348663e-05,
      "loss": 1.8225,
      "step": 5400
    },
    {
      "epoch": 0.005527921531656397,
      "grad_norm": 8.291467666625977,
      "learning_rate": 4.990788472574976e-05,
      "loss": 1.9261,
      "step": 5500
    },
    {
      "epoch": 0.005628429195868331,
      "grad_norm": 4.760439395904541,
      "learning_rate": 4.9906209598012896e-05,
      "loss": 1.9647,
      "step": 5600
    },
    {
      "epoch": 0.005728936860080265,
      "grad_norm": 6.737001895904541,
      "learning_rate": 4.990453447027603e-05,
      "loss": 1.8696,
      "step": 5700
    },
    {
      "epoch": 0.0058294445242921995,
      "grad_norm": 7.853945255279541,
      "learning_rate": 4.990285934253916e-05,
      "loss": 2.0007,
      "step": 5800
    },
    {
      "epoch": 0.005929952188504135,
      "grad_norm": 10.352252960205078,
      "learning_rate": 4.99011842148023e-05,
      "loss": 1.8785,
      "step": 5900
    },
    {
      "epoch": 0.006030459852716069,
      "grad_norm": 8.041518211364746,
      "learning_rate": 4.9899509087065436e-05,
      "loss": 1.8524,
      "step": 6000
    },
    {
      "epoch": 0.006130967516928003,
      "grad_norm": 8.193772315979004,
      "learning_rate": 4.989783395932857e-05,
      "loss": 1.8748,
      "step": 6100
    },
    {
      "epoch": 0.0062314751811399375,
      "grad_norm": 8.650490760803223,
      "learning_rate": 4.98961588315917e-05,
      "loss": 1.7917,
      "step": 6200
    },
    {
      "epoch": 0.006331982845351873,
      "grad_norm": 7.809291362762451,
      "learning_rate": 4.9894483703854836e-05,
      "loss": 1.9582,
      "step": 6300
    },
    {
      "epoch": 0.006432490509563807,
      "grad_norm": 7.28292179107666,
      "learning_rate": 4.989280857611797e-05,
      "loss": 1.8512,
      "step": 6400
    },
    {
      "epoch": 0.006532998173775741,
      "grad_norm": 6.747108459472656,
      "learning_rate": 4.989113344838111e-05,
      "loss": 1.933,
      "step": 6500
    },
    {
      "epoch": 0.006633505837987675,
      "grad_norm": 8.285090446472168,
      "learning_rate": 4.988945832064424e-05,
      "loss": 1.745,
      "step": 6600
    },
    {
      "epoch": 0.0067340135021996105,
      "grad_norm": 4.918050765991211,
      "learning_rate": 4.988778319290738e-05,
      "loss": 1.9096,
      "step": 6700
    },
    {
      "epoch": 0.006834521166411545,
      "grad_norm": 7.084717273712158,
      "learning_rate": 4.988610806517051e-05,
      "loss": 1.9321,
      "step": 6800
    },
    {
      "epoch": 0.006935028830623479,
      "grad_norm": 8.562551498413086,
      "learning_rate": 4.9884432937433644e-05,
      "loss": 1.8742,
      "step": 6900
    },
    {
      "epoch": 0.007035536494835413,
      "grad_norm": 8.281970977783203,
      "learning_rate": 4.988275780969678e-05,
      "loss": 1.8145,
      "step": 7000
    },
    {
      "epoch": 0.0071360441590473484,
      "grad_norm": 3.210388660430908,
      "learning_rate": 4.988108268195992e-05,
      "loss": 1.8641,
      "step": 7100
    },
    {
      "epoch": 0.007236551823259283,
      "grad_norm": 9.555294036865234,
      "learning_rate": 4.987940755422305e-05,
      "loss": 1.7439,
      "step": 7200
    },
    {
      "epoch": 0.007337059487471217,
      "grad_norm": 5.9329657554626465,
      "learning_rate": 4.9877732426486184e-05,
      "loss": 1.8979,
      "step": 7300
    },
    {
      "epoch": 0.007437567151683151,
      "grad_norm": 6.596440315246582,
      "learning_rate": 4.987605729874932e-05,
      "loss": 1.9589,
      "step": 7400
    },
    {
      "epoch": 0.007538074815895086,
      "grad_norm": 8.27810001373291,
      "learning_rate": 4.987438217101245e-05,
      "loss": 1.924,
      "step": 7500
    },
    {
      "epoch": 0.007638582480107021,
      "grad_norm": 8.820457458496094,
      "learning_rate": 4.9872707043275584e-05,
      "loss": 1.7103,
      "step": 7600
    },
    {
      "epoch": 0.007739090144318955,
      "grad_norm": 7.437747001647949,
      "learning_rate": 4.9871031915538725e-05,
      "loss": 1.8826,
      "step": 7700
    },
    {
      "epoch": 0.00783959780853089,
      "grad_norm": 8.27220344543457,
      "learning_rate": 4.986935678780186e-05,
      "loss": 1.9757,
      "step": 7800
    },
    {
      "epoch": 0.007940105472742823,
      "grad_norm": 7.25707483291626,
      "learning_rate": 4.986768166006499e-05,
      "loss": 1.8781,
      "step": 7900
    },
    {
      "epoch": 0.008040613136954759,
      "grad_norm": 7.802600383758545,
      "learning_rate": 4.9866006532328125e-05,
      "loss": 1.7826,
      "step": 8000
    },
    {
      "epoch": 0.008141120801166694,
      "grad_norm": 7.0692596435546875,
      "learning_rate": 4.986433140459126e-05,
      "loss": 1.9054,
      "step": 8100
    },
    {
      "epoch": 0.008241628465378627,
      "grad_norm": 8.309375762939453,
      "learning_rate": 4.986265627685439e-05,
      "loss": 1.8143,
      "step": 8200
    },
    {
      "epoch": 0.008342136129590562,
      "grad_norm": 8.172216415405273,
      "learning_rate": 4.986098114911753e-05,
      "loss": 1.8432,
      "step": 8300
    },
    {
      "epoch": 0.008442643793802496,
      "grad_norm": 6.65649938583374,
      "learning_rate": 4.9859306021380665e-05,
      "loss": 1.831,
      "step": 8400
    },
    {
      "epoch": 0.00854315145801443,
      "grad_norm": 6.85432767868042,
      "learning_rate": 4.98576308936438e-05,
      "loss": 1.8036,
      "step": 8500
    },
    {
      "epoch": 0.008643659122226366,
      "grad_norm": 6.884612083435059,
      "learning_rate": 4.985595576590693e-05,
      "loss": 1.8337,
      "step": 8600
    },
    {
      "epoch": 0.0087441667864383,
      "grad_norm": 7.930637836456299,
      "learning_rate": 4.9854280638170066e-05,
      "loss": 1.8504,
      "step": 8700
    },
    {
      "epoch": 0.008844674450650234,
      "grad_norm": 6.058685779571533,
      "learning_rate": 4.98526055104332e-05,
      "loss": 1.7705,
      "step": 8800
    },
    {
      "epoch": 0.00894518211486217,
      "grad_norm": 6.698023319244385,
      "learning_rate": 4.985093038269634e-05,
      "loss": 1.8403,
      "step": 8900
    },
    {
      "epoch": 0.009045689779074103,
      "grad_norm": 6.607020378112793,
      "learning_rate": 4.984925525495947e-05,
      "loss": 1.8667,
      "step": 9000
    },
    {
      "epoch": 0.009146197443286038,
      "grad_norm": 7.3931474685668945,
      "learning_rate": 4.9847580127222606e-05,
      "loss": 1.835,
      "step": 9100
    },
    {
      "epoch": 0.009246705107497971,
      "grad_norm": 7.683986663818359,
      "learning_rate": 4.984590499948574e-05,
      "loss": 1.8368,
      "step": 9200
    },
    {
      "epoch": 0.009347212771709907,
      "grad_norm": 7.188480854034424,
      "learning_rate": 4.984422987174887e-05,
      "loss": 1.8688,
      "step": 9300
    },
    {
      "epoch": 0.009447720435921842,
      "grad_norm": 6.515765190124512,
      "learning_rate": 4.9842554744012006e-05,
      "loss": 1.7098,
      "step": 9400
    },
    {
      "epoch": 0.009548228100133775,
      "grad_norm": 5.2759575843811035,
      "learning_rate": 4.9840879616275147e-05,
      "loss": 1.9531,
      "step": 9500
    },
    {
      "epoch": 0.00964873576434571,
      "grad_norm": 8.065885543823242,
      "learning_rate": 4.983920448853828e-05,
      "loss": 1.9747,
      "step": 9600
    },
    {
      "epoch": 0.009749243428557645,
      "grad_norm": 7.004158020019531,
      "learning_rate": 4.983752936080141e-05,
      "loss": 1.8586,
      "step": 9700
    },
    {
      "epoch": 0.009849751092769579,
      "grad_norm": 9.595636367797852,
      "learning_rate": 4.983585423306455e-05,
      "loss": 1.8234,
      "step": 9800
    },
    {
      "epoch": 0.009950258756981514,
      "grad_norm": 9.480986595153809,
      "learning_rate": 4.983417910532768e-05,
      "loss": 1.7864,
      "step": 9900
    },
    {
      "epoch": 0.010050766421193447,
      "grad_norm": 5.474135875701904,
      "learning_rate": 4.9832503977590814e-05,
      "loss": 1.8108,
      "step": 10000
    },
    {
      "epoch": 0.010151274085405382,
      "grad_norm": 7.295693874359131,
      "learning_rate": 4.983082884985395e-05,
      "loss": 1.8331,
      "step": 10100
    },
    {
      "epoch": 0.010251781749617318,
      "grad_norm": 7.920724391937256,
      "learning_rate": 4.982915372211708e-05,
      "loss": 1.8951,
      "step": 10200
    },
    {
      "epoch": 0.010352289413829251,
      "grad_norm": 6.282107353210449,
      "learning_rate": 4.9827478594380214e-05,
      "loss": 1.7879,
      "step": 10300
    },
    {
      "epoch": 0.010452797078041186,
      "grad_norm": 7.005638122558594,
      "learning_rate": 4.982580346664335e-05,
      "loss": 1.8536,
      "step": 10400
    },
    {
      "epoch": 0.010553304742253121,
      "grad_norm": 6.358765125274658,
      "learning_rate": 4.982412833890648e-05,
      "loss": 1.8641,
      "step": 10500
    },
    {
      "epoch": 0.010653812406465055,
      "grad_norm": 10.259896278381348,
      "learning_rate": 4.9822453211169614e-05,
      "loss": 1.8434,
      "step": 10600
    },
    {
      "epoch": 0.01075432007067699,
      "grad_norm": 8.820540428161621,
      "learning_rate": 4.9820778083432754e-05,
      "loss": 1.8599,
      "step": 10700
    },
    {
      "epoch": 0.010854827734888923,
      "grad_norm": 8.679121017456055,
      "learning_rate": 4.981910295569589e-05,
      "loss": 1.8491,
      "step": 10800
    },
    {
      "epoch": 0.010955335399100858,
      "grad_norm": 6.164820671081543,
      "learning_rate": 4.981742782795902e-05,
      "loss": 1.9798,
      "step": 10900
    },
    {
      "epoch": 0.011055843063312793,
      "grad_norm": 6.951307773590088,
      "learning_rate": 4.9815752700222155e-05,
      "loss": 1.8895,
      "step": 11000
    },
    {
      "epoch": 0.011156350727524727,
      "grad_norm": 6.913232326507568,
      "learning_rate": 4.981407757248529e-05,
      "loss": 1.9178,
      "step": 11100
    },
    {
      "epoch": 0.011256858391736662,
      "grad_norm": 7.609964847564697,
      "learning_rate": 4.981240244474843e-05,
      "loss": 1.8498,
      "step": 11200
    },
    {
      "epoch": 0.011357366055948597,
      "grad_norm": 6.26238489151001,
      "learning_rate": 4.981072731701156e-05,
      "loss": 1.6769,
      "step": 11300
    },
    {
      "epoch": 0.01145787372016053,
      "grad_norm": 5.542474746704102,
      "learning_rate": 4.9809052189274695e-05,
      "loss": 1.7382,
      "step": 11400
    },
    {
      "epoch": 0.011558381384372466,
      "grad_norm": 8.614225387573242,
      "learning_rate": 4.980737706153783e-05,
      "loss": 1.8664,
      "step": 11500
    },
    {
      "epoch": 0.011658889048584399,
      "grad_norm": 8.1668062210083,
      "learning_rate": 4.980570193380096e-05,
      "loss": 1.8713,
      "step": 11600
    },
    {
      "epoch": 0.011759396712796334,
      "grad_norm": 8.627702713012695,
      "learning_rate": 4.9804026806064095e-05,
      "loss": 1.8601,
      "step": 11700
    },
    {
      "epoch": 0.01185990437700827,
      "grad_norm": 4.918568134307861,
      "learning_rate": 4.9802351678327236e-05,
      "loss": 1.8713,
      "step": 11800
    },
    {
      "epoch": 0.011960412041220203,
      "grad_norm": 6.90708589553833,
      "learning_rate": 4.980067655059037e-05,
      "loss": 1.9399,
      "step": 11900
    },
    {
      "epoch": 0.012060919705432138,
      "grad_norm": 6.8326592445373535,
      "learning_rate": 4.97990014228535e-05,
      "loss": 1.824,
      "step": 12000
    },
    {
      "epoch": 0.012161427369644073,
      "grad_norm": 8.778961181640625,
      "learning_rate": 4.9797326295116636e-05,
      "loss": 1.7253,
      "step": 12100
    },
    {
      "epoch": 0.012261935033856006,
      "grad_norm": 8.501457214355469,
      "learning_rate": 4.979565116737977e-05,
      "loss": 1.8529,
      "step": 12200
    },
    {
      "epoch": 0.012362442698067942,
      "grad_norm": 4.140161037445068,
      "learning_rate": 4.97939760396429e-05,
      "loss": 1.8609,
      "step": 12300
    },
    {
      "epoch": 0.012462950362279875,
      "grad_norm": 7.4711151123046875,
      "learning_rate": 4.979230091190604e-05,
      "loss": 1.8562,
      "step": 12400
    },
    {
      "epoch": 0.01256345802649181,
      "grad_norm": 7.103501319885254,
      "learning_rate": 4.9790625784169176e-05,
      "loss": 1.7923,
      "step": 12500
    },
    {
      "epoch": 0.012663965690703745,
      "grad_norm": 7.536041259765625,
      "learning_rate": 4.978895065643231e-05,
      "loss": 1.7908,
      "step": 12600
    },
    {
      "epoch": 0.012764473354915679,
      "grad_norm": 8.711429595947266,
      "learning_rate": 4.978727552869544e-05,
      "loss": 1.8165,
      "step": 12700
    },
    {
      "epoch": 0.012864981019127614,
      "grad_norm": 10.98749828338623,
      "learning_rate": 4.978560040095858e-05,
      "loss": 1.9125,
      "step": 12800
    },
    {
      "epoch": 0.012965488683339549,
      "grad_norm": 5.373288631439209,
      "learning_rate": 4.978392527322171e-05,
      "loss": 1.8056,
      "step": 12900
    },
    {
      "epoch": 0.013065996347551482,
      "grad_norm": 8.687484741210938,
      "learning_rate": 4.978225014548485e-05,
      "loss": 1.8779,
      "step": 13000
    },
    {
      "epoch": 0.013166504011763417,
      "grad_norm": 7.089080333709717,
      "learning_rate": 4.9780575017747984e-05,
      "loss": 1.7969,
      "step": 13100
    },
    {
      "epoch": 0.01326701167597535,
      "grad_norm": 7.712758541107178,
      "learning_rate": 4.977889989001112e-05,
      "loss": 1.7977,
      "step": 13200
    },
    {
      "epoch": 0.013367519340187286,
      "grad_norm": 7.137664318084717,
      "learning_rate": 4.977722476227425e-05,
      "loss": 1.7303,
      "step": 13300
    },
    {
      "epoch": 0.013468027004399221,
      "grad_norm": 10.085108757019043,
      "learning_rate": 4.9775549634537384e-05,
      "loss": 1.9163,
      "step": 13400
    },
    {
      "epoch": 0.013568534668611154,
      "grad_norm": 6.698648452758789,
      "learning_rate": 4.977387450680052e-05,
      "loss": 1.8981,
      "step": 13500
    },
    {
      "epoch": 0.01366904233282309,
      "grad_norm": 6.642712593078613,
      "learning_rate": 4.977219937906366e-05,
      "loss": 1.8377,
      "step": 13600
    },
    {
      "epoch": 0.013769549997035025,
      "grad_norm": 6.806046485900879,
      "learning_rate": 4.977052425132679e-05,
      "loss": 1.8493,
      "step": 13700
    },
    {
      "epoch": 0.013870057661246958,
      "grad_norm": 6.704980850219727,
      "learning_rate": 4.9768849123589924e-05,
      "loss": 1.8772,
      "step": 13800
    },
    {
      "epoch": 0.013970565325458893,
      "grad_norm": 7.483757019042969,
      "learning_rate": 4.976717399585306e-05,
      "loss": 1.8334,
      "step": 13900
    },
    {
      "epoch": 0.014071072989670827,
      "grad_norm": 7.853562831878662,
      "learning_rate": 4.976549886811619e-05,
      "loss": 1.8794,
      "step": 14000
    },
    {
      "epoch": 0.014171580653882762,
      "grad_norm": 9.270325660705566,
      "learning_rate": 4.9763823740379325e-05,
      "loss": 1.7505,
      "step": 14100
    },
    {
      "epoch": 0.014272088318094697,
      "grad_norm": 6.550917148590088,
      "learning_rate": 4.9762148612642465e-05,
      "loss": 1.8481,
      "step": 14200
    },
    {
      "epoch": 0.01437259598230663,
      "grad_norm": 8.141030311584473,
      "learning_rate": 4.97604734849056e-05,
      "loss": 1.8437,
      "step": 14300
    },
    {
      "epoch": 0.014473103646518565,
      "grad_norm": 5.936029434204102,
      "learning_rate": 4.975879835716873e-05,
      "loss": 1.8092,
      "step": 14400
    },
    {
      "epoch": 0.0145736113107305,
      "grad_norm": 8.89894962310791,
      "learning_rate": 4.9757123229431865e-05,
      "loss": 1.7905,
      "step": 14500
    },
    {
      "epoch": 0.014674118974942434,
      "grad_norm": 10.257851600646973,
      "learning_rate": 4.9755448101695e-05,
      "loss": 1.7464,
      "step": 14600
    },
    {
      "epoch": 0.014774626639154369,
      "grad_norm": 8.606995582580566,
      "learning_rate": 4.975377297395813e-05,
      "loss": 1.7638,
      "step": 14700
    },
    {
      "epoch": 0.014875134303366302,
      "grad_norm": 8.021401405334473,
      "learning_rate": 4.9752097846221265e-05,
      "loss": 1.7973,
      "step": 14800
    },
    {
      "epoch": 0.014975641967578238,
      "grad_norm": 10.140002250671387,
      "learning_rate": 4.97504227184844e-05,
      "loss": 1.7506,
      "step": 14900
    },
    {
      "epoch": 0.015076149631790173,
      "grad_norm": 5.021138668060303,
      "learning_rate": 4.974874759074753e-05,
      "loss": 1.8367,
      "step": 15000
    },
    {
      "epoch": 0.015176657296002106,
      "grad_norm": 7.195178031921387,
      "learning_rate": 4.9747072463010666e-05,
      "loss": 1.7732,
      "step": 15100
    },
    {
      "epoch": 0.015277164960214041,
      "grad_norm": 6.9500732421875,
      "learning_rate": 4.97453973352738e-05,
      "loss": 1.8173,
      "step": 15200
    },
    {
      "epoch": 0.015377672624425976,
      "grad_norm": 8.376429557800293,
      "learning_rate": 4.974372220753693e-05,
      "loss": 1.8373,
      "step": 15300
    },
    {
      "epoch": 0.01547818028863791,
      "grad_norm": 2.7073071002960205,
      "learning_rate": 4.974204707980007e-05,
      "loss": 1.8754,
      "step": 15400
    },
    {
      "epoch": 0.015578687952849845,
      "grad_norm": 6.8460493087768555,
      "learning_rate": 4.9740371952063206e-05,
      "loss": 1.8234,
      "step": 15500
    },
    {
      "epoch": 0.01567919561706178,
      "grad_norm": 7.1140031814575195,
      "learning_rate": 4.973869682432634e-05,
      "loss": 1.8985,
      "step": 15600
    },
    {
      "epoch": 0.015779703281273715,
      "grad_norm": 7.5407395362854,
      "learning_rate": 4.973702169658947e-05,
      "loss": 1.8095,
      "step": 15700
    },
    {
      "epoch": 0.015880210945485647,
      "grad_norm": 8.916156768798828,
      "learning_rate": 4.9735346568852606e-05,
      "loss": 1.7923,
      "step": 15800
    },
    {
      "epoch": 0.015980718609697582,
      "grad_norm": 6.819804668426514,
      "learning_rate": 4.973367144111574e-05,
      "loss": 1.8001,
      "step": 15900
    },
    {
      "epoch": 0.016081226273909517,
      "grad_norm": 7.559659957885742,
      "learning_rate": 4.973199631337888e-05,
      "loss": 1.7862,
      "step": 16000
    },
    {
      "epoch": 0.016181733938121452,
      "grad_norm": 10.545552253723145,
      "learning_rate": 4.9730321185642014e-05,
      "loss": 1.7589,
      "step": 16100
    },
    {
      "epoch": 0.016282241602333387,
      "grad_norm": 6.812541484832764,
      "learning_rate": 4.972864605790515e-05,
      "loss": 1.7316,
      "step": 16200
    },
    {
      "epoch": 0.01638274926654532,
      "grad_norm": 6.009311676025391,
      "learning_rate": 4.972697093016828e-05,
      "loss": 1.8152,
      "step": 16300
    },
    {
      "epoch": 0.016483256930757254,
      "grad_norm": 6.255923271179199,
      "learning_rate": 4.9725295802431414e-05,
      "loss": 1.8139,
      "step": 16400
    },
    {
      "epoch": 0.01658376459496919,
      "grad_norm": 7.741255283355713,
      "learning_rate": 4.972362067469455e-05,
      "loss": 1.8908,
      "step": 16500
    },
    {
      "epoch": 0.016684272259181124,
      "grad_norm": 6.140219211578369,
      "learning_rate": 4.972194554695769e-05,
      "loss": 1.8858,
      "step": 16600
    },
    {
      "epoch": 0.01678477992339306,
      "grad_norm": 8.036836624145508,
      "learning_rate": 4.972027041922082e-05,
      "loss": 1.8056,
      "step": 16700
    },
    {
      "epoch": 0.01688528758760499,
      "grad_norm": 9.740246772766113,
      "learning_rate": 4.9718595291483954e-05,
      "loss": 1.8729,
      "step": 16800
    },
    {
      "epoch": 0.016985795251816926,
      "grad_norm": 8.585769653320312,
      "learning_rate": 4.971692016374709e-05,
      "loss": 1.8569,
      "step": 16900
    },
    {
      "epoch": 0.01708630291602886,
      "grad_norm": 4.550976276397705,
      "learning_rate": 4.971524503601022e-05,
      "loss": 1.8238,
      "step": 17000
    },
    {
      "epoch": 0.017186810580240797,
      "grad_norm": 8.169686317443848,
      "learning_rate": 4.9713569908273355e-05,
      "loss": 1.7013,
      "step": 17100
    },
    {
      "epoch": 0.017287318244452732,
      "grad_norm": 8.148822784423828,
      "learning_rate": 4.9711894780536495e-05,
      "loss": 1.8472,
      "step": 17200
    },
    {
      "epoch": 0.017387825908664667,
      "grad_norm": 6.951896667480469,
      "learning_rate": 4.971021965279963e-05,
      "loss": 1.8432,
      "step": 17300
    },
    {
      "epoch": 0.0174883335728766,
      "grad_norm": 6.640771389007568,
      "learning_rate": 4.970854452506276e-05,
      "loss": 1.8616,
      "step": 17400
    },
    {
      "epoch": 0.017588841237088534,
      "grad_norm": 6.860267162322998,
      "learning_rate": 4.9706869397325895e-05,
      "loss": 1.883,
      "step": 17500
    },
    {
      "epoch": 0.01768934890130047,
      "grad_norm": 7.351163387298584,
      "learning_rate": 4.970519426958903e-05,
      "loss": 1.7739,
      "step": 17600
    },
    {
      "epoch": 0.017789856565512404,
      "grad_norm": 7.3687543869018555,
      "learning_rate": 4.970351914185216e-05,
      "loss": 1.7981,
      "step": 17700
    },
    {
      "epoch": 0.01789036422972434,
      "grad_norm": 8.797514915466309,
      "learning_rate": 4.97018440141153e-05,
      "loss": 1.8406,
      "step": 17800
    },
    {
      "epoch": 0.01799087189393627,
      "grad_norm": 7.069625377655029,
      "learning_rate": 4.9700168886378435e-05,
      "loss": 1.7952,
      "step": 17900
    },
    {
      "epoch": 0.018091379558148206,
      "grad_norm": 9.298136711120605,
      "learning_rate": 4.969849375864157e-05,
      "loss": 1.7654,
      "step": 18000
    },
    {
      "epoch": 0.01819188722236014,
      "grad_norm": 6.468545436859131,
      "learning_rate": 4.96968186309047e-05,
      "loss": 1.828,
      "step": 18100
    },
    {
      "epoch": 0.018292394886572076,
      "grad_norm": 5.228848457336426,
      "learning_rate": 4.9695143503167836e-05,
      "loss": 1.7233,
      "step": 18200
    },
    {
      "epoch": 0.01839290255078401,
      "grad_norm": 4.904422760009766,
      "learning_rate": 4.969346837543097e-05,
      "loss": 1.8124,
      "step": 18300
    },
    {
      "epoch": 0.018493410214995943,
      "grad_norm": 7.415933132171631,
      "learning_rate": 4.969179324769411e-05,
      "loss": 1.8906,
      "step": 18400
    },
    {
      "epoch": 0.018593917879207878,
      "grad_norm": 9.07209300994873,
      "learning_rate": 4.969011811995724e-05,
      "loss": 1.7576,
      "step": 18500
    },
    {
      "epoch": 0.018694425543419813,
      "grad_norm": 8.288297653198242,
      "learning_rate": 4.9688442992220376e-05,
      "loss": 1.7835,
      "step": 18600
    },
    {
      "epoch": 0.01879493320763175,
      "grad_norm": 6.852365970611572,
      "learning_rate": 4.968676786448351e-05,
      "loss": 1.7945,
      "step": 18700
    },
    {
      "epoch": 0.018895440871843684,
      "grad_norm": 8.405649185180664,
      "learning_rate": 4.968509273674664e-05,
      "loss": 1.8106,
      "step": 18800
    },
    {
      "epoch": 0.01899594853605562,
      "grad_norm": 8.621320724487305,
      "learning_rate": 4.9683417609009776e-05,
      "loss": 1.9128,
      "step": 18900
    },
    {
      "epoch": 0.01909645620026755,
      "grad_norm": 5.364117622375488,
      "learning_rate": 4.968174248127292e-05,
      "loss": 1.8174,
      "step": 19000
    },
    {
      "epoch": 0.019196963864479485,
      "grad_norm": 8.828357696533203,
      "learning_rate": 4.968006735353605e-05,
      "loss": 1.8215,
      "step": 19100
    },
    {
      "epoch": 0.01929747152869142,
      "grad_norm": 9.503390312194824,
      "learning_rate": 4.9678392225799184e-05,
      "loss": 1.8108,
      "step": 19200
    },
    {
      "epoch": 0.019397979192903356,
      "grad_norm": 7.5289225578308105,
      "learning_rate": 4.967671709806232e-05,
      "loss": 1.7222,
      "step": 19300
    },
    {
      "epoch": 0.01949848685711529,
      "grad_norm": 8.235239028930664,
      "learning_rate": 4.967504197032545e-05,
      "loss": 1.7808,
      "step": 19400
    },
    {
      "epoch": 0.019598994521327223,
      "grad_norm": 8.98350715637207,
      "learning_rate": 4.9673366842588584e-05,
      "loss": 1.6389,
      "step": 19500
    },
    {
      "epoch": 0.019699502185539158,
      "grad_norm": 5.780063629150391,
      "learning_rate": 4.967169171485172e-05,
      "loss": 1.7446,
      "step": 19600
    },
    {
      "epoch": 0.019800009849751093,
      "grad_norm": 8.591300010681152,
      "learning_rate": 4.967001658711485e-05,
      "loss": 1.8235,
      "step": 19700
    },
    {
      "epoch": 0.019900517513963028,
      "grad_norm": 4.516369342803955,
      "learning_rate": 4.9668341459377984e-05,
      "loss": 1.7737,
      "step": 19800
    },
    {
      "epoch": 0.020001025178174963,
      "grad_norm": 7.616820335388184,
      "learning_rate": 4.966666633164112e-05,
      "loss": 1.9084,
      "step": 19900
    },
    {
      "epoch": 0.020101532842386895,
      "grad_norm": 7.432015419006348,
      "learning_rate": 4.966499120390425e-05,
      "loss": 1.8695,
      "step": 20000
    },
    {
      "epoch": 0.02020204050659883,
      "grad_norm": 4.34953498840332,
      "learning_rate": 4.966331607616739e-05,
      "loss": 1.7997,
      "step": 20100
    },
    {
      "epoch": 0.020302548170810765,
      "grad_norm": 7.817002773284912,
      "learning_rate": 4.9661640948430525e-05,
      "loss": 1.8334,
      "step": 20200
    },
    {
      "epoch": 0.0204030558350227,
      "grad_norm": 6.517098426818848,
      "learning_rate": 4.965996582069366e-05,
      "loss": 1.7076,
      "step": 20300
    },
    {
      "epoch": 0.020503563499234635,
      "grad_norm": 7.324019432067871,
      "learning_rate": 4.965829069295679e-05,
      "loss": 1.8411,
      "step": 20400
    },
    {
      "epoch": 0.020604071163446567,
      "grad_norm": 8.31684684753418,
      "learning_rate": 4.9656615565219925e-05,
      "loss": 1.7345,
      "step": 20500
    },
    {
      "epoch": 0.020704578827658502,
      "grad_norm": 6.761144638061523,
      "learning_rate": 4.965494043748306e-05,
      "loss": 1.8348,
      "step": 20600
    },
    {
      "epoch": 0.020805086491870437,
      "grad_norm": 6.220673561096191,
      "learning_rate": 4.96532653097462e-05,
      "loss": 1.7909,
      "step": 20700
    },
    {
      "epoch": 0.020905594156082372,
      "grad_norm": 5.119847297668457,
      "learning_rate": 4.965159018200933e-05,
      "loss": 1.8014,
      "step": 20800
    },
    {
      "epoch": 0.021006101820294307,
      "grad_norm": 9.149740219116211,
      "learning_rate": 4.9649915054272465e-05,
      "loss": 1.7488,
      "step": 20900
    },
    {
      "epoch": 0.021106609484506243,
      "grad_norm": 7.570335388183594,
      "learning_rate": 4.96482399265356e-05,
      "loss": 1.8427,
      "step": 21000
    },
    {
      "epoch": 0.021207117148718174,
      "grad_norm": 4.362758159637451,
      "learning_rate": 4.964656479879873e-05,
      "loss": 1.7053,
      "step": 21100
    },
    {
      "epoch": 0.02130762481293011,
      "grad_norm": 8.27115249633789,
      "learning_rate": 4.9644889671061866e-05,
      "loss": 1.9117,
      "step": 21200
    },
    {
      "epoch": 0.021408132477142044,
      "grad_norm": 6.500860691070557,
      "learning_rate": 4.9643214543325006e-05,
      "loss": 1.8644,
      "step": 21300
    },
    {
      "epoch": 0.02150864014135398,
      "grad_norm": 6.348081111907959,
      "learning_rate": 4.964153941558814e-05,
      "loss": 1.7995,
      "step": 21400
    },
    {
      "epoch": 0.021609147805565915,
      "grad_norm": 9.94438648223877,
      "learning_rate": 4.963986428785127e-05,
      "loss": 1.8171,
      "step": 21500
    },
    {
      "epoch": 0.021709655469777846,
      "grad_norm": 6.624969959259033,
      "learning_rate": 4.9638189160114406e-05,
      "loss": 1.7907,
      "step": 21600
    },
    {
      "epoch": 0.02181016313398978,
      "grad_norm": 7.388166904449463,
      "learning_rate": 4.963651403237754e-05,
      "loss": 1.7709,
      "step": 21700
    },
    {
      "epoch": 0.021910670798201717,
      "grad_norm": 5.4352030754089355,
      "learning_rate": 4.963483890464067e-05,
      "loss": 1.7399,
      "step": 21800
    },
    {
      "epoch": 0.022011178462413652,
      "grad_norm": 7.2197980880737305,
      "learning_rate": 4.963316377690381e-05,
      "loss": 1.7897,
      "step": 21900
    },
    {
      "epoch": 0.022111686126625587,
      "grad_norm": 8.021401405334473,
      "learning_rate": 4.9631488649166947e-05,
      "loss": 1.8791,
      "step": 22000
    },
    {
      "epoch": 0.02221219379083752,
      "grad_norm": 8.953442573547363,
      "learning_rate": 4.962981352143008e-05,
      "loss": 1.7975,
      "step": 22100
    },
    {
      "epoch": 0.022312701455049454,
      "grad_norm": 6.989100933074951,
      "learning_rate": 4.962813839369321e-05,
      "loss": 1.8841,
      "step": 22200
    },
    {
      "epoch": 0.02241320911926139,
      "grad_norm": 7.5000433921813965,
      "learning_rate": 4.962646326595635e-05,
      "loss": 1.7665,
      "step": 22300
    },
    {
      "epoch": 0.022513716783473324,
      "grad_norm": 7.078826904296875,
      "learning_rate": 4.962478813821948e-05,
      "loss": 1.7443,
      "step": 22400
    },
    {
      "epoch": 0.02261422444768526,
      "grad_norm": 7.278623104095459,
      "learning_rate": 4.962311301048262e-05,
      "loss": 1.8013,
      "step": 22500
    },
    {
      "epoch": 0.022714732111897194,
      "grad_norm": 7.987424850463867,
      "learning_rate": 4.9621437882745754e-05,
      "loss": 1.7833,
      "step": 22600
    },
    {
      "epoch": 0.022815239776109126,
      "grad_norm": 5.112180233001709,
      "learning_rate": 4.961976275500889e-05,
      "loss": 1.8672,
      "step": 22700
    },
    {
      "epoch": 0.02291574744032106,
      "grad_norm": 8.003792762756348,
      "learning_rate": 4.961808762727202e-05,
      "loss": 1.7984,
      "step": 22800
    },
    {
      "epoch": 0.023016255104532996,
      "grad_norm": 6.422151565551758,
      "learning_rate": 4.9616412499535154e-05,
      "loss": 1.9612,
      "step": 22900
    },
    {
      "epoch": 0.02311676276874493,
      "grad_norm": 10.855812072753906,
      "learning_rate": 4.961473737179829e-05,
      "loss": 1.8548,
      "step": 23000
    },
    {
      "epoch": 0.023217270432956866,
      "grad_norm": 7.109619617462158,
      "learning_rate": 4.961306224406143e-05,
      "loss": 1.7206,
      "step": 23100
    },
    {
      "epoch": 0.023317778097168798,
      "grad_norm": 4.2836079597473145,
      "learning_rate": 4.961138711632456e-05,
      "loss": 1.7834,
      "step": 23200
    },
    {
      "epoch": 0.023418285761380733,
      "grad_norm": 3.7010085582733154,
      "learning_rate": 4.9609711988587695e-05,
      "loss": 1.8293,
      "step": 23300
    },
    {
      "epoch": 0.02351879342559267,
      "grad_norm": 8.149353981018066,
      "learning_rate": 4.960803686085083e-05,
      "loss": 1.7784,
      "step": 23400
    },
    {
      "epoch": 0.023619301089804604,
      "grad_norm": 6.09313440322876,
      "learning_rate": 4.960636173311396e-05,
      "loss": 1.8061,
      "step": 23500
    },
    {
      "epoch": 0.02371980875401654,
      "grad_norm": 5.569558620452881,
      "learning_rate": 4.9604686605377095e-05,
      "loss": 1.7454,
      "step": 23600
    },
    {
      "epoch": 0.02382031641822847,
      "grad_norm": 2.596764326095581,
      "learning_rate": 4.9603011477640235e-05,
      "loss": 1.7143,
      "step": 23700
    },
    {
      "epoch": 0.023920824082440405,
      "grad_norm": 7.554256916046143,
      "learning_rate": 4.960133634990337e-05,
      "loss": 1.7875,
      "step": 23800
    },
    {
      "epoch": 0.02402133174665234,
      "grad_norm": 8.135743141174316,
      "learning_rate": 4.95996612221665e-05,
      "loss": 1.7741,
      "step": 23900
    },
    {
      "epoch": 0.024121839410864276,
      "grad_norm": 8.289738655090332,
      "learning_rate": 4.9597986094429635e-05,
      "loss": 1.7611,
      "step": 24000
    },
    {
      "epoch": 0.02422234707507621,
      "grad_norm": 3.637906789779663,
      "learning_rate": 4.959631096669277e-05,
      "loss": 1.6985,
      "step": 24100
    },
    {
      "epoch": 0.024322854739288146,
      "grad_norm": 4.836649417877197,
      "learning_rate": 4.95946358389559e-05,
      "loss": 1.8577,
      "step": 24200
    },
    {
      "epoch": 0.024423362403500078,
      "grad_norm": 6.481428623199463,
      "learning_rate": 4.9592960711219036e-05,
      "loss": 1.8951,
      "step": 24300
    },
    {
      "epoch": 0.024523870067712013,
      "grad_norm": 9.026412963867188,
      "learning_rate": 4.959128558348217e-05,
      "loss": 1.8657,
      "step": 24400
    },
    {
      "epoch": 0.024624377731923948,
      "grad_norm": 3.8457415103912354,
      "learning_rate": 4.95896104557453e-05,
      "loss": 1.8361,
      "step": 24500
    },
    {
      "epoch": 0.024724885396135883,
      "grad_norm": 4.66774320602417,
      "learning_rate": 4.9587935328008436e-05,
      "loss": 1.8231,
      "step": 24600
    },
    {
      "epoch": 0.024825393060347818,
      "grad_norm": 7.906130313873291,
      "learning_rate": 4.958626020027157e-05,
      "loss": 1.8029,
      "step": 24700
    },
    {
      "epoch": 0.02492590072455975,
      "grad_norm": 8.32071590423584,
      "learning_rate": 4.95845850725347e-05,
      "loss": 1.8373,
      "step": 24800
    },
    {
      "epoch": 0.025026408388771685,
      "grad_norm": 8.052734375,
      "learning_rate": 4.958290994479784e-05,
      "loss": 1.7394,
      "step": 24900
    },
    {
      "epoch": 0.02512691605298362,
      "grad_norm": 8.143525123596191,
      "learning_rate": 4.9581234817060976e-05,
      "loss": 1.7053,
      "step": 25000
    },
    {
      "epoch": 0.025227423717195555,
      "grad_norm": 4.50398588180542,
      "learning_rate": 4.957955968932411e-05,
      "loss": 1.7871,
      "step": 25100
    },
    {
      "epoch": 0.02532793138140749,
      "grad_norm": 6.853872776031494,
      "learning_rate": 4.957788456158724e-05,
      "loss": 1.8238,
      "step": 25200
    },
    {
      "epoch": 0.025428439045619422,
      "grad_norm": 8.782527923583984,
      "learning_rate": 4.957620943385038e-05,
      "loss": 1.8762,
      "step": 25300
    },
    {
      "epoch": 0.025528946709831357,
      "grad_norm": 11.33597183227539,
      "learning_rate": 4.957453430611351e-05,
      "loss": 2.0354,
      "step": 25400
    },
    {
      "epoch": 0.025629454374043292,
      "grad_norm": 7.147130489349365,
      "learning_rate": 4.957285917837665e-05,
      "loss": 1.9025,
      "step": 25500
    },
    {
      "epoch": 0.025729962038255227,
      "grad_norm": 7.014151573181152,
      "learning_rate": 4.9571184050639784e-05,
      "loss": 1.794,
      "step": 25600
    },
    {
      "epoch": 0.025830469702467163,
      "grad_norm": 8.12414264678955,
      "learning_rate": 4.956950892290292e-05,
      "loss": 1.7887,
      "step": 25700
    },
    {
      "epoch": 0.025930977366679098,
      "grad_norm": 7.89771032333374,
      "learning_rate": 4.956783379516605e-05,
      "loss": 1.8718,
      "step": 25800
    },
    {
      "epoch": 0.02603148503089103,
      "grad_norm": 6.4883036613464355,
      "learning_rate": 4.9566158667429184e-05,
      "loss": 1.7115,
      "step": 25900
    },
    {
      "epoch": 0.026131992695102964,
      "grad_norm": 7.768693447113037,
      "learning_rate": 4.956448353969232e-05,
      "loss": 1.7504,
      "step": 26000
    },
    {
      "epoch": 0.0262325003593149,
      "grad_norm": 8.407867431640625,
      "learning_rate": 4.956280841195546e-05,
      "loss": 1.7469,
      "step": 26100
    },
    {
      "epoch": 0.026333008023526835,
      "grad_norm": 9.014771461486816,
      "learning_rate": 4.956113328421859e-05,
      "loss": 1.9265,
      "step": 26200
    },
    {
      "epoch": 0.02643351568773877,
      "grad_norm": 9.403285026550293,
      "learning_rate": 4.9559458156481724e-05,
      "loss": 1.8064,
      "step": 26300
    },
    {
      "epoch": 0.0265340233519507,
      "grad_norm": 6.763368129730225,
      "learning_rate": 4.955778302874486e-05,
      "loss": 1.8302,
      "step": 26400
    },
    {
      "epoch": 0.026634531016162637,
      "grad_norm": 9.027478218078613,
      "learning_rate": 4.955610790100799e-05,
      "loss": 1.7394,
      "step": 26500
    },
    {
      "epoch": 0.026735038680374572,
      "grad_norm": 6.848546504974365,
      "learning_rate": 4.9554432773271125e-05,
      "loss": 1.8203,
      "step": 26600
    },
    {
      "epoch": 0.026835546344586507,
      "grad_norm": 7.680617332458496,
      "learning_rate": 4.9552757645534265e-05,
      "loss": 1.8742,
      "step": 26700
    },
    {
      "epoch": 0.026936054008798442,
      "grad_norm": 6.376925468444824,
      "learning_rate": 4.95510825177974e-05,
      "loss": 1.818,
      "step": 26800
    },
    {
      "epoch": 0.027036561673010374,
      "grad_norm": 6.922570705413818,
      "learning_rate": 4.954940739006053e-05,
      "loss": 1.7872,
      "step": 26900
    },
    {
      "epoch": 0.02713706933722231,
      "grad_norm": 9.025642395019531,
      "learning_rate": 4.9547732262323665e-05,
      "loss": 1.8042,
      "step": 27000
    },
    {
      "epoch": 0.027237577001434244,
      "grad_norm": 7.384274482727051,
      "learning_rate": 4.95460571345868e-05,
      "loss": 1.7601,
      "step": 27100
    },
    {
      "epoch": 0.02733808466564618,
      "grad_norm": 5.329200744628906,
      "learning_rate": 4.954438200684993e-05,
      "loss": 1.7432,
      "step": 27200
    },
    {
      "epoch": 0.027438592329858114,
      "grad_norm": 6.793598651885986,
      "learning_rate": 4.954270687911307e-05,
      "loss": 1.7842,
      "step": 27300
    },
    {
      "epoch": 0.02753909999407005,
      "grad_norm": 6.7672224044799805,
      "learning_rate": 4.9541031751376206e-05,
      "loss": 1.7908,
      "step": 27400
    },
    {
      "epoch": 0.02763960765828198,
      "grad_norm": 4.442169189453125,
      "learning_rate": 4.953935662363934e-05,
      "loss": 1.7123,
      "step": 27500
    },
    {
      "epoch": 0.027740115322493916,
      "grad_norm": 7.654606342315674,
      "learning_rate": 4.953768149590247e-05,
      "loss": 1.7954,
      "step": 27600
    },
    {
      "epoch": 0.02784062298670585,
      "grad_norm": 5.584439754486084,
      "learning_rate": 4.9536006368165606e-05,
      "loss": 1.7668,
      "step": 27700
    },
    {
      "epoch": 0.027941130650917786,
      "grad_norm": 7.683645725250244,
      "learning_rate": 4.953433124042874e-05,
      "loss": 1.7433,
      "step": 27800
    },
    {
      "epoch": 0.02804163831512972,
      "grad_norm": 5.268127918243408,
      "learning_rate": 4.953265611269188e-05,
      "loss": 1.7638,
      "step": 27900
    },
    {
      "epoch": 0.028142145979341653,
      "grad_norm": 6.316380500793457,
      "learning_rate": 4.953098098495501e-05,
      "loss": 1.7266,
      "step": 28000
    },
    {
      "epoch": 0.02824265364355359,
      "grad_norm": 7.526682376861572,
      "learning_rate": 4.9529305857218146e-05,
      "loss": 1.8468,
      "step": 28100
    },
    {
      "epoch": 0.028343161307765524,
      "grad_norm": 9.3959321975708,
      "learning_rate": 4.952763072948128e-05,
      "loss": 1.8538,
      "step": 28200
    },
    {
      "epoch": 0.02844366897197746,
      "grad_norm": 8.277077674865723,
      "learning_rate": 4.952595560174441e-05,
      "loss": 1.7559,
      "step": 28300
    },
    {
      "epoch": 0.028544176636189394,
      "grad_norm": 7.3470001220703125,
      "learning_rate": 4.9524280474007553e-05,
      "loss": 1.808,
      "step": 28400
    },
    {
      "epoch": 0.028644684300401325,
      "grad_norm": 7.893800258636475,
      "learning_rate": 4.952260534627069e-05,
      "loss": 1.8874,
      "step": 28500
    },
    {
      "epoch": 0.02874519196461326,
      "grad_norm": 6.265868663787842,
      "learning_rate": 4.952093021853382e-05,
      "loss": 1.8188,
      "step": 28600
    },
    {
      "epoch": 0.028845699628825196,
      "grad_norm": 9.965249061584473,
      "learning_rate": 4.9519255090796954e-05,
      "loss": 1.8682,
      "step": 28700
    },
    {
      "epoch": 0.02894620729303713,
      "grad_norm": 3.2656445503234863,
      "learning_rate": 4.951757996306009e-05,
      "loss": 1.8497,
      "step": 28800
    },
    {
      "epoch": 0.029046714957249066,
      "grad_norm": 7.608036518096924,
      "learning_rate": 4.951590483532322e-05,
      "loss": 1.8132,
      "step": 28900
    },
    {
      "epoch": 0.029147222621461,
      "grad_norm": 7.674694538116455,
      "learning_rate": 4.9514229707586354e-05,
      "loss": 1.8538,
      "step": 29000
    },
    {
      "epoch": 0.029247730285672933,
      "grad_norm": 7.242445468902588,
      "learning_rate": 4.951255457984949e-05,
      "loss": 1.8248,
      "step": 29100
    },
    {
      "epoch": 0.029348237949884868,
      "grad_norm": 9.086197853088379,
      "learning_rate": 4.951087945211262e-05,
      "loss": 1.8126,
      "step": 29200
    },
    {
      "epoch": 0.029448745614096803,
      "grad_norm": 7.4509453773498535,
      "learning_rate": 4.9509204324375754e-05,
      "loss": 1.8007,
      "step": 29300
    },
    {
      "epoch": 0.029549253278308738,
      "grad_norm": 10.431585311889648,
      "learning_rate": 4.950752919663889e-05,
      "loss": 1.8726,
      "step": 29400
    },
    {
      "epoch": 0.029649760942520673,
      "grad_norm": 5.537872314453125,
      "learning_rate": 4.950585406890203e-05,
      "loss": 1.829,
      "step": 29500
    },
    {
      "epoch": 0.029750268606732605,
      "grad_norm": 8.586527824401855,
      "learning_rate": 4.950417894116516e-05,
      "loss": 1.699,
      "step": 29600
    },
    {
      "epoch": 0.02985077627094454,
      "grad_norm": 6.391128063201904,
      "learning_rate": 4.9502503813428295e-05,
      "loss": 1.7497,
      "step": 29700
    },
    {
      "epoch": 0.029951283935156475,
      "grad_norm": 9.23879623413086,
      "learning_rate": 4.950082868569143e-05,
      "loss": 1.771,
      "step": 29800
    },
    {
      "epoch": 0.03005179159936841,
      "grad_norm": 8.253873825073242,
      "learning_rate": 4.949915355795456e-05,
      "loss": 1.8193,
      "step": 29900
    },
    {
      "epoch": 0.030152299263580346,
      "grad_norm": 8.805868148803711,
      "learning_rate": 4.9497478430217695e-05,
      "loss": 1.849,
      "step": 30000
    },
    {
      "epoch": 0.030252806927792277,
      "grad_norm": 7.138847351074219,
      "learning_rate": 4.949580330248083e-05,
      "loss": 2.0231,
      "step": 30100
    },
    {
      "epoch": 0.030353314592004212,
      "grad_norm": 6.3881516456604,
      "learning_rate": 4.949412817474397e-05,
      "loss": 2.0289,
      "step": 30200
    },
    {
      "epoch": 0.030453822256216147,
      "grad_norm": 8.682924270629883,
      "learning_rate": 4.94924530470071e-05,
      "loss": 2.0064,
      "step": 30300
    },
    {
      "epoch": 0.030554329920428083,
      "grad_norm": 8.332756042480469,
      "learning_rate": 4.9490777919270235e-05,
      "loss": 2.0204,
      "step": 30400
    },
    {
      "epoch": 0.030654837584640018,
      "grad_norm": 8.148329734802246,
      "learning_rate": 4.948910279153337e-05,
      "loss": 2.0206,
      "step": 30500
    },
    {
      "epoch": 0.030755345248851953,
      "grad_norm": 6.595611095428467,
      "learning_rate": 4.94874276637965e-05,
      "loss": 1.8917,
      "step": 30600
    },
    {
      "epoch": 0.030855852913063885,
      "grad_norm": 9.395344734191895,
      "learning_rate": 4.9485752536059636e-05,
      "loss": 2.1455,
      "step": 30700
    },
    {
      "epoch": 0.03095636057727582,
      "grad_norm": 6.428055286407471,
      "learning_rate": 4.9484077408322776e-05,
      "loss": 2.0002,
      "step": 30800
    },
    {
      "epoch": 0.031056868241487755,
      "grad_norm": 7.728043079376221,
      "learning_rate": 4.948240228058591e-05,
      "loss": 1.9206,
      "step": 30900
    },
    {
      "epoch": 0.03115737590569969,
      "grad_norm": 6.722405433654785,
      "learning_rate": 4.948072715284904e-05,
      "loss": 2.0437,
      "step": 31000
    },
    {
      "epoch": 0.031257883569911625,
      "grad_norm": 8.00188159942627,
      "learning_rate": 4.9479052025112176e-05,
      "loss": 2.17,
      "step": 31100
    },
    {
      "epoch": 0.03135839123412356,
      "grad_norm": 8.553473472595215,
      "learning_rate": 4.947737689737531e-05,
      "loss": 2.0857,
      "step": 31200
    },
    {
      "epoch": 0.031458898898335495,
      "grad_norm": 7.214665412902832,
      "learning_rate": 4.947570176963844e-05,
      "loss": 2.0891,
      "step": 31300
    },
    {
      "epoch": 0.03155940656254743,
      "grad_norm": 6.7334980964660645,
      "learning_rate": 4.947402664190158e-05,
      "loss": 2.0853,
      "step": 31400
    },
    {
      "epoch": 0.03165991422675936,
      "grad_norm": 8.51312255859375,
      "learning_rate": 4.947235151416472e-05,
      "loss": 2.1206,
      "step": 31500
    },
    {
      "epoch": 0.031760421890971294,
      "grad_norm": 7.977327346801758,
      "learning_rate": 4.947067638642785e-05,
      "loss": 1.9602,
      "step": 31600
    },
    {
      "epoch": 0.03186092955518323,
      "grad_norm": 5.469855785369873,
      "learning_rate": 4.9469001258690984e-05,
      "loss": 2.0961,
      "step": 31700
    },
    {
      "epoch": 0.031961437219395164,
      "grad_norm": 7.135127544403076,
      "learning_rate": 4.946732613095412e-05,
      "loss": 2.0187,
      "step": 31800
    },
    {
      "epoch": 0.0320619448836071,
      "grad_norm": 6.096853256225586,
      "learning_rate": 4.946565100321725e-05,
      "loss": 2.0792,
      "step": 31900
    },
    {
      "epoch": 0.032162452547819034,
      "grad_norm": 8.827083587646484,
      "learning_rate": 4.946397587548039e-05,
      "loss": 1.9799,
      "step": 32000
    },
    {
      "epoch": 0.03226296021203097,
      "grad_norm": 6.593751907348633,
      "learning_rate": 4.9462300747743524e-05,
      "loss": 2.0443,
      "step": 32100
    },
    {
      "epoch": 0.032363467876242905,
      "grad_norm": 5.2947678565979,
      "learning_rate": 4.946062562000666e-05,
      "loss": 2.0517,
      "step": 32200
    },
    {
      "epoch": 0.03246397554045484,
      "grad_norm": 2.9299159049987793,
      "learning_rate": 4.945895049226979e-05,
      "loss": 2.1213,
      "step": 32300
    },
    {
      "epoch": 0.032564483204666775,
      "grad_norm": 4.530685901641846,
      "learning_rate": 4.9457275364532924e-05,
      "loss": 1.9677,
      "step": 32400
    },
    {
      "epoch": 0.0326649908688787,
      "grad_norm": 6.079502105712891,
      "learning_rate": 4.945560023679606e-05,
      "loss": 2.0675,
      "step": 32500
    },
    {
      "epoch": 0.03276549853309064,
      "grad_norm": 6.58509635925293,
      "learning_rate": 4.94539251090592e-05,
      "loss": 2.0156,
      "step": 32600
    },
    {
      "epoch": 0.03286600619730257,
      "grad_norm": 6.473386764526367,
      "learning_rate": 4.945224998132233e-05,
      "loss": 2.0237,
      "step": 32700
    },
    {
      "epoch": 0.03296651386151451,
      "grad_norm": 6.400232791900635,
      "learning_rate": 4.9450574853585465e-05,
      "loss": 2.0478,
      "step": 32800
    },
    {
      "epoch": 0.033067021525726444,
      "grad_norm": 7.714925765991211,
      "learning_rate": 4.94488997258486e-05,
      "loss": 2.076,
      "step": 32900
    },
    {
      "epoch": 0.03316752918993838,
      "grad_norm": 6.936695575714111,
      "learning_rate": 4.944722459811173e-05,
      "loss": 2.1384,
      "step": 33000
    },
    {
      "epoch": 0.033268036854150314,
      "grad_norm": 7.19272518157959,
      "learning_rate": 4.9445549470374865e-05,
      "loss": 1.9689,
      "step": 33100
    },
    {
      "epoch": 0.03336854451836225,
      "grad_norm": 8.08339786529541,
      "learning_rate": 4.9443874342638005e-05,
      "loss": 2.0291,
      "step": 33200
    },
    {
      "epoch": 0.033469052182574184,
      "grad_norm": 6.441768169403076,
      "learning_rate": 4.944219921490114e-05,
      "loss": 2.0775,
      "step": 33300
    },
    {
      "epoch": 0.03356955984678612,
      "grad_norm": 6.5455756187438965,
      "learning_rate": 4.944052408716427e-05,
      "loss": 2.1089,
      "step": 33400
    },
    {
      "epoch": 0.033670067510998054,
      "grad_norm": 4.448860168457031,
      "learning_rate": 4.9438848959427405e-05,
      "loss": 2.0495,
      "step": 33500
    },
    {
      "epoch": 0.03377057517520998,
      "grad_norm": 6.001333236694336,
      "learning_rate": 4.943717383169054e-05,
      "loss": 1.9827,
      "step": 33600
    },
    {
      "epoch": 0.03387108283942192,
      "grad_norm": 9.46159553527832,
      "learning_rate": 4.943549870395367e-05,
      "loss": 1.9276,
      "step": 33700
    },
    {
      "epoch": 0.03397159050363385,
      "grad_norm": 7.286071300506592,
      "learning_rate": 4.9433823576216806e-05,
      "loss": 2.0275,
      "step": 33800
    },
    {
      "epoch": 0.03407209816784579,
      "grad_norm": 8.500449180603027,
      "learning_rate": 4.943214844847994e-05,
      "loss": 2.1065,
      "step": 33900
    },
    {
      "epoch": 0.03417260583205772,
      "grad_norm": 8.362936019897461,
      "learning_rate": 4.943047332074307e-05,
      "loss": 1.9692,
      "step": 34000
    },
    {
      "epoch": 0.03427311349626966,
      "grad_norm": 8.75384521484375,
      "learning_rate": 4.9428798193006206e-05,
      "loss": 1.9816,
      "step": 34100
    },
    {
      "epoch": 0.03437362116048159,
      "grad_norm": 9.970179557800293,
      "learning_rate": 4.9427123065269346e-05,
      "loss": 2.058,
      "step": 34200
    },
    {
      "epoch": 0.03447412882469353,
      "grad_norm": 8.624659538269043,
      "learning_rate": 4.942544793753248e-05,
      "loss": 2.0921,
      "step": 34300
    },
    {
      "epoch": 0.034574636488905464,
      "grad_norm": 8.515580177307129,
      "learning_rate": 4.942377280979561e-05,
      "loss": 2.0217,
      "step": 34400
    },
    {
      "epoch": 0.0346751441531174,
      "grad_norm": 4.6579270362854,
      "learning_rate": 4.9422097682058747e-05,
      "loss": 1.8878,
      "step": 34500
    },
    {
      "epoch": 0.034775651817329334,
      "grad_norm": 10.273581504821777,
      "learning_rate": 4.942042255432188e-05,
      "loss": 2.0055,
      "step": 34600
    },
    {
      "epoch": 0.03487615948154126,
      "grad_norm": 6.510052680969238,
      "learning_rate": 4.941874742658501e-05,
      "loss": 2.0622,
      "step": 34700
    },
    {
      "epoch": 0.0349766671457532,
      "grad_norm": 10.704941749572754,
      "learning_rate": 4.941707229884815e-05,
      "loss": 2.0118,
      "step": 34800
    },
    {
      "epoch": 0.03507717480996513,
      "grad_norm": 5.296908378601074,
      "learning_rate": 4.941539717111128e-05,
      "loss": 2.0306,
      "step": 34900
    },
    {
      "epoch": 0.03517768247417707,
      "grad_norm": 4.759916305541992,
      "learning_rate": 4.941372204337442e-05,
      "loss": 1.9603,
      "step": 35000
    },
    {
      "epoch": 0.035278190138389,
      "grad_norm": 4.934860706329346,
      "learning_rate": 4.9412046915637554e-05,
      "loss": 2.0802,
      "step": 35100
    },
    {
      "epoch": 0.03537869780260094,
      "grad_norm": 7.243815898895264,
      "learning_rate": 4.941037178790069e-05,
      "loss": 2.1323,
      "step": 35200
    },
    {
      "epoch": 0.03547920546681287,
      "grad_norm": 6.304067134857178,
      "learning_rate": 4.940869666016382e-05,
      "loss": 1.9922,
      "step": 35300
    },
    {
      "epoch": 0.03557971313102481,
      "grad_norm": 6.4538116455078125,
      "learning_rate": 4.9407021532426954e-05,
      "loss": 1.9935,
      "step": 35400
    },
    {
      "epoch": 0.03568022079523674,
      "grad_norm": 7.545917987823486,
      "learning_rate": 4.940534640469009e-05,
      "loss": 2.0651,
      "step": 35500
    },
    {
      "epoch": 0.03578072845944868,
      "grad_norm": 6.788231372833252,
      "learning_rate": 4.940367127695323e-05,
      "loss": 2.1022,
      "step": 35600
    },
    {
      "epoch": 0.035881236123660606,
      "grad_norm": 6.287163257598877,
      "learning_rate": 4.940199614921636e-05,
      "loss": 2.0412,
      "step": 35700
    },
    {
      "epoch": 0.03598174378787254,
      "grad_norm": 7.672428131103516,
      "learning_rate": 4.9400321021479495e-05,
      "loss": 2.0336,
      "step": 35800
    },
    {
      "epoch": 0.03608225145208448,
      "grad_norm": 6.933588027954102,
      "learning_rate": 4.939864589374263e-05,
      "loss": 1.9055,
      "step": 35900
    },
    {
      "epoch": 0.03618275911629641,
      "grad_norm": 6.131506443023682,
      "learning_rate": 4.939697076600576e-05,
      "loss": 2.0042,
      "step": 36000
    },
    {
      "epoch": 0.03628326678050835,
      "grad_norm": 7.915004253387451,
      "learning_rate": 4.9395295638268895e-05,
      "loss": 1.8968,
      "step": 36100
    },
    {
      "epoch": 0.03638377444472028,
      "grad_norm": 6.37233829498291,
      "learning_rate": 4.9393620510532035e-05,
      "loss": 2.0673,
      "step": 36200
    },
    {
      "epoch": 0.03648428210893222,
      "grad_norm": 6.573568344116211,
      "learning_rate": 4.939194538279517e-05,
      "loss": 2.0869,
      "step": 36300
    },
    {
      "epoch": 0.03658478977314415,
      "grad_norm": 7.749517440795898,
      "learning_rate": 4.93902702550583e-05,
      "loss": 1.9848,
      "step": 36400
    },
    {
      "epoch": 0.03668529743735609,
      "grad_norm": 13.734471321105957,
      "learning_rate": 4.9388595127321435e-05,
      "loss": 1.9787,
      "step": 36500
    },
    {
      "epoch": 0.03678580510156802,
      "grad_norm": 5.31687593460083,
      "learning_rate": 4.938691999958457e-05,
      "loss": 2.0147,
      "step": 36600
    },
    {
      "epoch": 0.03688631276577996,
      "grad_norm": 7.186860084533691,
      "learning_rate": 4.938524487184771e-05,
      "loss": 2.1167,
      "step": 36700
    },
    {
      "epoch": 0.036986820429991886,
      "grad_norm": 7.001540660858154,
      "learning_rate": 4.938356974411084e-05,
      "loss": 1.9702,
      "step": 36800
    },
    {
      "epoch": 0.03708732809420382,
      "grad_norm": 7.035643577575684,
      "learning_rate": 4.9381894616373976e-05,
      "loss": 2.0098,
      "step": 36900
    },
    {
      "epoch": 0.037187835758415756,
      "grad_norm": 7.4027419090271,
      "learning_rate": 4.938021948863711e-05,
      "loss": 1.9732,
      "step": 37000
    },
    {
      "epoch": 0.03728834342262769,
      "grad_norm": 6.061507225036621,
      "learning_rate": 4.937854436090024e-05,
      "loss": 1.9859,
      "step": 37100
    },
    {
      "epoch": 0.037388851086839626,
      "grad_norm": 8.45710277557373,
      "learning_rate": 4.9376869233163376e-05,
      "loss": 2.0577,
      "step": 37200
    },
    {
      "epoch": 0.03748935875105156,
      "grad_norm": 6.019538879394531,
      "learning_rate": 4.9375194105426516e-05,
      "loss": 2.0029,
      "step": 37300
    },
    {
      "epoch": 0.0375898664152635,
      "grad_norm": 7.4691362380981445,
      "learning_rate": 4.937351897768965e-05,
      "loss": 2.077,
      "step": 37400
    },
    {
      "epoch": 0.03769037407947543,
      "grad_norm": 7.223671913146973,
      "learning_rate": 4.937184384995278e-05,
      "loss": 1.9372,
      "step": 37500
    },
    {
      "epoch": 0.03779088174368737,
      "grad_norm": 6.2735915184021,
      "learning_rate": 4.9370168722215917e-05,
      "loss": 1.9947,
      "step": 37600
    },
    {
      "epoch": 0.0378913894078993,
      "grad_norm": 9.196352005004883,
      "learning_rate": 4.936849359447905e-05,
      "loss": 2.097,
      "step": 37700
    },
    {
      "epoch": 0.03799189707211124,
      "grad_norm": 8.167383193969727,
      "learning_rate": 4.9366818466742183e-05,
      "loss": 1.9712,
      "step": 37800
    },
    {
      "epoch": 0.038092404736323165,
      "grad_norm": 6.894033432006836,
      "learning_rate": 4.9365143339005324e-05,
      "loss": 2.0386,
      "step": 37900
    },
    {
      "epoch": 0.0381929124005351,
      "grad_norm": 5.676726818084717,
      "learning_rate": 4.936346821126846e-05,
      "loss": 2.0925,
      "step": 38000
    },
    {
      "epoch": 0.038293420064747036,
      "grad_norm": 7.544719219207764,
      "learning_rate": 4.936179308353159e-05,
      "loss": 1.9479,
      "step": 38100
    },
    {
      "epoch": 0.03839392772895897,
      "grad_norm": 7.086043357849121,
      "learning_rate": 4.9360117955794724e-05,
      "loss": 1.9806,
      "step": 38200
    },
    {
      "epoch": 0.038494435393170906,
      "grad_norm": 6.115241050720215,
      "learning_rate": 4.935844282805786e-05,
      "loss": 1.9923,
      "step": 38300
    },
    {
      "epoch": 0.03859494305738284,
      "grad_norm": 6.690289497375488,
      "learning_rate": 4.935676770032099e-05,
      "loss": 2.015,
      "step": 38400
    },
    {
      "epoch": 0.038695450721594776,
      "grad_norm": 5.471193313598633,
      "learning_rate": 4.9355092572584124e-05,
      "loss": 1.9312,
      "step": 38500
    },
    {
      "epoch": 0.03879595838580671,
      "grad_norm": 7.777219772338867,
      "learning_rate": 4.935341744484726e-05,
      "loss": 2.0045,
      "step": 38600
    },
    {
      "epoch": 0.03889646605001865,
      "grad_norm": 6.9915242195129395,
      "learning_rate": 4.935174231711039e-05,
      "loss": 1.9699,
      "step": 38700
    },
    {
      "epoch": 0.03899697371423058,
      "grad_norm": 7.366372585296631,
      "learning_rate": 4.9350067189373524e-05,
      "loss": 1.9149,
      "step": 38800
    },
    {
      "epoch": 0.03909748137844251,
      "grad_norm": 5.224327087402344,
      "learning_rate": 4.9348392061636665e-05,
      "loss": 1.984,
      "step": 38900
    },
    {
      "epoch": 0.039197989042654445,
      "grad_norm": 8.052453994750977,
      "learning_rate": 4.93467169338998e-05,
      "loss": 2.0849,
      "step": 39000
    },
    {
      "epoch": 0.03929849670686638,
      "grad_norm": 7.620514392852783,
      "learning_rate": 4.934504180616293e-05,
      "loss": 2.1553,
      "step": 39100
    },
    {
      "epoch": 0.039399004371078315,
      "grad_norm": 8.384954452514648,
      "learning_rate": 4.9343366678426065e-05,
      "loss": 1.8895,
      "step": 39200
    },
    {
      "epoch": 0.03949951203529025,
      "grad_norm": 9.498005867004395,
      "learning_rate": 4.93416915506892e-05,
      "loss": 2.1393,
      "step": 39300
    },
    {
      "epoch": 0.039600019699502186,
      "grad_norm": 6.209233283996582,
      "learning_rate": 4.934001642295233e-05,
      "loss": 1.9924,
      "step": 39400
    },
    {
      "epoch": 0.03970052736371412,
      "grad_norm": 4.798612117767334,
      "learning_rate": 4.9338341295215465e-05,
      "loss": 2.0132,
      "step": 39500
    },
    {
      "epoch": 0.039801035027926056,
      "grad_norm": 5.146772384643555,
      "learning_rate": 4.93366661674786e-05,
      "loss": 2.1181,
      "step": 39600
    },
    {
      "epoch": 0.03990154269213799,
      "grad_norm": 9.231719970703125,
      "learning_rate": 4.933499103974174e-05,
      "loss": 1.9352,
      "step": 39700
    },
    {
      "epoch": 0.040002050356349926,
      "grad_norm": 7.042062282562256,
      "learning_rate": 4.933331591200487e-05,
      "loss": 1.9857,
      "step": 39800
    },
    {
      "epoch": 0.04010255802056186,
      "grad_norm": 7.442486763000488,
      "learning_rate": 4.9331640784268006e-05,
      "loss": 2.1442,
      "step": 39900
    },
    {
      "epoch": 0.04020306568477379,
      "grad_norm": 6.089105606079102,
      "learning_rate": 4.932996565653114e-05,
      "loss": 1.9917,
      "step": 40000
    },
    {
      "epoch": 0.040303573348985725,
      "grad_norm": 6.927257537841797,
      "learning_rate": 4.932829052879427e-05,
      "loss": 1.9458,
      "step": 40100
    },
    {
      "epoch": 0.04040408101319766,
      "grad_norm": 11.334816932678223,
      "learning_rate": 4.9326615401057406e-05,
      "loss": 2.0416,
      "step": 40200
    },
    {
      "epoch": 0.040504588677409595,
      "grad_norm": 8.096550941467285,
      "learning_rate": 4.9324940273320546e-05,
      "loss": 1.9558,
      "step": 40300
    },
    {
      "epoch": 0.04060509634162153,
      "grad_norm": 6.851556777954102,
      "learning_rate": 4.932326514558368e-05,
      "loss": 1.9176,
      "step": 40400
    },
    {
      "epoch": 0.040705604005833465,
      "grad_norm": 6.463494777679443,
      "learning_rate": 4.932159001784681e-05,
      "loss": 2.138,
      "step": 40500
    },
    {
      "epoch": 0.0408061116700454,
      "grad_norm": 7.698417663574219,
      "learning_rate": 4.9319914890109946e-05,
      "loss": 2.0744,
      "step": 40600
    },
    {
      "epoch": 0.040906619334257335,
      "grad_norm": 6.4771881103515625,
      "learning_rate": 4.931823976237308e-05,
      "loss": 2.0387,
      "step": 40700
    },
    {
      "epoch": 0.04100712699846927,
      "grad_norm": 6.5845842361450195,
      "learning_rate": 4.931656463463621e-05,
      "loss": 2.0408,
      "step": 40800
    },
    {
      "epoch": 0.041107634662681206,
      "grad_norm": 6.405738353729248,
      "learning_rate": 4.9314889506899353e-05,
      "loss": 1.9481,
      "step": 40900
    },
    {
      "epoch": 0.041208142326893134,
      "grad_norm": 6.829430103302002,
      "learning_rate": 4.931321437916249e-05,
      "loss": 1.922,
      "step": 41000
    },
    {
      "epoch": 0.04130864999110507,
      "grad_norm": 7.63303279876709,
      "learning_rate": 4.931153925142562e-05,
      "loss": 1.95,
      "step": 41100
    },
    {
      "epoch": 0.041409157655317004,
      "grad_norm": 5.659090042114258,
      "learning_rate": 4.9309864123688754e-05,
      "loss": 1.8486,
      "step": 41200
    },
    {
      "epoch": 0.04150966531952894,
      "grad_norm": 7.608913421630859,
      "learning_rate": 4.930818899595189e-05,
      "loss": 2.0683,
      "step": 41300
    },
    {
      "epoch": 0.041610172983740874,
      "grad_norm": 7.765044212341309,
      "learning_rate": 4.930651386821502e-05,
      "loss": 2.0248,
      "step": 41400
    },
    {
      "epoch": 0.04171068064795281,
      "grad_norm": 4.2957024574279785,
      "learning_rate": 4.930483874047816e-05,
      "loss": 2.0258,
      "step": 41500
    },
    {
      "epoch": 0.041811188312164745,
      "grad_norm": 7.033343315124512,
      "learning_rate": 4.9303163612741294e-05,
      "loss": 2.0831,
      "step": 41600
    },
    {
      "epoch": 0.04191169597637668,
      "grad_norm": 5.265234470367432,
      "learning_rate": 4.930148848500443e-05,
      "loss": 1.9637,
      "step": 41700
    },
    {
      "epoch": 0.042012203640588615,
      "grad_norm": 7.042909622192383,
      "learning_rate": 4.929981335726756e-05,
      "loss": 2.0157,
      "step": 41800
    },
    {
      "epoch": 0.04211271130480055,
      "grad_norm": 6.69114875793457,
      "learning_rate": 4.9298138229530694e-05,
      "loss": 1.992,
      "step": 41900
    },
    {
      "epoch": 0.042213218969012485,
      "grad_norm": 6.014405250549316,
      "learning_rate": 4.929646310179383e-05,
      "loss": 2.008,
      "step": 42000
    },
    {
      "epoch": 0.04231372663322441,
      "grad_norm": 7.907511234283447,
      "learning_rate": 4.929478797405697e-05,
      "loss": 1.9228,
      "step": 42100
    },
    {
      "epoch": 0.04241423429743635,
      "grad_norm": 6.1851067543029785,
      "learning_rate": 4.92931128463201e-05,
      "loss": 1.9747,
      "step": 42200
    },
    {
      "epoch": 0.042514741961648284,
      "grad_norm": 5.673026084899902,
      "learning_rate": 4.9291437718583235e-05,
      "loss": 2.032,
      "step": 42300
    },
    {
      "epoch": 0.04261524962586022,
      "grad_norm": 8.399469375610352,
      "learning_rate": 4.928976259084637e-05,
      "loss": 2.0916,
      "step": 42400
    },
    {
      "epoch": 0.042715757290072154,
      "grad_norm": 7.388003349304199,
      "learning_rate": 4.92880874631095e-05,
      "loss": 2.0267,
      "step": 42500
    },
    {
      "epoch": 0.04281626495428409,
      "grad_norm": 6.853973388671875,
      "learning_rate": 4.9286412335372635e-05,
      "loss": 2.0132,
      "step": 42600
    },
    {
      "epoch": 0.042916772618496024,
      "grad_norm": 9.710334777832031,
      "learning_rate": 4.9284737207635775e-05,
      "loss": 1.9891,
      "step": 42700
    },
    {
      "epoch": 0.04301728028270796,
      "grad_norm": 8.734734535217285,
      "learning_rate": 4.928306207989891e-05,
      "loss": 1.9392,
      "step": 42800
    },
    {
      "epoch": 0.043117787946919894,
      "grad_norm": 7.884782791137695,
      "learning_rate": 4.928138695216204e-05,
      "loss": 1.9951,
      "step": 42900
    },
    {
      "epoch": 0.04321829561113183,
      "grad_norm": 7.410417556762695,
      "learning_rate": 4.9279711824425176e-05,
      "loss": 2.0345,
      "step": 43000
    },
    {
      "epoch": 0.043318803275343765,
      "grad_norm": 6.688415050506592,
      "learning_rate": 4.927803669668831e-05,
      "loss": 1.9691,
      "step": 43100
    },
    {
      "epoch": 0.04341931093955569,
      "grad_norm": 7.00565242767334,
      "learning_rate": 4.927636156895144e-05,
      "loss": 2.0481,
      "step": 43200
    },
    {
      "epoch": 0.04351981860376763,
      "grad_norm": 6.733659744262695,
      "learning_rate": 4.9274686441214576e-05,
      "loss": 2.0169,
      "step": 43300
    },
    {
      "epoch": 0.04362032626797956,
      "grad_norm": 6.410436630249023,
      "learning_rate": 4.927301131347771e-05,
      "loss": 1.9709,
      "step": 43400
    },
    {
      "epoch": 0.0437208339321915,
      "grad_norm": 7.9232330322265625,
      "learning_rate": 4.927133618574085e-05,
      "loss": 2.0974,
      "step": 43500
    },
    {
      "epoch": 0.04382134159640343,
      "grad_norm": 6.409467697143555,
      "learning_rate": 4.926966105800398e-05,
      "loss": 1.9491,
      "step": 43600
    },
    {
      "epoch": 0.04392184926061537,
      "grad_norm": 6.787278175354004,
      "learning_rate": 4.9267985930267116e-05,
      "loss": 1.9361,
      "step": 43700
    },
    {
      "epoch": 0.044022356924827304,
      "grad_norm": 5.857343673706055,
      "learning_rate": 4.926631080253025e-05,
      "loss": 2.1466,
      "step": 43800
    },
    {
      "epoch": 0.04412286458903924,
      "grad_norm": 6.122201442718506,
      "learning_rate": 4.926463567479338e-05,
      "loss": 1.9379,
      "step": 43900
    },
    {
      "epoch": 0.044223372253251174,
      "grad_norm": 8.410489082336426,
      "learning_rate": 4.926296054705652e-05,
      "loss": 1.9968,
      "step": 44000
    },
    {
      "epoch": 0.04432387991746311,
      "grad_norm": 7.316164970397949,
      "learning_rate": 4.926128541931965e-05,
      "loss": 2.1326,
      "step": 44100
    },
    {
      "epoch": 0.04442438758167504,
      "grad_norm": 5.270144462585449,
      "learning_rate": 4.9259610291582784e-05,
      "loss": 2.0223,
      "step": 44200
    },
    {
      "epoch": 0.04452489524588697,
      "grad_norm": 5.645151138305664,
      "learning_rate": 4.925793516384592e-05,
      "loss": 1.949,
      "step": 44300
    },
    {
      "epoch": 0.04462540291009891,
      "grad_norm": 4.026906967163086,
      "learning_rate": 4.925626003610905e-05,
      "loss": 1.9706,
      "step": 44400
    },
    {
      "epoch": 0.04472591057431084,
      "grad_norm": 6.304614067077637,
      "learning_rate": 4.925458490837219e-05,
      "loss": 1.9251,
      "step": 44500
    },
    {
      "epoch": 0.04482641823852278,
      "grad_norm": 5.2150559425354,
      "learning_rate": 4.9252909780635324e-05,
      "loss": 1.9684,
      "step": 44600
    },
    {
      "epoch": 0.04492692590273471,
      "grad_norm": 7.956095218658447,
      "learning_rate": 4.925123465289846e-05,
      "loss": 2.0087,
      "step": 44700
    },
    {
      "epoch": 0.04502743356694665,
      "grad_norm": 7.2583394050598145,
      "learning_rate": 4.924955952516159e-05,
      "loss": 2.0044,
      "step": 44800
    },
    {
      "epoch": 0.04512794123115858,
      "grad_norm": 8.481302261352539,
      "learning_rate": 4.9247884397424724e-05,
      "loss": 2.0594,
      "step": 44900
    },
    {
      "epoch": 0.04522844889537052,
      "grad_norm": 9.720837593078613,
      "learning_rate": 4.924620926968786e-05,
      "loss": 2.1017,
      "step": 45000
    },
    {
      "epoch": 0.04532895655958245,
      "grad_norm": 6.132822513580322,
      "learning_rate": 4.9244534141951e-05,
      "loss": 2.046,
      "step": 45100
    },
    {
      "epoch": 0.04542946422379439,
      "grad_norm": 7.872867107391357,
      "learning_rate": 4.924285901421413e-05,
      "loss": 2.0179,
      "step": 45200
    },
    {
      "epoch": 0.04552997188800632,
      "grad_norm": 6.927046775817871,
      "learning_rate": 4.9241183886477265e-05,
      "loss": 2.0722,
      "step": 45300
    },
    {
      "epoch": 0.04563047955221825,
      "grad_norm": 3.946460008621216,
      "learning_rate": 4.92395087587404e-05,
      "loss": 1.8835,
      "step": 45400
    },
    {
      "epoch": 0.04573098721643019,
      "grad_norm": 9.233348846435547,
      "learning_rate": 4.923783363100353e-05,
      "loss": 1.8597,
      "step": 45500
    },
    {
      "epoch": 0.04583149488064212,
      "grad_norm": 6.859275817871094,
      "learning_rate": 4.923615850326667e-05,
      "loss": 2.056,
      "step": 45600
    },
    {
      "epoch": 0.04593200254485406,
      "grad_norm": 6.47648286819458,
      "learning_rate": 4.9234483375529805e-05,
      "loss": 1.9587,
      "step": 45700
    },
    {
      "epoch": 0.04603251020906599,
      "grad_norm": 6.799388885498047,
      "learning_rate": 4.923280824779294e-05,
      "loss": 1.907,
      "step": 45800
    },
    {
      "epoch": 0.04613301787327793,
      "grad_norm": 8.089522361755371,
      "learning_rate": 4.923113312005607e-05,
      "loss": 1.9959,
      "step": 45900
    },
    {
      "epoch": 0.04623352553748986,
      "grad_norm": 8.015375137329102,
      "learning_rate": 4.9229457992319205e-05,
      "loss": 1.9563,
      "step": 46000
    },
    {
      "epoch": 0.0463340332017018,
      "grad_norm": 6.776707172393799,
      "learning_rate": 4.922778286458234e-05,
      "loss": 2.063,
      "step": 46100
    },
    {
      "epoch": 0.04643454086591373,
      "grad_norm": 5.491116046905518,
      "learning_rate": 4.922610773684548e-05,
      "loss": 1.9993,
      "step": 46200
    },
    {
      "epoch": 0.04653504853012567,
      "grad_norm": 6.390509128570557,
      "learning_rate": 4.922443260910861e-05,
      "loss": 2.1384,
      "step": 46300
    },
    {
      "epoch": 0.046635556194337596,
      "grad_norm": 8.355721473693848,
      "learning_rate": 4.9222757481371746e-05,
      "loss": 1.8142,
      "step": 46400
    },
    {
      "epoch": 0.04673606385854953,
      "grad_norm": 7.32274866104126,
      "learning_rate": 4.922108235363488e-05,
      "loss": 2.0722,
      "step": 46500
    },
    {
      "epoch": 0.046836571522761467,
      "grad_norm": 7.244762420654297,
      "learning_rate": 4.921940722589801e-05,
      "loss": 2.0427,
      "step": 46600
    },
    {
      "epoch": 0.0469370791869734,
      "grad_norm": 7.061040878295898,
      "learning_rate": 4.9217732098161146e-05,
      "loss": 1.9894,
      "step": 46700
    },
    {
      "epoch": 0.04703758685118534,
      "grad_norm": 7.485221862792969,
      "learning_rate": 4.9216056970424286e-05,
      "loss": 2.0548,
      "step": 46800
    },
    {
      "epoch": 0.04713809451539727,
      "grad_norm": 6.631341934204102,
      "learning_rate": 4.921438184268742e-05,
      "loss": 1.9299,
      "step": 46900
    },
    {
      "epoch": 0.04723860217960921,
      "grad_norm": 4.928760528564453,
      "learning_rate": 4.921270671495055e-05,
      "loss": 1.981,
      "step": 47000
    },
    {
      "epoch": 0.04733910984382114,
      "grad_norm": 6.578673362731934,
      "learning_rate": 4.921103158721369e-05,
      "loss": 2.1398,
      "step": 47100
    },
    {
      "epoch": 0.04743961750803308,
      "grad_norm": 8.69578742980957,
      "learning_rate": 4.920935645947682e-05,
      "loss": 2.0408,
      "step": 47200
    },
    {
      "epoch": 0.04754012517224501,
      "grad_norm": 7.710785865783691,
      "learning_rate": 4.9207681331739954e-05,
      "loss": 2.0245,
      "step": 47300
    },
    {
      "epoch": 0.04764063283645694,
      "grad_norm": 6.70588493347168,
      "learning_rate": 4.9206006204003094e-05,
      "loss": 2.0069,
      "step": 47400
    },
    {
      "epoch": 0.047741140500668876,
      "grad_norm": 6.423192977905273,
      "learning_rate": 4.920433107626623e-05,
      "loss": 1.9895,
      "step": 47500
    },
    {
      "epoch": 0.04784164816488081,
      "grad_norm": 5.253302574157715,
      "learning_rate": 4.920265594852936e-05,
      "loss": 1.9929,
      "step": 47600
    },
    {
      "epoch": 0.047942155829092746,
      "grad_norm": 5.344632625579834,
      "learning_rate": 4.9200980820792494e-05,
      "loss": 1.9637,
      "step": 47700
    },
    {
      "epoch": 0.04804266349330468,
      "grad_norm": 6.41392707824707,
      "learning_rate": 4.919930569305563e-05,
      "loss": 1.9924,
      "step": 47800
    },
    {
      "epoch": 0.048143171157516616,
      "grad_norm": 6.268825531005859,
      "learning_rate": 4.919763056531876e-05,
      "loss": 1.9719,
      "step": 47900
    },
    {
      "epoch": 0.04824367882172855,
      "grad_norm": 3.6746950149536133,
      "learning_rate": 4.9195955437581894e-05,
      "loss": 1.8361,
      "step": 48000
    },
    {
      "epoch": 0.04834418648594049,
      "grad_norm": 7.392910003662109,
      "learning_rate": 4.919428030984503e-05,
      "loss": 2.037,
      "step": 48100
    },
    {
      "epoch": 0.04844469415015242,
      "grad_norm": 4.955902099609375,
      "learning_rate": 4.919260518210817e-05,
      "loss": 2.1037,
      "step": 48200
    },
    {
      "epoch": 0.04854520181436436,
      "grad_norm": 9.349141120910645,
      "learning_rate": 4.91909300543713e-05,
      "loss": 1.9338,
      "step": 48300
    },
    {
      "epoch": 0.04864570947857629,
      "grad_norm": 7.024845600128174,
      "learning_rate": 4.9189254926634435e-05,
      "loss": 1.9909,
      "step": 48400
    },
    {
      "epoch": 0.04874621714278822,
      "grad_norm": 7.392526149749756,
      "learning_rate": 4.918757979889757e-05,
      "loss": 1.8456,
      "step": 48500
    },
    {
      "epoch": 0.048846724807000155,
      "grad_norm": 8.825103759765625,
      "learning_rate": 4.91859046711607e-05,
      "loss": 1.8599,
      "step": 48600
    },
    {
      "epoch": 0.04894723247121209,
      "grad_norm": 7.2477216720581055,
      "learning_rate": 4.9184229543423835e-05,
      "loss": 2.0514,
      "step": 48700
    },
    {
      "epoch": 0.049047740135424026,
      "grad_norm": 6.767156600952148,
      "learning_rate": 4.918255441568697e-05,
      "loss": 1.8658,
      "step": 48800
    },
    {
      "epoch": 0.04914824779963596,
      "grad_norm": 6.846757411956787,
      "learning_rate": 4.91808792879501e-05,
      "loss": 1.9211,
      "step": 48900
    },
    {
      "epoch": 0.049248755463847896,
      "grad_norm": 8.130217552185059,
      "learning_rate": 4.9179204160213235e-05,
      "loss": 2.0101,
      "step": 49000
    },
    {
      "epoch": 0.04934926312805983,
      "grad_norm": 9.005941390991211,
      "learning_rate": 4.917752903247637e-05,
      "loss": 2.0975,
      "step": 49100
    },
    {
      "epoch": 0.049449770792271766,
      "grad_norm": 6.112335205078125,
      "learning_rate": 4.917585390473951e-05,
      "loss": 1.9045,
      "step": 49200
    },
    {
      "epoch": 0.0495502784564837,
      "grad_norm": 6.841143608093262,
      "learning_rate": 4.917417877700264e-05,
      "loss": 1.9769,
      "step": 49300
    },
    {
      "epoch": 0.049650786120695636,
      "grad_norm": 8.59567928314209,
      "learning_rate": 4.9172503649265776e-05,
      "loss": 1.9997,
      "step": 49400
    },
    {
      "epoch": 0.04975129378490757,
      "grad_norm": 5.418025016784668,
      "learning_rate": 4.917082852152891e-05,
      "loss": 1.9591,
      "step": 49500
    },
    {
      "epoch": 0.0498518014491195,
      "grad_norm": 6.600994110107422,
      "learning_rate": 4.916915339379204e-05,
      "loss": 2.031,
      "step": 49600
    },
    {
      "epoch": 0.049952309113331435,
      "grad_norm": 7.133761405944824,
      "learning_rate": 4.9167478266055176e-05,
      "loss": 1.8891,
      "step": 49700
    },
    {
      "epoch": 0.05005281677754337,
      "grad_norm": 4.6619367599487305,
      "learning_rate": 4.9165803138318316e-05,
      "loss": 2.0767,
      "step": 49800
    },
    {
      "epoch": 0.050153324441755305,
      "grad_norm": 8.675516128540039,
      "learning_rate": 4.916412801058145e-05,
      "loss": 1.9808,
      "step": 49900
    },
    {
      "epoch": 0.05025383210596724,
      "grad_norm": 9.598816871643066,
      "learning_rate": 4.916245288284458e-05,
      "loss": 2.0043,
      "step": 50000
    },
    {
      "epoch": 0.050354339770179175,
      "grad_norm": 7.252224922180176,
      "learning_rate": 4.9160777755107717e-05,
      "loss": 1.8993,
      "step": 50100
    },
    {
      "epoch": 0.05045484743439111,
      "grad_norm": 6.696311950683594,
      "learning_rate": 4.915910262737085e-05,
      "loss": 1.859,
      "step": 50200
    },
    {
      "epoch": 0.050555355098603046,
      "grad_norm": 6.715450286865234,
      "learning_rate": 4.9157427499633983e-05,
      "loss": 2.0076,
      "step": 50300
    },
    {
      "epoch": 0.05065586276281498,
      "grad_norm": 5.820286750793457,
      "learning_rate": 4.9155752371897124e-05,
      "loss": 1.9781,
      "step": 50400
    },
    {
      "epoch": 0.050756370427026916,
      "grad_norm": 8.780754089355469,
      "learning_rate": 4.915407724416026e-05,
      "loss": 2.05,
      "step": 50500
    },
    {
      "epoch": 0.050856878091238844,
      "grad_norm": 7.489847660064697,
      "learning_rate": 4.915240211642339e-05,
      "loss": 1.9562,
      "step": 50600
    },
    {
      "epoch": 0.05095738575545078,
      "grad_norm": 6.80869197845459,
      "learning_rate": 4.9150726988686524e-05,
      "loss": 1.9478,
      "step": 50700
    },
    {
      "epoch": 0.051057893419662714,
      "grad_norm": 4.738742351531982,
      "learning_rate": 4.914905186094966e-05,
      "loss": 1.8706,
      "step": 50800
    },
    {
      "epoch": 0.05115840108387465,
      "grad_norm": 7.117733478546143,
      "learning_rate": 4.914737673321279e-05,
      "loss": 2.0263,
      "step": 50900
    },
    {
      "epoch": 0.051258908748086585,
      "grad_norm": 6.948517322540283,
      "learning_rate": 4.914570160547593e-05,
      "loss": 1.9707,
      "step": 51000
    },
    {
      "epoch": 0.05135941641229852,
      "grad_norm": 8.638978958129883,
      "learning_rate": 4.9144026477739064e-05,
      "loss": 1.8782,
      "step": 51100
    },
    {
      "epoch": 0.051459924076510455,
      "grad_norm": 11.55146598815918,
      "learning_rate": 4.91423513500022e-05,
      "loss": 1.9064,
      "step": 51200
    },
    {
      "epoch": 0.05156043174072239,
      "grad_norm": 5.563669681549072,
      "learning_rate": 4.914067622226533e-05,
      "loss": 2.0013,
      "step": 51300
    },
    {
      "epoch": 0.051660939404934325,
      "grad_norm": 6.869042873382568,
      "learning_rate": 4.9139001094528465e-05,
      "loss": 2.0573,
      "step": 51400
    },
    {
      "epoch": 0.05176144706914626,
      "grad_norm": 5.290738105773926,
      "learning_rate": 4.91373259667916e-05,
      "loss": 1.9486,
      "step": 51500
    },
    {
      "epoch": 0.051861954733358195,
      "grad_norm": 4.680289268493652,
      "learning_rate": 4.913565083905474e-05,
      "loss": 2.0316,
      "step": 51600
    },
    {
      "epoch": 0.051962462397570124,
      "grad_norm": 9.8546724319458,
      "learning_rate": 4.913397571131787e-05,
      "loss": 2.0541,
      "step": 51700
    },
    {
      "epoch": 0.05206297006178206,
      "grad_norm": 6.0212249755859375,
      "learning_rate": 4.9132300583581005e-05,
      "loss": 2.0035,
      "step": 51800
    },
    {
      "epoch": 0.052163477725993994,
      "grad_norm": 8.82518482208252,
      "learning_rate": 4.913062545584414e-05,
      "loss": 2.0845,
      "step": 51900
    },
    {
      "epoch": 0.05226398539020593,
      "grad_norm": 6.572654724121094,
      "learning_rate": 4.912895032810727e-05,
      "loss": 2.0194,
      "step": 52000
    },
    {
      "epoch": 0.052364493054417864,
      "grad_norm": 7.806747913360596,
      "learning_rate": 4.9127275200370405e-05,
      "loss": 1.9996,
      "step": 52100
    },
    {
      "epoch": 0.0524650007186298,
      "grad_norm": 7.945257186889648,
      "learning_rate": 4.9125600072633546e-05,
      "loss": 2.1153,
      "step": 52200
    },
    {
      "epoch": 0.052565508382841734,
      "grad_norm": 6.346557140350342,
      "learning_rate": 4.912392494489668e-05,
      "loss": 1.9898,
      "step": 52300
    },
    {
      "epoch": 0.05266601604705367,
      "grad_norm": 6.476069450378418,
      "learning_rate": 4.912224981715981e-05,
      "loss": 1.9625,
      "step": 52400
    },
    {
      "epoch": 0.052766523711265605,
      "grad_norm": 7.2460856437683105,
      "learning_rate": 4.9120574689422946e-05,
      "loss": 1.9676,
      "step": 52500
    },
    {
      "epoch": 0.05286703137547754,
      "grad_norm": 5.911940574645996,
      "learning_rate": 4.911889956168608e-05,
      "loss": 1.8651,
      "step": 52600
    },
    {
      "epoch": 0.052967539039689475,
      "grad_norm": 7.399354934692383,
      "learning_rate": 4.911722443394921e-05,
      "loss": 2.0185,
      "step": 52700
    },
    {
      "epoch": 0.0530680467039014,
      "grad_norm": 7.008653163909912,
      "learning_rate": 4.911554930621235e-05,
      "loss": 2.0294,
      "step": 52800
    },
    {
      "epoch": 0.05316855436811334,
      "grad_norm": 5.0588531494140625,
      "learning_rate": 4.9113874178475486e-05,
      "loss": 1.9505,
      "step": 52900
    },
    {
      "epoch": 0.05326906203232527,
      "grad_norm": 4.373822212219238,
      "learning_rate": 4.911219905073862e-05,
      "loss": 1.936,
      "step": 53000
    },
    {
      "epoch": 0.05336956969653721,
      "grad_norm": 6.062835216522217,
      "learning_rate": 4.911052392300175e-05,
      "loss": 1.9493,
      "step": 53100
    },
    {
      "epoch": 0.053470077360749144,
      "grad_norm": 6.47906494140625,
      "learning_rate": 4.9108848795264887e-05,
      "loss": 2.0541,
      "step": 53200
    },
    {
      "epoch": 0.05357058502496108,
      "grad_norm": 5.056389808654785,
      "learning_rate": 4.910717366752802e-05,
      "loss": 1.9332,
      "step": 53300
    },
    {
      "epoch": 0.053671092689173014,
      "grad_norm": 7.24618673324585,
      "learning_rate": 4.9105498539791153e-05,
      "loss": 2.0136,
      "step": 53400
    },
    {
      "epoch": 0.05377160035338495,
      "grad_norm": 5.874180316925049,
      "learning_rate": 4.910382341205429e-05,
      "loss": 1.9075,
      "step": 53500
    },
    {
      "epoch": 0.053872108017596884,
      "grad_norm": 5.58725118637085,
      "learning_rate": 4.910214828431742e-05,
      "loss": 1.9316,
      "step": 53600
    },
    {
      "epoch": 0.05397261568180882,
      "grad_norm": 7.385156154632568,
      "learning_rate": 4.9100473156580554e-05,
      "loss": 2.0621,
      "step": 53700
    },
    {
      "epoch": 0.05407312334602075,
      "grad_norm": 7.623576641082764,
      "learning_rate": 4.909879802884369e-05,
      "loss": 1.8918,
      "step": 53800
    },
    {
      "epoch": 0.05417363101023268,
      "grad_norm": 6.516379356384277,
      "learning_rate": 4.909712290110683e-05,
      "loss": 1.9816,
      "step": 53900
    },
    {
      "epoch": 0.05427413867444462,
      "grad_norm": 7.224794864654541,
      "learning_rate": 4.909544777336996e-05,
      "loss": 2.0918,
      "step": 54000
    },
    {
      "epoch": 0.05437464633865655,
      "grad_norm": 9.377969741821289,
      "learning_rate": 4.9093772645633094e-05,
      "loss": 2.0717,
      "step": 54100
    },
    {
      "epoch": 0.05447515400286849,
      "grad_norm": 7.553059101104736,
      "learning_rate": 4.909209751789623e-05,
      "loss": 1.9031,
      "step": 54200
    },
    {
      "epoch": 0.05457566166708042,
      "grad_norm": 5.728347301483154,
      "learning_rate": 4.909042239015936e-05,
      "loss": 1.9618,
      "step": 54300
    },
    {
      "epoch": 0.05467616933129236,
      "grad_norm": 6.200674057006836,
      "learning_rate": 4.9088747262422494e-05,
      "loss": 1.9818,
      "step": 54400
    },
    {
      "epoch": 0.05477667699550429,
      "grad_norm": 5.139785289764404,
      "learning_rate": 4.9087072134685635e-05,
      "loss": 1.9671,
      "step": 54500
    },
    {
      "epoch": 0.05487718465971623,
      "grad_norm": 6.856826305389404,
      "learning_rate": 4.908539700694877e-05,
      "loss": 1.9664,
      "step": 54600
    },
    {
      "epoch": 0.054977692323928164,
      "grad_norm": 6.418473720550537,
      "learning_rate": 4.90837218792119e-05,
      "loss": 1.9954,
      "step": 54700
    },
    {
      "epoch": 0.0550781999881401,
      "grad_norm": 6.57800817489624,
      "learning_rate": 4.9082046751475035e-05,
      "loss": 2.0208,
      "step": 54800
    },
    {
      "epoch": 0.05517870765235203,
      "grad_norm": 5.388638019561768,
      "learning_rate": 4.908037162373817e-05,
      "loss": 1.9713,
      "step": 54900
    },
    {
      "epoch": 0.05527921531656396,
      "grad_norm": 8.044857025146484,
      "learning_rate": 4.90786964960013e-05,
      "loss": 2.0049,
      "step": 55000
    },
    {
      "epoch": 0.0553797229807759,
      "grad_norm": 6.780516147613525,
      "learning_rate": 4.907702136826444e-05,
      "loss": 1.9467,
      "step": 55100
    },
    {
      "epoch": 0.05548023064498783,
      "grad_norm": 6.978885650634766,
      "learning_rate": 4.9075346240527575e-05,
      "loss": 1.9371,
      "step": 55200
    },
    {
      "epoch": 0.05558073830919977,
      "grad_norm": 7.005418300628662,
      "learning_rate": 4.907367111279071e-05,
      "loss": 1.8885,
      "step": 55300
    },
    {
      "epoch": 0.0556812459734117,
      "grad_norm": 7.585089206695557,
      "learning_rate": 4.907199598505384e-05,
      "loss": 2.0052,
      "step": 55400
    },
    {
      "epoch": 0.05578175363762364,
      "grad_norm": 7.827527046203613,
      "learning_rate": 4.9070320857316976e-05,
      "loss": 1.9983,
      "step": 55500
    },
    {
      "epoch": 0.05588226130183557,
      "grad_norm": 6.882386207580566,
      "learning_rate": 4.906864572958011e-05,
      "loss": 1.9866,
      "step": 55600
    },
    {
      "epoch": 0.05598276896604751,
      "grad_norm": 7.036718368530273,
      "learning_rate": 4.906697060184325e-05,
      "loss": 1.9682,
      "step": 55700
    },
    {
      "epoch": 0.05608327663025944,
      "grad_norm": 7.435359001159668,
      "learning_rate": 4.906529547410638e-05,
      "loss": 1.9229,
      "step": 55800
    },
    {
      "epoch": 0.05618378429447138,
      "grad_norm": 9.805089950561523,
      "learning_rate": 4.9063620346369516e-05,
      "loss": 1.9969,
      "step": 55900
    },
    {
      "epoch": 0.05628429195868331,
      "grad_norm": 6.7416181564331055,
      "learning_rate": 4.906194521863265e-05,
      "loss": 1.9695,
      "step": 56000
    },
    {
      "epoch": 0.05638479962289524,
      "grad_norm": 6.433270454406738,
      "learning_rate": 4.906027009089578e-05,
      "loss": 1.8504,
      "step": 56100
    },
    {
      "epoch": 0.05648530728710718,
      "grad_norm": 7.829600811004639,
      "learning_rate": 4.9058594963158916e-05,
      "loss": 1.8789,
      "step": 56200
    },
    {
      "epoch": 0.05658581495131911,
      "grad_norm": 14.623351097106934,
      "learning_rate": 4.9056919835422057e-05,
      "loss": 1.9903,
      "step": 56300
    },
    {
      "epoch": 0.05668632261553105,
      "grad_norm": 5.333030700683594,
      "learning_rate": 4.905524470768519e-05,
      "loss": 1.8583,
      "step": 56400
    },
    {
      "epoch": 0.05678683027974298,
      "grad_norm": 8.41947078704834,
      "learning_rate": 4.9053569579948323e-05,
      "loss": 2.0263,
      "step": 56500
    },
    {
      "epoch": 0.05688733794395492,
      "grad_norm": 7.02418327331543,
      "learning_rate": 4.905189445221146e-05,
      "loss": 1.9636,
      "step": 56600
    },
    {
      "epoch": 0.05698784560816685,
      "grad_norm": 6.0085954666137695,
      "learning_rate": 4.905021932447459e-05,
      "loss": 1.9033,
      "step": 56700
    },
    {
      "epoch": 0.05708835327237879,
      "grad_norm": 5.49366569519043,
      "learning_rate": 4.9048544196737724e-05,
      "loss": 2.0123,
      "step": 56800
    },
    {
      "epoch": 0.05718886093659072,
      "grad_norm": 6.201637268066406,
      "learning_rate": 4.9046869069000864e-05,
      "loss": 1.9969,
      "step": 56900
    },
    {
      "epoch": 0.05728936860080265,
      "grad_norm": 6.439027309417725,
      "learning_rate": 4.9045193941264e-05,
      "loss": 1.8874,
      "step": 57000
    },
    {
      "epoch": 0.057389876265014586,
      "grad_norm": 8.27962589263916,
      "learning_rate": 4.904351881352713e-05,
      "loss": 1.9395,
      "step": 57100
    },
    {
      "epoch": 0.05749038392922652,
      "grad_norm": 5.684893608093262,
      "learning_rate": 4.9041843685790264e-05,
      "loss": 2.0421,
      "step": 57200
    },
    {
      "epoch": 0.057590891593438456,
      "grad_norm": 6.973920822143555,
      "learning_rate": 4.90401685580534e-05,
      "loss": 2.0263,
      "step": 57300
    },
    {
      "epoch": 0.05769139925765039,
      "grad_norm": 6.100912094116211,
      "learning_rate": 4.903849343031653e-05,
      "loss": 2.0288,
      "step": 57400
    },
    {
      "epoch": 0.05779190692186233,
      "grad_norm": 8.597681045532227,
      "learning_rate": 4.903681830257967e-05,
      "loss": 1.9526,
      "step": 57500
    },
    {
      "epoch": 0.05789241458607426,
      "grad_norm": 6.47206449508667,
      "learning_rate": 4.9035143174842805e-05,
      "loss": 1.9383,
      "step": 57600
    },
    {
      "epoch": 0.0579929222502862,
      "grad_norm": 6.257202625274658,
      "learning_rate": 4.903346804710594e-05,
      "loss": 1.9985,
      "step": 57700
    },
    {
      "epoch": 0.05809342991449813,
      "grad_norm": 6.689820766448975,
      "learning_rate": 4.903179291936907e-05,
      "loss": 1.9584,
      "step": 57800
    },
    {
      "epoch": 0.05819393757871007,
      "grad_norm": 7.034118175506592,
      "learning_rate": 4.9030117791632205e-05,
      "loss": 2.0749,
      "step": 57900
    },
    {
      "epoch": 0.058294445242922,
      "grad_norm": 8.599583625793457,
      "learning_rate": 4.902844266389534e-05,
      "loss": 1.9989,
      "step": 58000
    },
    {
      "epoch": 0.05839495290713393,
      "grad_norm": 6.873775005340576,
      "learning_rate": 4.902676753615847e-05,
      "loss": 1.9285,
      "step": 58100
    },
    {
      "epoch": 0.058495460571345866,
      "grad_norm": 5.42852783203125,
      "learning_rate": 4.9025092408421605e-05,
      "loss": 1.88,
      "step": 58200
    },
    {
      "epoch": 0.0585959682355578,
      "grad_norm": 7.826550483703613,
      "learning_rate": 4.902341728068474e-05,
      "loss": 1.9793,
      "step": 58300
    },
    {
      "epoch": 0.058696475899769736,
      "grad_norm": 6.521853446960449,
      "learning_rate": 4.902174215294787e-05,
      "loss": 1.9604,
      "step": 58400
    },
    {
      "epoch": 0.05879698356398167,
      "grad_norm": 8.46878719329834,
      "learning_rate": 4.9020067025211005e-05,
      "loss": 1.9394,
      "step": 58500
    },
    {
      "epoch": 0.058897491228193606,
      "grad_norm": 7.90887975692749,
      "learning_rate": 4.901839189747414e-05,
      "loss": 1.9532,
      "step": 58600
    },
    {
      "epoch": 0.05899799889240554,
      "grad_norm": 6.9323954582214355,
      "learning_rate": 4.901671676973728e-05,
      "loss": 1.986,
      "step": 58700
    },
    {
      "epoch": 0.059098506556617476,
      "grad_norm": 4.190006732940674,
      "learning_rate": 4.901504164200041e-05,
      "loss": 1.877,
      "step": 58800
    },
    {
      "epoch": 0.05919901422082941,
      "grad_norm": 7.068341255187988,
      "learning_rate": 4.9013366514263546e-05,
      "loss": 1.9862,
      "step": 58900
    },
    {
      "epoch": 0.05929952188504135,
      "grad_norm": 5.893054008483887,
      "learning_rate": 4.901169138652668e-05,
      "loss": 1.9871,
      "step": 59000
    },
    {
      "epoch": 0.05940002954925328,
      "grad_norm": 7.668826580047607,
      "learning_rate": 4.901001625878981e-05,
      "loss": 1.9791,
      "step": 59100
    },
    {
      "epoch": 0.05950053721346521,
      "grad_norm": 6.647095680236816,
      "learning_rate": 4.9008341131052946e-05,
      "loss": 1.8849,
      "step": 59200
    },
    {
      "epoch": 0.059601044877677145,
      "grad_norm": 6.874995708465576,
      "learning_rate": 4.9006666003316086e-05,
      "loss": 1.9254,
      "step": 59300
    },
    {
      "epoch": 0.05970155254188908,
      "grad_norm": 6.034256458282471,
      "learning_rate": 4.900499087557922e-05,
      "loss": 2.0088,
      "step": 59400
    },
    {
      "epoch": 0.059802060206101015,
      "grad_norm": 7.221602916717529,
      "learning_rate": 4.900331574784235e-05,
      "loss": 1.9762,
      "step": 59500
    },
    {
      "epoch": 0.05990256787031295,
      "grad_norm": 8.086633682250977,
      "learning_rate": 4.900164062010549e-05,
      "loss": 1.9999,
      "step": 59600
    },
    {
      "epoch": 0.060003075534524886,
      "grad_norm": 6.835814952850342,
      "learning_rate": 4.899996549236862e-05,
      "loss": 1.9758,
      "step": 59700
    },
    {
      "epoch": 0.06010358319873682,
      "grad_norm": 5.400475978851318,
      "learning_rate": 4.8998290364631754e-05,
      "loss": 2.0579,
      "step": 59800
    },
    {
      "epoch": 0.060204090862948756,
      "grad_norm": 6.824014663696289,
      "learning_rate": 4.8996615236894894e-05,
      "loss": 1.9679,
      "step": 59900
    },
    {
      "epoch": 0.06030459852716069,
      "grad_norm": 5.831616401672363,
      "learning_rate": 4.899494010915803e-05,
      "loss": 1.9621,
      "step": 60000
    },
    {
      "epoch": 0.060405106191372626,
      "grad_norm": 8.396175384521484,
      "learning_rate": 4.899326498142116e-05,
      "loss": 2.003,
      "step": 60100
    },
    {
      "epoch": 0.060505613855584554,
      "grad_norm": 6.84444522857666,
      "learning_rate": 4.8991589853684294e-05,
      "loss": 1.9562,
      "step": 60200
    },
    {
      "epoch": 0.06060612151979649,
      "grad_norm": 5.999375820159912,
      "learning_rate": 4.898991472594743e-05,
      "loss": 1.9034,
      "step": 60300
    },
    {
      "epoch": 0.060706629184008425,
      "grad_norm": 6.82354211807251,
      "learning_rate": 4.898823959821056e-05,
      "loss": 1.986,
      "step": 60400
    },
    {
      "epoch": 0.06080713684822036,
      "grad_norm": 9.209763526916504,
      "learning_rate": 4.89865644704737e-05,
      "loss": 1.9365,
      "step": 60500
    },
    {
      "epoch": 0.060907644512432295,
      "grad_norm": 6.068411350250244,
      "learning_rate": 4.8984889342736835e-05,
      "loss": 1.9549,
      "step": 60600
    },
    {
      "epoch": 0.06100815217664423,
      "grad_norm": 6.657948970794678,
      "learning_rate": 4.898321421499997e-05,
      "loss": 2.0645,
      "step": 60700
    },
    {
      "epoch": 0.061108659840856165,
      "grad_norm": 6.830615043640137,
      "learning_rate": 4.89815390872631e-05,
      "loss": 1.9446,
      "step": 60800
    },
    {
      "epoch": 0.0612091675050681,
      "grad_norm": 7.749054431915283,
      "learning_rate": 4.8979863959526235e-05,
      "loss": 1.9112,
      "step": 60900
    },
    {
      "epoch": 0.061309675169280035,
      "grad_norm": 5.141472816467285,
      "learning_rate": 4.897818883178937e-05,
      "loss": 2.0418,
      "step": 61000
    },
    {
      "epoch": 0.06141018283349197,
      "grad_norm": 6.756884574890137,
      "learning_rate": 4.897651370405251e-05,
      "loss": 1.9275,
      "step": 61100
    },
    {
      "epoch": 0.061510690497703906,
      "grad_norm": 9.002732276916504,
      "learning_rate": 4.897483857631564e-05,
      "loss": 2.0448,
      "step": 61200
    },
    {
      "epoch": 0.061611198161915834,
      "grad_norm": 7.04788875579834,
      "learning_rate": 4.8973163448578775e-05,
      "loss": 2.0287,
      "step": 61300
    },
    {
      "epoch": 0.06171170582612777,
      "grad_norm": 5.649333953857422,
      "learning_rate": 4.897148832084191e-05,
      "loss": 1.8815,
      "step": 61400
    },
    {
      "epoch": 0.061812213490339704,
      "grad_norm": 5.476451396942139,
      "learning_rate": 4.896981319310504e-05,
      "loss": 1.9393,
      "step": 61500
    },
    {
      "epoch": 0.06191272115455164,
      "grad_norm": 4.060372829437256,
      "learning_rate": 4.8968138065368176e-05,
      "loss": 1.9623,
      "step": 61600
    },
    {
      "epoch": 0.062013228818763574,
      "grad_norm": 4.96203088760376,
      "learning_rate": 4.8966462937631316e-05,
      "loss": 2.0107,
      "step": 61700
    },
    {
      "epoch": 0.06211373648297551,
      "grad_norm": 6.879908084869385,
      "learning_rate": 4.896478780989445e-05,
      "loss": 2.0157,
      "step": 61800
    },
    {
      "epoch": 0.062214244147187445,
      "grad_norm": 5.6392741203308105,
      "learning_rate": 4.896311268215758e-05,
      "loss": 1.9822,
      "step": 61900
    },
    {
      "epoch": 0.06231475181139938,
      "grad_norm": 6.107083797454834,
      "learning_rate": 4.8961437554420716e-05,
      "loss": 1.9376,
      "step": 62000
    },
    {
      "epoch": 0.062415259475611315,
      "grad_norm": 9.783181190490723,
      "learning_rate": 4.895976242668385e-05,
      "loss": 2.0925,
      "step": 62100
    },
    {
      "epoch": 0.06251576713982325,
      "grad_norm": 6.465400695800781,
      "learning_rate": 4.895808729894699e-05,
      "loss": 1.8971,
      "step": 62200
    },
    {
      "epoch": 0.06261627480403519,
      "grad_norm": 7.29615592956543,
      "learning_rate": 4.895641217121012e-05,
      "loss": 1.9487,
      "step": 62300
    },
    {
      "epoch": 0.06271678246824712,
      "grad_norm": 6.685301303863525,
      "learning_rate": 4.8954737043473256e-05,
      "loss": 1.892,
      "step": 62400
    },
    {
      "epoch": 0.06281729013245906,
      "grad_norm": 8.167073249816895,
      "learning_rate": 4.895306191573639e-05,
      "loss": 2.0266,
      "step": 62500
    },
    {
      "epoch": 0.06291779779667099,
      "grad_norm": 6.266855716705322,
      "learning_rate": 4.895138678799952e-05,
      "loss": 1.8724,
      "step": 62600
    },
    {
      "epoch": 0.06301830546088293,
      "grad_norm": 5.375868797302246,
      "learning_rate": 4.894971166026266e-05,
      "loss": 2.0021,
      "step": 62700
    },
    {
      "epoch": 0.06311881312509486,
      "grad_norm": 7.105795860290527,
      "learning_rate": 4.894803653252579e-05,
      "loss": 1.9836,
      "step": 62800
    },
    {
      "epoch": 0.06321932078930678,
      "grad_norm": 6.070801258087158,
      "learning_rate": 4.8946361404788924e-05,
      "loss": 1.8558,
      "step": 62900
    },
    {
      "epoch": 0.06331982845351872,
      "grad_norm": 7.839402198791504,
      "learning_rate": 4.894468627705206e-05,
      "loss": 2.0024,
      "step": 63000
    },
    {
      "epoch": 0.06342033611773065,
      "grad_norm": 7.2162556648254395,
      "learning_rate": 4.894301114931519e-05,
      "loss": 1.965,
      "step": 63100
    },
    {
      "epoch": 0.06352084378194259,
      "grad_norm": 6.369053840637207,
      "learning_rate": 4.8941336021578324e-05,
      "loss": 1.9386,
      "step": 63200
    },
    {
      "epoch": 0.06362135144615452,
      "grad_norm": 7.7506303787231445,
      "learning_rate": 4.893966089384146e-05,
      "loss": 2.0021,
      "step": 63300
    },
    {
      "epoch": 0.06372185911036646,
      "grad_norm": 5.276858329772949,
      "learning_rate": 4.89379857661046e-05,
      "loss": 1.9541,
      "step": 63400
    },
    {
      "epoch": 0.06382236677457839,
      "grad_norm": 6.513329982757568,
      "learning_rate": 4.893631063836773e-05,
      "loss": 2.0648,
      "step": 63500
    },
    {
      "epoch": 0.06392287443879033,
      "grad_norm": 10.14943790435791,
      "learning_rate": 4.8934635510630864e-05,
      "loss": 2.0003,
      "step": 63600
    },
    {
      "epoch": 0.06402338210300226,
      "grad_norm": 5.442229270935059,
      "learning_rate": 4.8932960382894e-05,
      "loss": 1.9462,
      "step": 63700
    },
    {
      "epoch": 0.0641238897672142,
      "grad_norm": 6.3286848068237305,
      "learning_rate": 4.893128525515713e-05,
      "loss": 1.9206,
      "step": 63800
    },
    {
      "epoch": 0.06422439743142613,
      "grad_norm": 7.503147125244141,
      "learning_rate": 4.8929610127420265e-05,
      "loss": 1.955,
      "step": 63900
    },
    {
      "epoch": 0.06432490509563807,
      "grad_norm": 4.701399803161621,
      "learning_rate": 4.8927934999683405e-05,
      "loss": 2.0557,
      "step": 64000
    },
    {
      "epoch": 0.06442541275985,
      "grad_norm": 7.5960001945495605,
      "learning_rate": 4.892625987194654e-05,
      "loss": 1.9676,
      "step": 64100
    },
    {
      "epoch": 0.06452592042406194,
      "grad_norm": 6.761974334716797,
      "learning_rate": 4.892458474420967e-05,
      "loss": 2.0984,
      "step": 64200
    },
    {
      "epoch": 0.06462642808827387,
      "grad_norm": 5.739292144775391,
      "learning_rate": 4.8922909616472805e-05,
      "loss": 2.0091,
      "step": 64300
    },
    {
      "epoch": 0.06472693575248581,
      "grad_norm": 6.815933704376221,
      "learning_rate": 4.892123448873594e-05,
      "loss": 1.9581,
      "step": 64400
    },
    {
      "epoch": 0.06482744341669774,
      "grad_norm": 2.3980979919433594,
      "learning_rate": 4.891955936099907e-05,
      "loss": 1.9479,
      "step": 64500
    },
    {
      "epoch": 0.06492795108090968,
      "grad_norm": 7.0070929527282715,
      "learning_rate": 4.891788423326221e-05,
      "loss": 2.0358,
      "step": 64600
    },
    {
      "epoch": 0.06502845874512161,
      "grad_norm": 7.415131568908691,
      "learning_rate": 4.8916209105525346e-05,
      "loss": 1.9475,
      "step": 64700
    },
    {
      "epoch": 0.06512896640933355,
      "grad_norm": 4.896481990814209,
      "learning_rate": 4.891453397778848e-05,
      "loss": 1.9635,
      "step": 64800
    },
    {
      "epoch": 0.06522947407354548,
      "grad_norm": 7.714511394500732,
      "learning_rate": 4.891285885005161e-05,
      "loss": 1.9167,
      "step": 64900
    },
    {
      "epoch": 0.0653299817377574,
      "grad_norm": 6.412282943725586,
      "learning_rate": 4.8911183722314746e-05,
      "loss": 1.9528,
      "step": 65000
    },
    {
      "epoch": 0.06543048940196934,
      "grad_norm": 5.688710689544678,
      "learning_rate": 4.890950859457788e-05,
      "loss": 1.9969,
      "step": 65100
    },
    {
      "epoch": 0.06553099706618128,
      "grad_norm": 5.7586445808410645,
      "learning_rate": 4.890783346684102e-05,
      "loss": 1.9502,
      "step": 65200
    },
    {
      "epoch": 0.06563150473039321,
      "grad_norm": 8.738232612609863,
      "learning_rate": 4.890615833910415e-05,
      "loss": 1.9032,
      "step": 65300
    },
    {
      "epoch": 0.06573201239460515,
      "grad_norm": 5.593501091003418,
      "learning_rate": 4.8904483211367286e-05,
      "loss": 2.0039,
      "step": 65400
    },
    {
      "epoch": 0.06583252005881708,
      "grad_norm": 7.53011417388916,
      "learning_rate": 4.890280808363042e-05,
      "loss": 1.9831,
      "step": 65500
    },
    {
      "epoch": 0.06593302772302902,
      "grad_norm": 8.340636253356934,
      "learning_rate": 4.890113295589355e-05,
      "loss": 1.9815,
      "step": 65600
    },
    {
      "epoch": 0.06603353538724095,
      "grad_norm": 7.518121719360352,
      "learning_rate": 4.8899457828156687e-05,
      "loss": 1.9128,
      "step": 65700
    },
    {
      "epoch": 0.06613404305145289,
      "grad_norm": 7.506353378295898,
      "learning_rate": 4.889778270041983e-05,
      "loss": 1.9829,
      "step": 65800
    },
    {
      "epoch": 0.06623455071566482,
      "grad_norm": 5.605213642120361,
      "learning_rate": 4.889610757268296e-05,
      "loss": 1.9813,
      "step": 65900
    },
    {
      "epoch": 0.06633505837987676,
      "grad_norm": 5.902741432189941,
      "learning_rate": 4.8894432444946094e-05,
      "loss": 1.9473,
      "step": 66000
    },
    {
      "epoch": 0.06643556604408869,
      "grad_norm": 7.210864067077637,
      "learning_rate": 4.889275731720923e-05,
      "loss": 2.0129,
      "step": 66100
    },
    {
      "epoch": 0.06653607370830063,
      "grad_norm": 7.563469886779785,
      "learning_rate": 4.889108218947236e-05,
      "loss": 1.9964,
      "step": 66200
    },
    {
      "epoch": 0.06663658137251256,
      "grad_norm": 6.309096813201904,
      "learning_rate": 4.8889407061735494e-05,
      "loss": 1.8798,
      "step": 66300
    },
    {
      "epoch": 0.0667370890367245,
      "grad_norm": 9.089079856872559,
      "learning_rate": 4.8887731933998634e-05,
      "loss": 1.994,
      "step": 66400
    },
    {
      "epoch": 0.06683759670093643,
      "grad_norm": 6.3759565353393555,
      "learning_rate": 4.888605680626177e-05,
      "loss": 1.9874,
      "step": 66500
    },
    {
      "epoch": 0.06693810436514837,
      "grad_norm": 7.260879039764404,
      "learning_rate": 4.88843816785249e-05,
      "loss": 1.9548,
      "step": 66600
    },
    {
      "epoch": 0.0670386120293603,
      "grad_norm": 9.113238334655762,
      "learning_rate": 4.8882706550788034e-05,
      "loss": 1.9796,
      "step": 66700
    },
    {
      "epoch": 0.06713911969357224,
      "grad_norm": 5.683205604553223,
      "learning_rate": 4.888103142305117e-05,
      "loss": 2.0287,
      "step": 66800
    },
    {
      "epoch": 0.06723962735778417,
      "grad_norm": 7.671689510345459,
      "learning_rate": 4.88793562953143e-05,
      "loss": 1.9667,
      "step": 66900
    },
    {
      "epoch": 0.06734013502199611,
      "grad_norm": 5.9740753173828125,
      "learning_rate": 4.887768116757744e-05,
      "loss": 1.9673,
      "step": 67000
    },
    {
      "epoch": 0.06744064268620803,
      "grad_norm": 6.26717472076416,
      "learning_rate": 4.8876006039840575e-05,
      "loss": 2.0021,
      "step": 67100
    },
    {
      "epoch": 0.06754115035041997,
      "grad_norm": 8.35313606262207,
      "learning_rate": 4.887433091210371e-05,
      "loss": 1.9214,
      "step": 67200
    },
    {
      "epoch": 0.0676416580146319,
      "grad_norm": 7.4932050704956055,
      "learning_rate": 4.887265578436684e-05,
      "loss": 1.8803,
      "step": 67300
    },
    {
      "epoch": 0.06774216567884384,
      "grad_norm": 3.9514753818511963,
      "learning_rate": 4.8870980656629975e-05,
      "loss": 1.931,
      "step": 67400
    },
    {
      "epoch": 0.06784267334305577,
      "grad_norm": 4.364475727081299,
      "learning_rate": 4.886930552889311e-05,
      "loss": 1.9739,
      "step": 67500
    },
    {
      "epoch": 0.0679431810072677,
      "grad_norm": 4.552826404571533,
      "learning_rate": 4.886763040115624e-05,
      "loss": 1.8324,
      "step": 67600
    },
    {
      "epoch": 0.06804368867147964,
      "grad_norm": 6.092297077178955,
      "learning_rate": 4.8865955273419375e-05,
      "loss": 1.9012,
      "step": 67700
    },
    {
      "epoch": 0.06814419633569158,
      "grad_norm": 5.0324249267578125,
      "learning_rate": 4.886428014568251e-05,
      "loss": 1.8735,
      "step": 67800
    },
    {
      "epoch": 0.06824470399990351,
      "grad_norm": 7.522434711456299,
      "learning_rate": 4.886260501794564e-05,
      "loss": 1.8622,
      "step": 67900
    },
    {
      "epoch": 0.06834521166411545,
      "grad_norm": 6.54075813293457,
      "learning_rate": 4.8860929890208776e-05,
      "loss": 1.9246,
      "step": 68000
    },
    {
      "epoch": 0.06844571932832738,
      "grad_norm": 8.382461547851562,
      "learning_rate": 4.885925476247191e-05,
      "loss": 1.9716,
      "step": 68100
    },
    {
      "epoch": 0.06854622699253932,
      "grad_norm": 6.329591274261475,
      "learning_rate": 4.885757963473505e-05,
      "loss": 1.948,
      "step": 68200
    },
    {
      "epoch": 0.06864673465675125,
      "grad_norm": 7.826361179351807,
      "learning_rate": 4.885590450699818e-05,
      "loss": 2.0744,
      "step": 68300
    },
    {
      "epoch": 0.06874724232096319,
      "grad_norm": 7.608278751373291,
      "learning_rate": 4.8854229379261316e-05,
      "loss": 1.9764,
      "step": 68400
    },
    {
      "epoch": 0.06884774998517512,
      "grad_norm": 5.587988376617432,
      "learning_rate": 4.885255425152445e-05,
      "loss": 2.0184,
      "step": 68500
    },
    {
      "epoch": 0.06894825764938706,
      "grad_norm": 6.241646766662598,
      "learning_rate": 4.885087912378758e-05,
      "loss": 2.0111,
      "step": 68600
    },
    {
      "epoch": 0.06904876531359899,
      "grad_norm": 7.555324554443359,
      "learning_rate": 4.8849203996050716e-05,
      "loss": 1.9562,
      "step": 68700
    },
    {
      "epoch": 0.06914927297781093,
      "grad_norm": 8.267462730407715,
      "learning_rate": 4.8847528868313857e-05,
      "loss": 2.0434,
      "step": 68800
    },
    {
      "epoch": 0.06924978064202286,
      "grad_norm": 7.401355743408203,
      "learning_rate": 4.884585374057699e-05,
      "loss": 2.0569,
      "step": 68900
    },
    {
      "epoch": 0.0693502883062348,
      "grad_norm": 6.312142848968506,
      "learning_rate": 4.8844178612840123e-05,
      "loss": 1.932,
      "step": 69000
    },
    {
      "epoch": 0.06945079597044673,
      "grad_norm": 7.053755760192871,
      "learning_rate": 4.884250348510326e-05,
      "loss": 1.9935,
      "step": 69100
    },
    {
      "epoch": 0.06955130363465867,
      "grad_norm": 4.4601969718933105,
      "learning_rate": 4.884082835736639e-05,
      "loss": 2.0521,
      "step": 69200
    },
    {
      "epoch": 0.06965181129887059,
      "grad_norm": 7.32342529296875,
      "learning_rate": 4.8839153229629524e-05,
      "loss": 2.0055,
      "step": 69300
    },
    {
      "epoch": 0.06975231896308252,
      "grad_norm": 6.065074443817139,
      "learning_rate": 4.8837478101892664e-05,
      "loss": 2.0202,
      "step": 69400
    },
    {
      "epoch": 0.06985282662729446,
      "grad_norm": 6.526333332061768,
      "learning_rate": 4.88358029741558e-05,
      "loss": 1.9638,
      "step": 69500
    },
    {
      "epoch": 0.0699533342915064,
      "grad_norm": 4.626415252685547,
      "learning_rate": 4.883412784641893e-05,
      "loss": 1.9742,
      "step": 69600
    },
    {
      "epoch": 0.07005384195571833,
      "grad_norm": 6.693112850189209,
      "learning_rate": 4.8832452718682064e-05,
      "loss": 2.0218,
      "step": 69700
    },
    {
      "epoch": 0.07015434961993026,
      "grad_norm": 6.636335849761963,
      "learning_rate": 4.88307775909452e-05,
      "loss": 1.9143,
      "step": 69800
    },
    {
      "epoch": 0.0702548572841422,
      "grad_norm": 5.859709739685059,
      "learning_rate": 4.882910246320833e-05,
      "loss": 1.9934,
      "step": 69900
    },
    {
      "epoch": 0.07035536494835413,
      "grad_norm": 5.9951019287109375,
      "learning_rate": 4.882742733547147e-05,
      "loss": 2.0442,
      "step": 70000
    },
    {
      "epoch": 0.07045587261256607,
      "grad_norm": 4.711256980895996,
      "learning_rate": 4.8825752207734605e-05,
      "loss": 1.9686,
      "step": 70100
    },
    {
      "epoch": 0.070556380276778,
      "grad_norm": 6.1066083908081055,
      "learning_rate": 4.882407707999774e-05,
      "loss": 2.0053,
      "step": 70200
    },
    {
      "epoch": 0.07065688794098994,
      "grad_norm": 8.566339492797852,
      "learning_rate": 4.882240195226087e-05,
      "loss": 1.973,
      "step": 70300
    },
    {
      "epoch": 0.07075739560520188,
      "grad_norm": 6.797321319580078,
      "learning_rate": 4.8820726824524005e-05,
      "loss": 1.9677,
      "step": 70400
    },
    {
      "epoch": 0.07085790326941381,
      "grad_norm": 8.915377616882324,
      "learning_rate": 4.881905169678714e-05,
      "loss": 1.9823,
      "step": 70500
    },
    {
      "epoch": 0.07095841093362575,
      "grad_norm": 7.13493537902832,
      "learning_rate": 4.881737656905028e-05,
      "loss": 2.0759,
      "step": 70600
    },
    {
      "epoch": 0.07105891859783768,
      "grad_norm": 7.598613739013672,
      "learning_rate": 4.881570144131341e-05,
      "loss": 1.972,
      "step": 70700
    },
    {
      "epoch": 0.07115942626204962,
      "grad_norm": 4.954408645629883,
      "learning_rate": 4.8814026313576545e-05,
      "loss": 1.9122,
      "step": 70800
    },
    {
      "epoch": 0.07125993392626155,
      "grad_norm": 6.013726711273193,
      "learning_rate": 4.881235118583968e-05,
      "loss": 1.9123,
      "step": 70900
    },
    {
      "epoch": 0.07136044159047349,
      "grad_norm": 6.089282989501953,
      "learning_rate": 4.881067605810281e-05,
      "loss": 2.0027,
      "step": 71000
    },
    {
      "epoch": 0.07146094925468542,
      "grad_norm": 4.941153526306152,
      "learning_rate": 4.880900093036595e-05,
      "loss": 2.0086,
      "step": 71100
    },
    {
      "epoch": 0.07156145691889736,
      "grad_norm": 8.006290435791016,
      "learning_rate": 4.8807325802629086e-05,
      "loss": 1.9281,
      "step": 71200
    },
    {
      "epoch": 0.07166196458310929,
      "grad_norm": 6.977761268615723,
      "learning_rate": 4.880565067489222e-05,
      "loss": 2.0242,
      "step": 71300
    },
    {
      "epoch": 0.07176247224732121,
      "grad_norm": 4.071716785430908,
      "learning_rate": 4.880397554715535e-05,
      "loss": 1.9394,
      "step": 71400
    },
    {
      "epoch": 0.07186297991153315,
      "grad_norm": 8.876855850219727,
      "learning_rate": 4.8802300419418486e-05,
      "loss": 1.9278,
      "step": 71500
    },
    {
      "epoch": 0.07196348757574508,
      "grad_norm": 4.7765116691589355,
      "learning_rate": 4.880062529168162e-05,
      "loss": 2.0779,
      "step": 71600
    },
    {
      "epoch": 0.07206399523995702,
      "grad_norm": 5.697664737701416,
      "learning_rate": 4.879895016394476e-05,
      "loss": 1.8405,
      "step": 71700
    },
    {
      "epoch": 0.07216450290416895,
      "grad_norm": 5.6298065185546875,
      "learning_rate": 4.879727503620789e-05,
      "loss": 2.0221,
      "step": 71800
    },
    {
      "epoch": 0.07226501056838089,
      "grad_norm": 5.980556964874268,
      "learning_rate": 4.879559990847103e-05,
      "loss": 2.0834,
      "step": 71900
    },
    {
      "epoch": 0.07236551823259282,
      "grad_norm": 6.445398807525635,
      "learning_rate": 4.879392478073416e-05,
      "loss": 1.9894,
      "step": 72000
    },
    {
      "epoch": 0.07246602589680476,
      "grad_norm": 6.762855529785156,
      "learning_rate": 4.8792249652997293e-05,
      "loss": 1.9409,
      "step": 72100
    },
    {
      "epoch": 0.0725665335610167,
      "grad_norm": 8.249570846557617,
      "learning_rate": 4.879057452526043e-05,
      "loss": 1.925,
      "step": 72200
    },
    {
      "epoch": 0.07266704122522863,
      "grad_norm": 7.355729103088379,
      "learning_rate": 4.878889939752356e-05,
      "loss": 1.9934,
      "step": 72300
    },
    {
      "epoch": 0.07276754888944056,
      "grad_norm": 6.342062950134277,
      "learning_rate": 4.8787224269786694e-05,
      "loss": 1.9457,
      "step": 72400
    },
    {
      "epoch": 0.0728680565536525,
      "grad_norm": 4.997710704803467,
      "learning_rate": 4.878554914204983e-05,
      "loss": 1.9934,
      "step": 72500
    },
    {
      "epoch": 0.07296856421786443,
      "grad_norm": 7.574533939361572,
      "learning_rate": 4.878387401431296e-05,
      "loss": 1.8852,
      "step": 72600
    },
    {
      "epoch": 0.07306907188207637,
      "grad_norm": 6.779788970947266,
      "learning_rate": 4.8782198886576094e-05,
      "loss": 1.9604,
      "step": 72700
    },
    {
      "epoch": 0.0731695795462883,
      "grad_norm": 6.35487174987793,
      "learning_rate": 4.878052375883923e-05,
      "loss": 1.886,
      "step": 72800
    },
    {
      "epoch": 0.07327008721050024,
      "grad_norm": 4.929390907287598,
      "learning_rate": 4.877884863110237e-05,
      "loss": 2.0336,
      "step": 72900
    },
    {
      "epoch": 0.07337059487471218,
      "grad_norm": 8.326679229736328,
      "learning_rate": 4.87771735033655e-05,
      "loss": 1.985,
      "step": 73000
    },
    {
      "epoch": 0.07347110253892411,
      "grad_norm": 6.666865348815918,
      "learning_rate": 4.8775498375628635e-05,
      "loss": 2.0509,
      "step": 73100
    },
    {
      "epoch": 0.07357161020313605,
      "grad_norm": 5.964995861053467,
      "learning_rate": 4.877382324789177e-05,
      "loss": 1.902,
      "step": 73200
    },
    {
      "epoch": 0.07367211786734798,
      "grad_norm": 7.069206237792969,
      "learning_rate": 4.87721481201549e-05,
      "loss": 1.843,
      "step": 73300
    },
    {
      "epoch": 0.07377262553155992,
      "grad_norm": 5.222026824951172,
      "learning_rate": 4.8770472992418035e-05,
      "loss": 2.0561,
      "step": 73400
    },
    {
      "epoch": 0.07387313319577184,
      "grad_norm": 5.560924053192139,
      "learning_rate": 4.8768797864681175e-05,
      "loss": 1.8944,
      "step": 73500
    },
    {
      "epoch": 0.07397364085998377,
      "grad_norm": 7.786972999572754,
      "learning_rate": 4.876712273694431e-05,
      "loss": 1.8757,
      "step": 73600
    },
    {
      "epoch": 0.07407414852419571,
      "grad_norm": 7.656132221221924,
      "learning_rate": 4.876544760920744e-05,
      "loss": 1.9544,
      "step": 73700
    },
    {
      "epoch": 0.07417465618840764,
      "grad_norm": 6.716036796569824,
      "learning_rate": 4.8763772481470575e-05,
      "loss": 2.0365,
      "step": 73800
    },
    {
      "epoch": 0.07427516385261958,
      "grad_norm": 6.068427562713623,
      "learning_rate": 4.876209735373371e-05,
      "loss": 1.8787,
      "step": 73900
    },
    {
      "epoch": 0.07437567151683151,
      "grad_norm": 7.682361125946045,
      "learning_rate": 4.876042222599684e-05,
      "loss": 2.0108,
      "step": 74000
    },
    {
      "epoch": 0.07447617918104345,
      "grad_norm": 4.340372562408447,
      "learning_rate": 4.875874709825998e-05,
      "loss": 1.921,
      "step": 74100
    },
    {
      "epoch": 0.07457668684525538,
      "grad_norm": 7.405364990234375,
      "learning_rate": 4.8757071970523116e-05,
      "loss": 1.9501,
      "step": 74200
    },
    {
      "epoch": 0.07467719450946732,
      "grad_norm": 3.936028480529785,
      "learning_rate": 4.875539684278625e-05,
      "loss": 1.9049,
      "step": 74300
    },
    {
      "epoch": 0.07477770217367925,
      "grad_norm": 9.39865493774414,
      "learning_rate": 4.875372171504938e-05,
      "loss": 2.0088,
      "step": 74400
    },
    {
      "epoch": 0.07487820983789119,
      "grad_norm": 5.56553840637207,
      "learning_rate": 4.8752046587312516e-05,
      "loss": 1.902,
      "step": 74500
    },
    {
      "epoch": 0.07497871750210312,
      "grad_norm": 5.715391635894775,
      "learning_rate": 4.875037145957565e-05,
      "loss": 1.9632,
      "step": 74600
    },
    {
      "epoch": 0.07507922516631506,
      "grad_norm": 4.99590539932251,
      "learning_rate": 4.874869633183879e-05,
      "loss": 1.9857,
      "step": 74700
    },
    {
      "epoch": 0.075179732830527,
      "grad_norm": 5.247188091278076,
      "learning_rate": 4.874702120410192e-05,
      "loss": 1.9428,
      "step": 74800
    },
    {
      "epoch": 0.07528024049473893,
      "grad_norm": 3.8508150577545166,
      "learning_rate": 4.8745346076365056e-05,
      "loss": 1.8499,
      "step": 74900
    },
    {
      "epoch": 0.07538074815895086,
      "grad_norm": 5.878424644470215,
      "learning_rate": 4.874367094862819e-05,
      "loss": 2.0068,
      "step": 75000
    },
    {
      "epoch": 0.0754812558231628,
      "grad_norm": 6.126941204071045,
      "learning_rate": 4.874199582089132e-05,
      "loss": 1.973,
      "step": 75100
    },
    {
      "epoch": 0.07558176348737473,
      "grad_norm": 5.600993633270264,
      "learning_rate": 4.874032069315446e-05,
      "loss": 1.9196,
      "step": 75200
    },
    {
      "epoch": 0.07568227115158667,
      "grad_norm": 3.400773048400879,
      "learning_rate": 4.87386455654176e-05,
      "loss": 1.9313,
      "step": 75300
    },
    {
      "epoch": 0.0757827788157986,
      "grad_norm": 6.139612674713135,
      "learning_rate": 4.873697043768073e-05,
      "loss": 1.8644,
      "step": 75400
    },
    {
      "epoch": 0.07588328648001054,
      "grad_norm": 7.857072830200195,
      "learning_rate": 4.8735295309943864e-05,
      "loss": 2.1223,
      "step": 75500
    },
    {
      "epoch": 0.07598379414422247,
      "grad_norm": 8.65355110168457,
      "learning_rate": 4.8733620182207e-05,
      "loss": 1.9102,
      "step": 75600
    },
    {
      "epoch": 0.0760843018084344,
      "grad_norm": 5.1608805656433105,
      "learning_rate": 4.873194505447013e-05,
      "loss": 1.7931,
      "step": 75700
    },
    {
      "epoch": 0.07618480947264633,
      "grad_norm": 7.13994026184082,
      "learning_rate": 4.8730269926733264e-05,
      "loss": 1.8574,
      "step": 75800
    },
    {
      "epoch": 0.07628531713685827,
      "grad_norm": 6.411266803741455,
      "learning_rate": 4.8728594798996404e-05,
      "loss": 1.9595,
      "step": 75900
    },
    {
      "epoch": 0.0763858248010702,
      "grad_norm": 5.486778736114502,
      "learning_rate": 4.872691967125954e-05,
      "loss": 1.9698,
      "step": 76000
    },
    {
      "epoch": 0.07648633246528214,
      "grad_norm": 6.606032848358154,
      "learning_rate": 4.872524454352267e-05,
      "loss": 2.0307,
      "step": 76100
    },
    {
      "epoch": 0.07658684012949407,
      "grad_norm": 6.127530097961426,
      "learning_rate": 4.8723569415785805e-05,
      "loss": 2.0287,
      "step": 76200
    },
    {
      "epoch": 0.076687347793706,
      "grad_norm": 8.45871639251709,
      "learning_rate": 4.872189428804894e-05,
      "loss": 1.9268,
      "step": 76300
    },
    {
      "epoch": 0.07678785545791794,
      "grad_norm": 6.82484769821167,
      "learning_rate": 4.872021916031207e-05,
      "loss": 1.8667,
      "step": 76400
    },
    {
      "epoch": 0.07688836312212988,
      "grad_norm": 7.426780700683594,
      "learning_rate": 4.871854403257521e-05,
      "loss": 1.9482,
      "step": 76500
    },
    {
      "epoch": 0.07698887078634181,
      "grad_norm": 5.874197959899902,
      "learning_rate": 4.8716868904838345e-05,
      "loss": 1.9808,
      "step": 76600
    },
    {
      "epoch": 0.07708937845055375,
      "grad_norm": 5.686187267303467,
      "learning_rate": 4.871519377710148e-05,
      "loss": 1.841,
      "step": 76700
    },
    {
      "epoch": 0.07718988611476568,
      "grad_norm": 5.821074485778809,
      "learning_rate": 4.871351864936461e-05,
      "loss": 1.9027,
      "step": 76800
    },
    {
      "epoch": 0.07729039377897762,
      "grad_norm": 5.173367977142334,
      "learning_rate": 4.8711843521627745e-05,
      "loss": 1.9186,
      "step": 76900
    },
    {
      "epoch": 0.07739090144318955,
      "grad_norm": 7.901559829711914,
      "learning_rate": 4.871016839389088e-05,
      "loss": 1.9158,
      "step": 77000
    },
    {
      "epoch": 0.07749140910740149,
      "grad_norm": 8.095697402954102,
      "learning_rate": 4.870849326615401e-05,
      "loss": 1.9533,
      "step": 77100
    },
    {
      "epoch": 0.07759191677161342,
      "grad_norm": 6.394923686981201,
      "learning_rate": 4.8706818138417146e-05,
      "loss": 1.9441,
      "step": 77200
    },
    {
      "epoch": 0.07769242443582536,
      "grad_norm": 7.058408737182617,
      "learning_rate": 4.870514301068028e-05,
      "loss": 1.8844,
      "step": 77300
    },
    {
      "epoch": 0.0777929321000373,
      "grad_norm": 7.6490559577941895,
      "learning_rate": 4.870346788294341e-05,
      "loss": 1.9559,
      "step": 77400
    },
    {
      "epoch": 0.07789343976424923,
      "grad_norm": 6.124887943267822,
      "learning_rate": 4.8701792755206546e-05,
      "loss": 1.9698,
      "step": 77500
    },
    {
      "epoch": 0.07799394742846116,
      "grad_norm": 5.786078929901123,
      "learning_rate": 4.870011762746968e-05,
      "loss": 1.9581,
      "step": 77600
    },
    {
      "epoch": 0.0780944550926731,
      "grad_norm": 6.282103538513184,
      "learning_rate": 4.869844249973282e-05,
      "loss": 1.9785,
      "step": 77700
    },
    {
      "epoch": 0.07819496275688502,
      "grad_norm": 5.113678455352783,
      "learning_rate": 4.869676737199595e-05,
      "loss": 1.8709,
      "step": 77800
    },
    {
      "epoch": 0.07829547042109695,
      "grad_norm": 5.184174060821533,
      "learning_rate": 4.8695092244259086e-05,
      "loss": 1.869,
      "step": 77900
    },
    {
      "epoch": 0.07839597808530889,
      "grad_norm": 5.898503303527832,
      "learning_rate": 4.869341711652222e-05,
      "loss": 1.9429,
      "step": 78000
    },
    {
      "epoch": 0.07849648574952083,
      "grad_norm": 7.555103778839111,
      "learning_rate": 4.869174198878535e-05,
      "loss": 1.9101,
      "step": 78100
    },
    {
      "epoch": 0.07859699341373276,
      "grad_norm": 5.518395900726318,
      "learning_rate": 4.8690066861048487e-05,
      "loss": 1.9495,
      "step": 78200
    },
    {
      "epoch": 0.0786975010779447,
      "grad_norm": 6.866978168487549,
      "learning_rate": 4.868839173331163e-05,
      "loss": 1.9621,
      "step": 78300
    },
    {
      "epoch": 0.07879800874215663,
      "grad_norm": 6.465054035186768,
      "learning_rate": 4.868671660557476e-05,
      "loss": 2.036,
      "step": 78400
    },
    {
      "epoch": 0.07889851640636857,
      "grad_norm": 5.9325690269470215,
      "learning_rate": 4.8685041477837894e-05,
      "loss": 1.8659,
      "step": 78500
    },
    {
      "epoch": 0.0789990240705805,
      "grad_norm": 6.206876277923584,
      "learning_rate": 4.868336635010103e-05,
      "loss": 1.936,
      "step": 78600
    },
    {
      "epoch": 0.07909953173479244,
      "grad_norm": 9.899845123291016,
      "learning_rate": 4.868169122236416e-05,
      "loss": 1.9524,
      "step": 78700
    },
    {
      "epoch": 0.07920003939900437,
      "grad_norm": 4.900125503540039,
      "learning_rate": 4.8680016094627294e-05,
      "loss": 1.9319,
      "step": 78800
    },
    {
      "epoch": 0.0793005470632163,
      "grad_norm": 5.23525857925415,
      "learning_rate": 4.8678340966890434e-05,
      "loss": 1.9626,
      "step": 78900
    },
    {
      "epoch": 0.07940105472742824,
      "grad_norm": 5.571563720703125,
      "learning_rate": 4.867666583915357e-05,
      "loss": 1.894,
      "step": 79000
    },
    {
      "epoch": 0.07950156239164018,
      "grad_norm": 9.119353294372559,
      "learning_rate": 4.86749907114167e-05,
      "loss": 1.9196,
      "step": 79100
    },
    {
      "epoch": 0.07960207005585211,
      "grad_norm": 6.993946552276611,
      "learning_rate": 4.8673315583679834e-05,
      "loss": 1.872,
      "step": 79200
    },
    {
      "epoch": 0.07970257772006405,
      "grad_norm": 5.0582685470581055,
      "learning_rate": 4.867164045594297e-05,
      "loss": 1.8553,
      "step": 79300
    },
    {
      "epoch": 0.07980308538427598,
      "grad_norm": 6.233940124511719,
      "learning_rate": 4.866996532820611e-05,
      "loss": 1.8872,
      "step": 79400
    },
    {
      "epoch": 0.07990359304848792,
      "grad_norm": 5.772041320800781,
      "learning_rate": 4.866829020046924e-05,
      "loss": 1.814,
      "step": 79500
    },
    {
      "epoch": 0.08000410071269985,
      "grad_norm": 5.904325008392334,
      "learning_rate": 4.8666615072732375e-05,
      "loss": 1.8633,
      "step": 79600
    },
    {
      "epoch": 0.08010460837691179,
      "grad_norm": 5.8892388343811035,
      "learning_rate": 4.866493994499551e-05,
      "loss": 1.8591,
      "step": 79700
    },
    {
      "epoch": 0.08020511604112372,
      "grad_norm": 7.0865254402160645,
      "learning_rate": 4.866326481725864e-05,
      "loss": 2.0541,
      "step": 79800
    },
    {
      "epoch": 0.08030562370533564,
      "grad_norm": 4.912364482879639,
      "learning_rate": 4.8661589689521775e-05,
      "loss": 1.9205,
      "step": 79900
    },
    {
      "epoch": 0.08040613136954758,
      "grad_norm": 5.614762783050537,
      "learning_rate": 4.8659914561784915e-05,
      "loss": 1.922,
      "step": 80000
    },
    {
      "epoch": 0.32202655613503806,
      "grad_norm": 3.162633180618286,
      "learning_rate": 4.463295234189794e-05,
      "loss": 1.9367,
      "step": 80100
    },
    {
      "epoch": 0.3224285867918858,
      "grad_norm": 3.2984583377838135,
      "learning_rate": 4.462625182421594e-05,
      "loss": 1.9136,
      "step": 80200
    },
    {
      "epoch": 0.32283061744873354,
      "grad_norm": 3.963890314102173,
      "learning_rate": 4.461955130653395e-05,
      "loss": 1.9599,
      "step": 80300
    },
    {
      "epoch": 0.3232326481055813,
      "grad_norm": 3.068469762802124,
      "learning_rate": 4.461285078885195e-05,
      "loss": 1.9513,
      "step": 80400
    },
    {
      "epoch": 0.323634678762429,
      "grad_norm": 3.2362804412841797,
      "learning_rate": 4.460615027116996e-05,
      "loss": 1.9315,
      "step": 80500
    },
    {
      "epoch": 0.32403670941927676,
      "grad_norm": 3.220399856567383,
      "learning_rate": 4.4599449753487956e-05,
      "loss": 1.9409,
      "step": 80600
    },
    {
      "epoch": 0.3244387400761245,
      "grad_norm": 3.2179203033447266,
      "learning_rate": 4.459274923580596e-05,
      "loss": 1.9089,
      "step": 80700
    },
    {
      "epoch": 0.32484077073297224,
      "grad_norm": 3.1953816413879395,
      "learning_rate": 4.458604871812396e-05,
      "loss": 1.9027,
      "step": 80800
    },
    {
      "epoch": 0.32524280138982,
      "grad_norm": 3.2903172969818115,
      "learning_rate": 4.4579348200441965e-05,
      "loss": 1.9751,
      "step": 80900
    },
    {
      "epoch": 0.3256448320466677,
      "grad_norm": 3.3174169063568115,
      "learning_rate": 4.457264768275997e-05,
      "loss": 1.893,
      "step": 81000
    },
    {
      "epoch": 0.32604686270351546,
      "grad_norm": 3.7747859954833984,
      "learning_rate": 4.4565947165077975e-05,
      "loss": 1.9754,
      "step": 81100
    },
    {
      "epoch": 0.3264488933603632,
      "grad_norm": 2.9509172439575195,
      "learning_rate": 4.455924664739598e-05,
      "loss": 1.9663,
      "step": 81200
    },
    {
      "epoch": 0.32685092401721094,
      "grad_norm": 3.331582546234131,
      "learning_rate": 4.4552546129713985e-05,
      "loss": 1.9491,
      "step": 81300
    },
    {
      "epoch": 0.3272529546740587,
      "grad_norm": 4.352489948272705,
      "learning_rate": 4.454584561203199e-05,
      "loss": 1.9224,
      "step": 81400
    },
    {
      "epoch": 0.3276549853309064,
      "grad_norm": 3.6154932975769043,
      "learning_rate": 4.4539145094349995e-05,
      "loss": 1.9356,
      "step": 81500
    },
    {
      "epoch": 0.32805701598775416,
      "grad_norm": 3.512816905975342,
      "learning_rate": 4.4532444576668e-05,
      "loss": 1.9154,
      "step": 81600
    },
    {
      "epoch": 0.3284590466446019,
      "grad_norm": 3.2448058128356934,
      "learning_rate": 4.4525744058986e-05,
      "loss": 1.9102,
      "step": 81700
    },
    {
      "epoch": 0.32886107730144964,
      "grad_norm": 3.2687041759490967,
      "learning_rate": 4.4519043541304004e-05,
      "loss": 1.9213,
      "step": 81800
    },
    {
      "epoch": 0.3292631079582974,
      "grad_norm": 2.777998208999634,
      "learning_rate": 4.4512343023622e-05,
      "loss": 1.9541,
      "step": 81900
    },
    {
      "epoch": 0.32966513861514507,
      "grad_norm": 3.300359010696411,
      "learning_rate": 4.450564250594001e-05,
      "loss": 1.9196,
      "step": 82000
    },
    {
      "epoch": 0.3300671692719928,
      "grad_norm": 3.023240089416504,
      "learning_rate": 4.449894198825801e-05,
      "loss": 1.8858,
      "step": 82100
    },
    {
      "epoch": 0.33046919992884055,
      "grad_norm": 3.703542709350586,
      "learning_rate": 4.449224147057602e-05,
      "loss": 1.91,
      "step": 82200
    },
    {
      "epoch": 0.3308712305856883,
      "grad_norm": 2.836621046066284,
      "learning_rate": 4.448554095289402e-05,
      "loss": 1.9125,
      "step": 82300
    },
    {
      "epoch": 0.33127326124253603,
      "grad_norm": 2.9710021018981934,
      "learning_rate": 4.447884043521203e-05,
      "loss": 1.8744,
      "step": 82400
    },
    {
      "epoch": 0.3316752918993838,
      "grad_norm": 2.892512321472168,
      "learning_rate": 4.447213991753003e-05,
      "loss": 1.8916,
      "step": 82500
    },
    {
      "epoch": 0.3320773225562315,
      "grad_norm": 3.075958251953125,
      "learning_rate": 4.446543939984804e-05,
      "loss": 1.8905,
      "step": 82600
    },
    {
      "epoch": 0.33247935321307925,
      "grad_norm": 2.919860363006592,
      "learning_rate": 4.445873888216604e-05,
      "loss": 1.9183,
      "step": 82700
    },
    {
      "epoch": 0.332881383869927,
      "grad_norm": 2.9348368644714355,
      "learning_rate": 4.445203836448404e-05,
      "loss": 1.8901,
      "step": 82800
    },
    {
      "epoch": 0.33328341452677473,
      "grad_norm": 3.41513991355896,
      "learning_rate": 4.444533784680205e-05,
      "loss": 1.9014,
      "step": 82900
    },
    {
      "epoch": 0.3336854451836225,
      "grad_norm": 3.643266439437866,
      "learning_rate": 4.4438637329120045e-05,
      "loss": 1.8778,
      "step": 83000
    },
    {
      "epoch": 0.3340874758404702,
      "grad_norm": 3.616828203201294,
      "learning_rate": 4.443193681143805e-05,
      "loss": 1.8861,
      "step": 83100
    },
    {
      "epoch": 0.33448950649731796,
      "grad_norm": 3.154205799102783,
      "learning_rate": 4.4425236293756055e-05,
      "loss": 1.9578,
      "step": 83200
    },
    {
      "epoch": 0.3348915371541657,
      "grad_norm": 2.649373769760132,
      "learning_rate": 4.441853577607406e-05,
      "loss": 1.8887,
      "step": 83300
    },
    {
      "epoch": 0.33529356781101344,
      "grad_norm": 2.950751781463623,
      "learning_rate": 4.4411835258392066e-05,
      "loss": 1.9125,
      "step": 83400
    },
    {
      "epoch": 0.3356955984678612,
      "grad_norm": 2.9838993549346924,
      "learning_rate": 4.440513474071007e-05,
      "loss": 1.908,
      "step": 83500
    },
    {
      "epoch": 0.3360976291247089,
      "grad_norm": 3.674762725830078,
      "learning_rate": 4.4398434223028076e-05,
      "loss": 1.9121,
      "step": 83600
    },
    {
      "epoch": 0.33649965978155666,
      "grad_norm": 3.3193299770355225,
      "learning_rate": 4.439173370534608e-05,
      "loss": 1.9273,
      "step": 83700
    },
    {
      "epoch": 0.3369016904384044,
      "grad_norm": 3.026456117630005,
      "learning_rate": 4.438503318766408e-05,
      "loss": 1.9094,
      "step": 83800
    },
    {
      "epoch": 0.33730372109525214,
      "grad_norm": 3.0610318183898926,
      "learning_rate": 4.4378332669982085e-05,
      "loss": 1.8948,
      "step": 83900
    },
    {
      "epoch": 0.3377057517520999,
      "grad_norm": 3.4753072261810303,
      "learning_rate": 4.437163215230009e-05,
      "loss": 1.9309,
      "step": 84000
    },
    {
      "epoch": 0.3381077824089476,
      "grad_norm": 3.008478879928589,
      "learning_rate": 4.4364931634618095e-05,
      "loss": 1.9112,
      "step": 84100
    },
    {
      "epoch": 0.3385098130657953,
      "grad_norm": 2.867021083831787,
      "learning_rate": 4.435823111693609e-05,
      "loss": 1.8933,
      "step": 84200
    },
    {
      "epoch": 0.33891184372264305,
      "grad_norm": 2.623105525970459,
      "learning_rate": 4.43515305992541e-05,
      "loss": 1.9238,
      "step": 84300
    },
    {
      "epoch": 0.3393138743794908,
      "grad_norm": 3.313537120819092,
      "learning_rate": 4.43448300815721e-05,
      "loss": 1.8377,
      "step": 84400
    },
    {
      "epoch": 0.33971590503633853,
      "grad_norm": 4.00941276550293,
      "learning_rate": 4.433812956389011e-05,
      "loss": 1.8388,
      "step": 84500
    },
    {
      "epoch": 0.34011793569318627,
      "grad_norm": 2.6600849628448486,
      "learning_rate": 4.4331429046208114e-05,
      "loss": 1.8721,
      "step": 84600
    },
    {
      "epoch": 0.340519966350034,
      "grad_norm": 3.1191658973693848,
      "learning_rate": 4.432472852852612e-05,
      "loss": 1.8905,
      "step": 84700
    },
    {
      "epoch": 0.34092199700688175,
      "grad_norm": 3.1762936115264893,
      "learning_rate": 4.431802801084412e-05,
      "loss": 1.8986,
      "step": 84800
    },
    {
      "epoch": 0.3413240276637295,
      "grad_norm": 3.598484516143799,
      "learning_rate": 4.431132749316212e-05,
      "loss": 1.8225,
      "step": 84900
    },
    {
      "epoch": 0.34172605832057723,
      "grad_norm": 3.604102849960327,
      "learning_rate": 4.430462697548013e-05,
      "loss": 1.8634,
      "step": 85000
    },
    {
      "epoch": 0.34212808897742497,
      "grad_norm": 3.3556153774261475,
      "learning_rate": 4.429792645779813e-05,
      "loss": 1.8417,
      "step": 85100
    },
    {
      "epoch": 0.3425301196342727,
      "grad_norm": 3.401505708694458,
      "learning_rate": 4.429122594011614e-05,
      "loss": 1.9057,
      "step": 85200
    },
    {
      "epoch": 0.34293215029112045,
      "grad_norm": 3.211103916168213,
      "learning_rate": 4.428452542243414e-05,
      "loss": 1.8441,
      "step": 85300
    },
    {
      "epoch": 0.3433341809479682,
      "grad_norm": 3.2937896251678467,
      "learning_rate": 4.427782490475214e-05,
      "loss": 1.8968,
      "step": 85400
    },
    {
      "epoch": 0.34373621160481593,
      "grad_norm": 3.370668411254883,
      "learning_rate": 4.4271124387070146e-05,
      "loss": 1.8408,
      "step": 85500
    },
    {
      "epoch": 0.3441382422616637,
      "grad_norm": 3.41312313079834,
      "learning_rate": 4.426442386938815e-05,
      "loss": 1.9091,
      "step": 85600
    },
    {
      "epoch": 0.3445402729185114,
      "grad_norm": 3.5112967491149902,
      "learning_rate": 4.4257723351706156e-05,
      "loss": 1.8867,
      "step": 85700
    },
    {
      "epoch": 0.34494230357535915,
      "grad_norm": 2.7668144702911377,
      "learning_rate": 4.425102283402416e-05,
      "loss": 1.8837,
      "step": 85800
    },
    {
      "epoch": 0.3453443342322069,
      "grad_norm": 3.1573450565338135,
      "learning_rate": 4.424432231634216e-05,
      "loss": 1.8324,
      "step": 85900
    },
    {
      "epoch": 0.34574636488905464,
      "grad_norm": 3.049616813659668,
      "learning_rate": 4.4237621798660165e-05,
      "loss": 1.9052,
      "step": 86000
    },
    {
      "epoch": 0.3461483955459024,
      "grad_norm": 3.1427338123321533,
      "learning_rate": 4.423092128097817e-05,
      "loss": 1.8815,
      "step": 86100
    },
    {
      "epoch": 0.3465504262027501,
      "grad_norm": 3.193094253540039,
      "learning_rate": 4.4224220763296175e-05,
      "loss": 1.9149,
      "step": 86200
    },
    {
      "epoch": 0.3469524568595978,
      "grad_norm": 2.645918607711792,
      "learning_rate": 4.421752024561418e-05,
      "loss": 1.8523,
      "step": 86300
    },
    {
      "epoch": 0.34735448751644554,
      "grad_norm": 3.692656993865967,
      "learning_rate": 4.4210819727932186e-05,
      "loss": 1.8792,
      "step": 86400
    },
    {
      "epoch": 0.3477565181732933,
      "grad_norm": 2.7368323802948,
      "learning_rate": 4.4204119210250184e-05,
      "loss": 1.8722,
      "step": 86500
    },
    {
      "epoch": 0.348158548830141,
      "grad_norm": 3.3764896392822266,
      "learning_rate": 4.419741869256819e-05,
      "loss": 1.8728,
      "step": 86600
    },
    {
      "epoch": 0.34856057948698876,
      "grad_norm": 3.049790143966675,
      "learning_rate": 4.4190718174886194e-05,
      "loss": 1.868,
      "step": 86700
    },
    {
      "epoch": 0.3489626101438365,
      "grad_norm": 3.1034798622131348,
      "learning_rate": 4.41840176572042e-05,
      "loss": 1.8639,
      "step": 86800
    },
    {
      "epoch": 0.34936464080068425,
      "grad_norm": 3.1059443950653076,
      "learning_rate": 4.41773171395222e-05,
      "loss": 1.8626,
      "step": 86900
    },
    {
      "epoch": 0.349766671457532,
      "grad_norm": 2.77146053314209,
      "learning_rate": 4.41706166218402e-05,
      "loss": 1.8461,
      "step": 87000
    },
    {
      "epoch": 0.3501687021143797,
      "grad_norm": 3.4940738677978516,
      "learning_rate": 4.416391610415821e-05,
      "loss": 1.8371,
      "step": 87100
    },
    {
      "epoch": 0.35057073277122747,
      "grad_norm": 3.4801175594329834,
      "learning_rate": 4.415721558647621e-05,
      "loss": 1.9074,
      "step": 87200
    },
    {
      "epoch": 0.3509727634280752,
      "grad_norm": 3.107523202896118,
      "learning_rate": 4.415051506879422e-05,
      "loss": 1.8869,
      "step": 87300
    },
    {
      "epoch": 0.35137479408492295,
      "grad_norm": 3.895925521850586,
      "learning_rate": 4.414381455111222e-05,
      "loss": 1.8253,
      "step": 87400
    },
    {
      "epoch": 0.3517768247417707,
      "grad_norm": 3.665639877319336,
      "learning_rate": 4.413711403343023e-05,
      "loss": 1.9068,
      "step": 87500
    },
    {
      "epoch": 0.35217885539861843,
      "grad_norm": 3.0060572624206543,
      "learning_rate": 4.4130413515748234e-05,
      "loss": 1.8405,
      "step": 87600
    },
    {
      "epoch": 0.35258088605546617,
      "grad_norm": 2.56876802444458,
      "learning_rate": 4.412371299806623e-05,
      "loss": 1.8682,
      "step": 87700
    },
    {
      "epoch": 0.3529829167123139,
      "grad_norm": 3.1525306701660156,
      "learning_rate": 4.411701248038424e-05,
      "loss": 1.8456,
      "step": 87800
    },
    {
      "epoch": 0.35338494736916165,
      "grad_norm": 3.171088933944702,
      "learning_rate": 4.411031196270224e-05,
      "loss": 1.8636,
      "step": 87900
    },
    {
      "epoch": 0.3537869780260094,
      "grad_norm": 3.1258554458618164,
      "learning_rate": 4.410361144502024e-05,
      "loss": 1.8924,
      "step": 88000
    },
    {
      "epoch": 0.35418900868285713,
      "grad_norm": 3.1416208744049072,
      "learning_rate": 4.4096910927338246e-05,
      "loss": 1.7933,
      "step": 88100
    },
    {
      "epoch": 0.35459103933970487,
      "grad_norm": 3.1046249866485596,
      "learning_rate": 4.409021040965625e-05,
      "loss": 1.9378,
      "step": 88200
    },
    {
      "epoch": 0.3549930699965526,
      "grad_norm": 3.1828815937042236,
      "learning_rate": 4.4083509891974256e-05,
      "loss": 1.8582,
      "step": 88300
    },
    {
      "epoch": 0.3553951006534003,
      "grad_norm": 3.1525754928588867,
      "learning_rate": 4.407680937429226e-05,
      "loss": 1.8506,
      "step": 88400
    },
    {
      "epoch": 0.35579713131024804,
      "grad_norm": 3.7728288173675537,
      "learning_rate": 4.4070108856610266e-05,
      "loss": 1.8422,
      "step": 88500
    },
    {
      "epoch": 0.3561991619670958,
      "grad_norm": 3.037961483001709,
      "learning_rate": 4.406340833892827e-05,
      "loss": 1.889,
      "step": 88600
    },
    {
      "epoch": 0.3566011926239435,
      "grad_norm": 3.256662368774414,
      "learning_rate": 4.4056707821246276e-05,
      "loss": 1.844,
      "step": 88700
    },
    {
      "epoch": 0.35700322328079126,
      "grad_norm": 2.9183573722839355,
      "learning_rate": 4.4050007303564275e-05,
      "loss": 1.839,
      "step": 88800
    },
    {
      "epoch": 0.357405253937639,
      "grad_norm": 3.380228042602539,
      "learning_rate": 4.404330678588228e-05,
      "loss": 1.8361,
      "step": 88900
    },
    {
      "epoch": 0.35780728459448674,
      "grad_norm": 3.0121216773986816,
      "learning_rate": 4.403660626820028e-05,
      "loss": 1.8488,
      "step": 89000
    },
    {
      "epoch": 0.3582093152513345,
      "grad_norm": 3.883025646209717,
      "learning_rate": 4.402990575051828e-05,
      "loss": 1.8361,
      "step": 89100
    },
    {
      "epoch": 0.3586113459081822,
      "grad_norm": 3.6535332202911377,
      "learning_rate": 4.402320523283629e-05,
      "loss": 1.8252,
      "step": 89200
    },
    {
      "epoch": 0.35901337656502996,
      "grad_norm": 3.3909997940063477,
      "learning_rate": 4.4016504715154294e-05,
      "loss": 1.8272,
      "step": 89300
    },
    {
      "epoch": 0.3594154072218777,
      "grad_norm": 3.1823461055755615,
      "learning_rate": 4.40098041974723e-05,
      "loss": 1.8748,
      "step": 89400
    },
    {
      "epoch": 0.35981743787872544,
      "grad_norm": 3.210691452026367,
      "learning_rate": 4.4003103679790304e-05,
      "loss": 1.8562,
      "step": 89500
    },
    {
      "epoch": 0.3602194685355732,
      "grad_norm": 2.9373207092285156,
      "learning_rate": 4.399640316210831e-05,
      "loss": 1.9056,
      "step": 89600
    },
    {
      "epoch": 0.3606214991924209,
      "grad_norm": 3.3346493244171143,
      "learning_rate": 4.3989702644426314e-05,
      "loss": 1.7834,
      "step": 89700
    },
    {
      "epoch": 0.36102352984926867,
      "grad_norm": 3.6853013038635254,
      "learning_rate": 4.398300212674432e-05,
      "loss": 1.8563,
      "step": 89800
    },
    {
      "epoch": 0.3614255605061164,
      "grad_norm": 3.074108839035034,
      "learning_rate": 4.397630160906232e-05,
      "loss": 1.8558,
      "step": 89900
    },
    {
      "epoch": 0.36182759116296415,
      "grad_norm": 3.3117289543151855,
      "learning_rate": 4.396960109138032e-05,
      "loss": 1.8505,
      "step": 90000
    },
    {
      "epoch": 0.3622296218198119,
      "grad_norm": 3.523313045501709,
      "learning_rate": 4.396290057369832e-05,
      "loss": 1.8722,
      "step": 90100
    },
    {
      "epoch": 0.3626316524766596,
      "grad_norm": 3.2411108016967773,
      "learning_rate": 4.3956200056016326e-05,
      "loss": 1.8204,
      "step": 90200
    },
    {
      "epoch": 0.36303368313350737,
      "grad_norm": 3.3988149166107178,
      "learning_rate": 4.394949953833433e-05,
      "loss": 1.767,
      "step": 90300
    },
    {
      "epoch": 0.3634357137903551,
      "grad_norm": 3.1778905391693115,
      "learning_rate": 4.3942799020652336e-05,
      "loss": 1.8493,
      "step": 90400
    },
    {
      "epoch": 0.36383774444720285,
      "grad_norm": 3.6114187240600586,
      "learning_rate": 4.393609850297034e-05,
      "loss": 1.8296,
      "step": 90500
    },
    {
      "epoch": 0.36423977510405053,
      "grad_norm": 2.569439649581909,
      "learning_rate": 4.392939798528835e-05,
      "loss": 1.8413,
      "step": 90600
    },
    {
      "epoch": 0.3646418057608983,
      "grad_norm": 3.4541797637939453,
      "learning_rate": 4.392269746760635e-05,
      "loss": 1.8473,
      "step": 90700
    },
    {
      "epoch": 0.365043836417746,
      "grad_norm": 3.020435333251953,
      "learning_rate": 4.391599694992436e-05,
      "loss": 1.8222,
      "step": 90800
    },
    {
      "epoch": 0.36544586707459376,
      "grad_norm": 3.334437608718872,
      "learning_rate": 4.3909296432242355e-05,
      "loss": 1.8402,
      "step": 90900
    },
    {
      "epoch": 0.3658478977314415,
      "grad_norm": 3.5207412242889404,
      "learning_rate": 4.390259591456036e-05,
      "loss": 1.8786,
      "step": 91000
    },
    {
      "epoch": 0.36624992838828924,
      "grad_norm": 3.4284167289733887,
      "learning_rate": 4.3895895396878365e-05,
      "loss": 1.844,
      "step": 91100
    },
    {
      "epoch": 0.366651959045137,
      "grad_norm": 3.0347108840942383,
      "learning_rate": 4.388919487919637e-05,
      "loss": 1.8051,
      "step": 91200
    },
    {
      "epoch": 0.3670539897019847,
      "grad_norm": 3.0692553520202637,
      "learning_rate": 4.388249436151437e-05,
      "loss": 1.8144,
      "step": 91300
    },
    {
      "epoch": 0.36745602035883246,
      "grad_norm": 3.144610643386841,
      "learning_rate": 4.3875793843832374e-05,
      "loss": 1.9099,
      "step": 91400
    },
    {
      "epoch": 0.3678580510156802,
      "grad_norm": 3.5986099243164062,
      "learning_rate": 4.386909332615038e-05,
      "loss": 1.7804,
      "step": 91500
    },
    {
      "epoch": 0.36826008167252794,
      "grad_norm": 3.2514564990997314,
      "learning_rate": 4.3862392808468384e-05,
      "loss": 1.8504,
      "step": 91600
    },
    {
      "epoch": 0.3686621123293757,
      "grad_norm": 3.198230743408203,
      "learning_rate": 4.385569229078639e-05,
      "loss": 1.8906,
      "step": 91700
    },
    {
      "epoch": 0.3690641429862234,
      "grad_norm": 2.791895866394043,
      "learning_rate": 4.3848991773104395e-05,
      "loss": 1.8344,
      "step": 91800
    },
    {
      "epoch": 0.36946617364307116,
      "grad_norm": 3.345282793045044,
      "learning_rate": 4.38422912554224e-05,
      "loss": 1.8496,
      "step": 91900
    },
    {
      "epoch": 0.3698682042999189,
      "grad_norm": 3.038754940032959,
      "learning_rate": 4.38355907377404e-05,
      "loss": 1.8937,
      "step": 92000
    },
    {
      "epoch": 0.37027023495676664,
      "grad_norm": 2.739837884902954,
      "learning_rate": 4.38288902200584e-05,
      "loss": 1.8148,
      "step": 92100
    },
    {
      "epoch": 0.3706722656136144,
      "grad_norm": 3.400904893875122,
      "learning_rate": 4.382218970237641e-05,
      "loss": 1.814,
      "step": 92200
    },
    {
      "epoch": 0.3710742962704621,
      "grad_norm": 3.304535388946533,
      "learning_rate": 4.3815489184694413e-05,
      "loss": 1.8425,
      "step": 92300
    },
    {
      "epoch": 0.37147632692730986,
      "grad_norm": 3.195571184158325,
      "learning_rate": 4.380878866701242e-05,
      "loss": 1.8651,
      "step": 92400
    },
    {
      "epoch": 0.3718783575841576,
      "grad_norm": 2.914069890975952,
      "learning_rate": 4.380208814933042e-05,
      "loss": 1.8122,
      "step": 92500
    },
    {
      "epoch": 0.37228038824100534,
      "grad_norm": 3.4950449466705322,
      "learning_rate": 4.379538763164842e-05,
      "loss": 1.8917,
      "step": 92600
    },
    {
      "epoch": 0.37268241889785303,
      "grad_norm": 3.2881195545196533,
      "learning_rate": 4.378868711396643e-05,
      "loss": 1.8301,
      "step": 92700
    },
    {
      "epoch": 0.37308444955470077,
      "grad_norm": 3.378164768218994,
      "learning_rate": 4.378198659628443e-05,
      "loss": 1.8132,
      "step": 92800
    },
    {
      "epoch": 0.3734864802115485,
      "grad_norm": 2.9219813346862793,
      "learning_rate": 4.377528607860244e-05,
      "loss": 1.8463,
      "step": 92900
    },
    {
      "epoch": 0.37388851086839625,
      "grad_norm": 2.8818295001983643,
      "learning_rate": 4.3768585560920436e-05,
      "loss": 1.8448,
      "step": 93000
    },
    {
      "epoch": 0.374290541525244,
      "grad_norm": 3.0783581733703613,
      "learning_rate": 4.376188504323844e-05,
      "loss": 1.8748,
      "step": 93100
    },
    {
      "epoch": 0.37469257218209173,
      "grad_norm": 3.7287356853485107,
      "learning_rate": 4.3755184525556446e-05,
      "loss": 1.8313,
      "step": 93200
    },
    {
      "epoch": 0.3750946028389395,
      "grad_norm": 2.820005178451538,
      "learning_rate": 4.374848400787445e-05,
      "loss": 1.8344,
      "step": 93300
    },
    {
      "epoch": 0.3754966334957872,
      "grad_norm": 3.4194459915161133,
      "learning_rate": 4.3741783490192456e-05,
      "loss": 1.8328,
      "step": 93400
    },
    {
      "epoch": 0.37589866415263495,
      "grad_norm": 3.359666109085083,
      "learning_rate": 4.373508297251046e-05,
      "loss": 1.7986,
      "step": 93500
    },
    {
      "epoch": 0.3763006948094827,
      "grad_norm": 2.6389827728271484,
      "learning_rate": 4.3728382454828467e-05,
      "loss": 1.847,
      "step": 93600
    },
    {
      "epoch": 0.37670272546633043,
      "grad_norm": 3.342775583267212,
      "learning_rate": 4.3721681937146465e-05,
      "loss": 1.8475,
      "step": 93700
    },
    {
      "epoch": 0.3771047561231782,
      "grad_norm": 2.801337242126465,
      "learning_rate": 4.371498141946447e-05,
      "loss": 1.7786,
      "step": 93800
    },
    {
      "epoch": 0.3775067867800259,
      "grad_norm": 3.031327486038208,
      "learning_rate": 4.3708280901782475e-05,
      "loss": 1.8467,
      "step": 93900
    },
    {
      "epoch": 0.37790881743687366,
      "grad_norm": 3.3323850631713867,
      "learning_rate": 4.3701580384100474e-05,
      "loss": 1.7885,
      "step": 94000
    },
    {
      "epoch": 0.3783108480937214,
      "grad_norm": 3.195803642272949,
      "learning_rate": 4.369487986641848e-05,
      "loss": 1.7591,
      "step": 94100
    },
    {
      "epoch": 0.37871287875056914,
      "grad_norm": 3.01100754737854,
      "learning_rate": 4.3688179348736484e-05,
      "loss": 1.8033,
      "step": 94200
    },
    {
      "epoch": 0.3791149094074169,
      "grad_norm": 2.7819669246673584,
      "learning_rate": 4.368147883105449e-05,
      "loss": 1.8106,
      "step": 94300
    },
    {
      "epoch": 0.3795169400642646,
      "grad_norm": 3.562741994857788,
      "learning_rate": 4.3674778313372494e-05,
      "loss": 1.7817,
      "step": 94400
    },
    {
      "epoch": 0.37991897072111236,
      "grad_norm": 3.032052516937256,
      "learning_rate": 4.36680777956905e-05,
      "loss": 1.8374,
      "step": 94500
    },
    {
      "epoch": 0.3803210013779601,
      "grad_norm": 3.3259215354919434,
      "learning_rate": 4.3661377278008504e-05,
      "loss": 1.8451,
      "step": 94600
    },
    {
      "epoch": 0.38072303203480784,
      "grad_norm": 3.3031649589538574,
      "learning_rate": 4.365467676032651e-05,
      "loss": 1.8568,
      "step": 94700
    },
    {
      "epoch": 0.3811250626916555,
      "grad_norm": 3.25351881980896,
      "learning_rate": 4.3647976242644515e-05,
      "loss": 1.773,
      "step": 94800
    },
    {
      "epoch": 0.38152709334850327,
      "grad_norm": 3.3389368057250977,
      "learning_rate": 4.364127572496251e-05,
      "loss": 1.8317,
      "step": 94900
    },
    {
      "epoch": 0.381929124005351,
      "grad_norm": 3.1466610431671143,
      "learning_rate": 4.363457520728052e-05,
      "loss": 1.8333,
      "step": 95000
    },
    {
      "epoch": 0.38233115466219875,
      "grad_norm": 3.4183404445648193,
      "learning_rate": 4.3627874689598516e-05,
      "loss": 1.8067,
      "step": 95100
    },
    {
      "epoch": 0.3827331853190465,
      "grad_norm": 3.0112204551696777,
      "learning_rate": 4.362117417191652e-05,
      "loss": 1.8269,
      "step": 95200
    },
    {
      "epoch": 0.3831352159758942,
      "grad_norm": 3.635054111480713,
      "learning_rate": 4.3614473654234527e-05,
      "loss": 1.8283,
      "step": 95300
    },
    {
      "epoch": 0.38353724663274197,
      "grad_norm": 3.3094449043273926,
      "learning_rate": 4.360777313655253e-05,
      "loss": 1.8336,
      "step": 95400
    },
    {
      "epoch": 0.3839392772895897,
      "grad_norm": 3.1306087970733643,
      "learning_rate": 4.360107261887054e-05,
      "loss": 1.8205,
      "step": 95500
    },
    {
      "epoch": 0.38434130794643745,
      "grad_norm": 3.2508864402770996,
      "learning_rate": 4.359437210118854e-05,
      "loss": 1.8338,
      "step": 95600
    },
    {
      "epoch": 0.3847433386032852,
      "grad_norm": 3.3683276176452637,
      "learning_rate": 4.358767158350655e-05,
      "loss": 1.8097,
      "step": 95700
    },
    {
      "epoch": 0.38514536926013293,
      "grad_norm": 3.3452110290527344,
      "learning_rate": 4.358097106582455e-05,
      "loss": 1.7855,
      "step": 95800
    },
    {
      "epoch": 0.38554739991698067,
      "grad_norm": 2.6756110191345215,
      "learning_rate": 4.357427054814255e-05,
      "loss": 1.8028,
      "step": 95900
    },
    {
      "epoch": 0.3859494305738284,
      "grad_norm": 3.6147403717041016,
      "learning_rate": 4.3567570030460556e-05,
      "loss": 1.8133,
      "step": 96000
    },
    {
      "epoch": 0.38635146123067615,
      "grad_norm": 2.8137993812561035,
      "learning_rate": 4.3560869512778554e-05,
      "loss": 1.811,
      "step": 96100
    },
    {
      "epoch": 0.3867534918875239,
      "grad_norm": 3.1505508422851562,
      "learning_rate": 4.355416899509656e-05,
      "loss": 1.784,
      "step": 96200
    },
    {
      "epoch": 0.38715552254437163,
      "grad_norm": 3.171931028366089,
      "learning_rate": 4.3547468477414564e-05,
      "loss": 1.8175,
      "step": 96300
    },
    {
      "epoch": 0.3875575532012194,
      "grad_norm": 2.866375684738159,
      "learning_rate": 4.354076795973257e-05,
      "loss": 1.8767,
      "step": 96400
    },
    {
      "epoch": 0.3879595838580671,
      "grad_norm": 3.119619131088257,
      "learning_rate": 4.3534067442050575e-05,
      "loss": 1.8427,
      "step": 96500
    },
    {
      "epoch": 0.38836161451491485,
      "grad_norm": 3.0405476093292236,
      "learning_rate": 4.352736692436858e-05,
      "loss": 1.7919,
      "step": 96600
    },
    {
      "epoch": 0.3887636451717626,
      "grad_norm": 3.7449355125427246,
      "learning_rate": 4.3520666406686585e-05,
      "loss": 1.803,
      "step": 96700
    },
    {
      "epoch": 0.38916567582861034,
      "grad_norm": 3.7189173698425293,
      "learning_rate": 4.351396588900459e-05,
      "loss": 1.8101,
      "step": 96800
    },
    {
      "epoch": 0.3895677064854581,
      "grad_norm": 3.148618459701538,
      "learning_rate": 4.3507265371322595e-05,
      "loss": 1.7944,
      "step": 96900
    },
    {
      "epoch": 0.38996973714230576,
      "grad_norm": 3.4808475971221924,
      "learning_rate": 4.3500564853640593e-05,
      "loss": 1.837,
      "step": 97000
    },
    {
      "epoch": 0.3903717677991535,
      "grad_norm": 2.9962034225463867,
      "learning_rate": 4.34938643359586e-05,
      "loss": 1.7783,
      "step": 97100
    },
    {
      "epoch": 0.39077379845600124,
      "grad_norm": 3.071009397506714,
      "learning_rate": 4.34871638182766e-05,
      "loss": 1.8555,
      "step": 97200
    },
    {
      "epoch": 0.391175829112849,
      "grad_norm": 3.5299220085144043,
      "learning_rate": 4.34804633005946e-05,
      "loss": 1.79,
      "step": 97300
    },
    {
      "epoch": 0.3915778597696967,
      "grad_norm": 3.01587176322937,
      "learning_rate": 4.347376278291261e-05,
      "loss": 1.8416,
      "step": 97400
    },
    {
      "epoch": 0.39197989042654446,
      "grad_norm": 3.7735142707824707,
      "learning_rate": 4.346706226523061e-05,
      "loss": 1.8061,
      "step": 97500
    },
    {
      "epoch": 0.3923819210833922,
      "grad_norm": 3.396724224090576,
      "learning_rate": 4.346036174754862e-05,
      "loss": 1.779,
      "step": 97600
    },
    {
      "epoch": 0.39278395174023994,
      "grad_norm": 3.0995371341705322,
      "learning_rate": 4.345366122986662e-05,
      "loss": 1.7921,
      "step": 97700
    },
    {
      "epoch": 0.3931859823970877,
      "grad_norm": 3.8081398010253906,
      "learning_rate": 4.344696071218463e-05,
      "loss": 1.7998,
      "step": 97800
    },
    {
      "epoch": 0.3935880130539354,
      "grad_norm": 3.3024871349334717,
      "learning_rate": 4.344026019450263e-05,
      "loss": 1.7728,
      "step": 97900
    },
    {
      "epoch": 0.39399004371078317,
      "grad_norm": 2.9884555339813232,
      "learning_rate": 4.343355967682063e-05,
      "loss": 1.7773,
      "step": 98000
    },
    {
      "epoch": 0.3943920743676309,
      "grad_norm": 3.4064176082611084,
      "learning_rate": 4.3426859159138636e-05,
      "loss": 1.8428,
      "step": 98100
    },
    {
      "epoch": 0.39479410502447865,
      "grad_norm": 3.0245795249938965,
      "learning_rate": 4.342015864145664e-05,
      "loss": 1.8645,
      "step": 98200
    },
    {
      "epoch": 0.3951961356813264,
      "grad_norm": 2.960139513015747,
      "learning_rate": 4.3413458123774646e-05,
      "loss": 1.7965,
      "step": 98300
    },
    {
      "epoch": 0.39559816633817413,
      "grad_norm": 3.018141031265259,
      "learning_rate": 4.3406757606092645e-05,
      "loss": 1.7693,
      "step": 98400
    },
    {
      "epoch": 0.39600019699502187,
      "grad_norm": 3.6126585006713867,
      "learning_rate": 4.340005708841065e-05,
      "loss": 1.8967,
      "step": 98500
    },
    {
      "epoch": 0.3964022276518696,
      "grad_norm": 3.2129569053649902,
      "learning_rate": 4.3393356570728655e-05,
      "loss": 1.7898,
      "step": 98600
    },
    {
      "epoch": 0.39680425830871735,
      "grad_norm": 3.3532207012176514,
      "learning_rate": 4.338665605304666e-05,
      "loss": 1.7689,
      "step": 98700
    },
    {
      "epoch": 0.3972062889655651,
      "grad_norm": 2.959062337875366,
      "learning_rate": 4.3379955535364665e-05,
      "loss": 1.7858,
      "step": 98800
    },
    {
      "epoch": 0.39760831962241283,
      "grad_norm": 2.841376781463623,
      "learning_rate": 4.337325501768267e-05,
      "loss": 1.815,
      "step": 98900
    },
    {
      "epoch": 0.39801035027926057,
      "grad_norm": 3.4316842555999756,
      "learning_rate": 4.3366554500000676e-05,
      "loss": 1.7693,
      "step": 99000
    },
    {
      "epoch": 0.39841238093610826,
      "grad_norm": 3.056212902069092,
      "learning_rate": 4.3359853982318674e-05,
      "loss": 1.8015,
      "step": 99100
    },
    {
      "epoch": 0.398814411592956,
      "grad_norm": 3.57106614112854,
      "learning_rate": 4.335315346463668e-05,
      "loss": 1.8355,
      "step": 99200
    },
    {
      "epoch": 0.39921644224980374,
      "grad_norm": 3.2180230617523193,
      "learning_rate": 4.3346452946954684e-05,
      "loss": 1.7775,
      "step": 99300
    },
    {
      "epoch": 0.3996184729066515,
      "grad_norm": 3.7150371074676514,
      "learning_rate": 4.333975242927269e-05,
      "loss": 1.8341,
      "step": 99400
    },
    {
      "epoch": 0.4000205035634992,
      "grad_norm": 3.2346460819244385,
      "learning_rate": 4.3333051911590694e-05,
      "loss": 1.8416,
      "step": 99500
    },
    {
      "epoch": 0.40042253422034696,
      "grad_norm": 3.075415849685669,
      "learning_rate": 4.332635139390869e-05,
      "loss": 1.8065,
      "step": 99600
    },
    {
      "epoch": 0.4008245648771947,
      "grad_norm": 3.466730833053589,
      "learning_rate": 4.33196508762267e-05,
      "loss": 1.7992,
      "step": 99700
    },
    {
      "epoch": 0.40122659553404244,
      "grad_norm": 3.1769707202911377,
      "learning_rate": 4.33129503585447e-05,
      "loss": 1.8078,
      "step": 99800
    },
    {
      "epoch": 0.4016286261908902,
      "grad_norm": 2.7104239463806152,
      "learning_rate": 4.330624984086271e-05,
      "loss": 1.7871,
      "step": 99900
    },
    {
      "epoch": 0.4020306568477379,
      "grad_norm": 2.930361032485962,
      "learning_rate": 4.329954932318071e-05,
      "loss": 1.8039,
      "step": 100000
    },
    {
      "epoch": 0.40243268750458566,
      "grad_norm": 2.586106300354004,
      "learning_rate": 4.329284880549871e-05,
      "loss": 1.7711,
      "step": 100100
    },
    {
      "epoch": 0.4028347181614334,
      "grad_norm": 4.06913948059082,
      "learning_rate": 4.328614828781672e-05,
      "loss": 1.8145,
      "step": 100200
    },
    {
      "epoch": 0.40323674881828114,
      "grad_norm": 4.293764591217041,
      "learning_rate": 4.327944777013472e-05,
      "loss": 1.7393,
      "step": 100300
    },
    {
      "epoch": 0.4036387794751289,
      "grad_norm": 3.6235711574554443,
      "learning_rate": 4.327274725245273e-05,
      "loss": 1.8209,
      "step": 100400
    },
    {
      "epoch": 0.4040408101319766,
      "grad_norm": 3.2666661739349365,
      "learning_rate": 4.326604673477073e-05,
      "loss": 1.829,
      "step": 100500
    },
    {
      "epoch": 0.40444284078882436,
      "grad_norm": 3.361377000808716,
      "learning_rate": 4.325934621708874e-05,
      "loss": 1.7983,
      "step": 100600
    },
    {
      "epoch": 0.4048448714456721,
      "grad_norm": 3.216783285140991,
      "learning_rate": 4.325264569940674e-05,
      "loss": 1.7702,
      "step": 100700
    },
    {
      "epoch": 0.40524690210251985,
      "grad_norm": 3.783121347427368,
      "learning_rate": 4.324594518172474e-05,
      "loss": 1.8257,
      "step": 100800
    },
    {
      "epoch": 0.4056489327593676,
      "grad_norm": 3.304426431655884,
      "learning_rate": 4.3239244664042746e-05,
      "loss": 1.856,
      "step": 100900
    },
    {
      "epoch": 0.4060509634162153,
      "grad_norm": 3.241443634033203,
      "learning_rate": 4.323254414636075e-05,
      "loss": 1.8303,
      "step": 101000
    },
    {
      "epoch": 0.40645299407306307,
      "grad_norm": 3.1075663566589355,
      "learning_rate": 4.322584362867875e-05,
      "loss": 1.8322,
      "step": 101100
    },
    {
      "epoch": 0.40685502472991075,
      "grad_norm": 2.680359363555908,
      "learning_rate": 4.3219143110996754e-05,
      "loss": 1.8264,
      "step": 101200
    },
    {
      "epoch": 0.4072570553867585,
      "grad_norm": 3.2267448902130127,
      "learning_rate": 4.321244259331476e-05,
      "loss": 1.7863,
      "step": 101300
    },
    {
      "epoch": 0.40765908604360623,
      "grad_norm": 3.3343703746795654,
      "learning_rate": 4.3205742075632765e-05,
      "loss": 1.8092,
      "step": 101400
    },
    {
      "epoch": 0.408061116700454,
      "grad_norm": 3.1575675010681152,
      "learning_rate": 4.319904155795077e-05,
      "loss": 1.8239,
      "step": 101500
    },
    {
      "epoch": 0.4084631473573017,
      "grad_norm": 3.6747357845306396,
      "learning_rate": 4.3192341040268775e-05,
      "loss": 1.7921,
      "step": 101600
    },
    {
      "epoch": 0.40886517801414946,
      "grad_norm": 3.1014795303344727,
      "learning_rate": 4.318564052258678e-05,
      "loss": 1.8105,
      "step": 101700
    },
    {
      "epoch": 0.4092672086709972,
      "grad_norm": 2.8639206886291504,
      "learning_rate": 4.3178940004904785e-05,
      "loss": 1.809,
      "step": 101800
    },
    {
      "epoch": 0.40966923932784494,
      "grad_norm": 3.207397222518921,
      "learning_rate": 4.317223948722279e-05,
      "loss": 1.7735,
      "step": 101900
    },
    {
      "epoch": 0.4100712699846927,
      "grad_norm": 3.0551440715789795,
      "learning_rate": 4.316553896954079e-05,
      "loss": 1.817,
      "step": 102000
    },
    {
      "epoch": 0.4104733006415404,
      "grad_norm": 3.297348737716675,
      "learning_rate": 4.3158838451858794e-05,
      "loss": 1.7703,
      "step": 102100
    },
    {
      "epoch": 0.41087533129838816,
      "grad_norm": 3.319300651550293,
      "learning_rate": 4.315213793417679e-05,
      "loss": 1.7859,
      "step": 102200
    },
    {
      "epoch": 0.4112773619552359,
      "grad_norm": 3.3998823165893555,
      "learning_rate": 4.31454374164948e-05,
      "loss": 1.7837,
      "step": 102300
    },
    {
      "epoch": 0.41167939261208364,
      "grad_norm": 4.306739330291748,
      "learning_rate": 4.31387368988128e-05,
      "loss": 1.7833,
      "step": 102400
    },
    {
      "epoch": 0.4120814232689314,
      "grad_norm": 3.2428784370422363,
      "learning_rate": 4.313203638113081e-05,
      "loss": 1.765,
      "step": 102500
    },
    {
      "epoch": 0.4124834539257791,
      "grad_norm": 3.269378423690796,
      "learning_rate": 4.312533586344881e-05,
      "loss": 1.8187,
      "step": 102600
    },
    {
      "epoch": 0.41288548458262686,
      "grad_norm": 2.413113832473755,
      "learning_rate": 4.311863534576682e-05,
      "loss": 1.7629,
      "step": 102700
    },
    {
      "epoch": 0.4132875152394746,
      "grad_norm": 2.872612714767456,
      "learning_rate": 4.311193482808482e-05,
      "loss": 1.7803,
      "step": 102800
    },
    {
      "epoch": 0.41368954589632234,
      "grad_norm": 3.602881669998169,
      "learning_rate": 4.310523431040283e-05,
      "loss": 1.7883,
      "step": 102900
    },
    {
      "epoch": 0.4140915765531701,
      "grad_norm": 3.3042876720428467,
      "learning_rate": 4.309853379272083e-05,
      "loss": 1.7691,
      "step": 103000
    },
    {
      "epoch": 0.4144936072100178,
      "grad_norm": 3.2518012523651123,
      "learning_rate": 4.309183327503883e-05,
      "loss": 1.7536,
      "step": 103100
    },
    {
      "epoch": 0.41489563786686556,
      "grad_norm": 2.835555076599121,
      "learning_rate": 4.308513275735683e-05,
      "loss": 1.8033,
      "step": 103200
    },
    {
      "epoch": 0.41529766852371325,
      "grad_norm": 3.238603115081787,
      "learning_rate": 4.3078432239674835e-05,
      "loss": 1.771,
      "step": 103300
    },
    {
      "epoch": 0.415699699180561,
      "grad_norm": 2.600446939468384,
      "learning_rate": 4.307173172199284e-05,
      "loss": 1.8244,
      "step": 103400
    },
    {
      "epoch": 0.41610172983740873,
      "grad_norm": 3.3486251831054688,
      "learning_rate": 4.3065031204310845e-05,
      "loss": 1.7792,
      "step": 103500
    },
    {
      "epoch": 0.41650376049425647,
      "grad_norm": 2.7508082389831543,
      "learning_rate": 4.305833068662885e-05,
      "loss": 1.78,
      "step": 103600
    },
    {
      "epoch": 0.4169057911511042,
      "grad_norm": 3.343799114227295,
      "learning_rate": 4.3051630168946856e-05,
      "loss": 1.7735,
      "step": 103700
    },
    {
      "epoch": 0.41730782180795195,
      "grad_norm": 2.858774423599243,
      "learning_rate": 4.304492965126486e-05,
      "loss": 1.8115,
      "step": 103800
    },
    {
      "epoch": 0.4177098524647997,
      "grad_norm": 3.514845371246338,
      "learning_rate": 4.3038229133582866e-05,
      "loss": 1.816,
      "step": 103900
    },
    {
      "epoch": 0.41811188312164743,
      "grad_norm": 3.0438904762268066,
      "learning_rate": 4.303152861590087e-05,
      "loss": 1.7636,
      "step": 104000
    },
    {
      "epoch": 0.4185139137784952,
      "grad_norm": 3.562309980392456,
      "learning_rate": 4.302482809821887e-05,
      "loss": 1.7895,
      "step": 104100
    },
    {
      "epoch": 0.4189159444353429,
      "grad_norm": 3.018878221511841,
      "learning_rate": 4.3018127580536874e-05,
      "loss": 1.8038,
      "step": 104200
    },
    {
      "epoch": 0.41931797509219065,
      "grad_norm": 3.118095874786377,
      "learning_rate": 4.301142706285488e-05,
      "loss": 1.8001,
      "step": 104300
    },
    {
      "epoch": 0.4197200057490384,
      "grad_norm": 3.3729610443115234,
      "learning_rate": 4.300472654517288e-05,
      "loss": 1.8062,
      "step": 104400
    },
    {
      "epoch": 0.42012203640588613,
      "grad_norm": 2.9578418731689453,
      "learning_rate": 4.299802602749088e-05,
      "loss": 1.7938,
      "step": 104500
    },
    {
      "epoch": 0.4205240670627339,
      "grad_norm": 3.263338327407837,
      "learning_rate": 4.299132550980889e-05,
      "loss": 1.7681,
      "step": 104600
    },
    {
      "epoch": 0.4209260977195816,
      "grad_norm": 3.0928220748901367,
      "learning_rate": 4.298462499212689e-05,
      "loss": 1.7907,
      "step": 104700
    },
    {
      "epoch": 0.42132812837642936,
      "grad_norm": 3.5101611614227295,
      "learning_rate": 4.29779244744449e-05,
      "loss": 1.7388,
      "step": 104800
    },
    {
      "epoch": 0.4217301590332771,
      "grad_norm": 3.0921123027801514,
      "learning_rate": 4.2971223956762904e-05,
      "loss": 1.7532,
      "step": 104900
    },
    {
      "epoch": 0.42213218969012484,
      "grad_norm": 3.3299853801727295,
      "learning_rate": 4.296452343908091e-05,
      "loss": 1.7877,
      "step": 105000
    },
    {
      "epoch": 0.4225342203469726,
      "grad_norm": 3.1705806255340576,
      "learning_rate": 4.295782292139891e-05,
      "loss": 1.7891,
      "step": 105100
    },
    {
      "epoch": 0.4229362510038203,
      "grad_norm": 3.8699450492858887,
      "learning_rate": 4.295112240371691e-05,
      "loss": 1.8154,
      "step": 105200
    },
    {
      "epoch": 0.42333828166066806,
      "grad_norm": 2.6018295288085938,
      "learning_rate": 4.294442188603492e-05,
      "loss": 1.7222,
      "step": 105300
    },
    {
      "epoch": 0.4237403123175158,
      "grad_norm": 3.2614071369171143,
      "learning_rate": 4.293772136835292e-05,
      "loss": 1.8075,
      "step": 105400
    },
    {
      "epoch": 0.4241423429743635,
      "grad_norm": 3.2680611610412598,
      "learning_rate": 4.293102085067092e-05,
      "loss": 1.7831,
      "step": 105500
    },
    {
      "epoch": 0.4245443736312112,
      "grad_norm": 2.414133071899414,
      "learning_rate": 4.2924320332988926e-05,
      "loss": 1.8,
      "step": 105600
    },
    {
      "epoch": 0.42494640428805897,
      "grad_norm": 3.0152645111083984,
      "learning_rate": 4.291761981530693e-05,
      "loss": 1.7959,
      "step": 105700
    },
    {
      "epoch": 0.4253484349449067,
      "grad_norm": 3.240842342376709,
      "learning_rate": 4.2910919297624936e-05,
      "loss": 1.8013,
      "step": 105800
    },
    {
      "epoch": 0.42575046560175445,
      "grad_norm": 2.7644855976104736,
      "learning_rate": 4.290421877994294e-05,
      "loss": 1.7746,
      "step": 105900
    },
    {
      "epoch": 0.4261524962586022,
      "grad_norm": 2.960625171661377,
      "learning_rate": 4.2897518262260946e-05,
      "loss": 1.7613,
      "step": 106000
    },
    {
      "epoch": 0.4265545269154499,
      "grad_norm": 3.4563918113708496,
      "learning_rate": 4.289081774457895e-05,
      "loss": 1.8246,
      "step": 106100
    },
    {
      "epoch": 0.42695655757229767,
      "grad_norm": 3.2597217559814453,
      "learning_rate": 4.288411722689695e-05,
      "loss": 1.8061,
      "step": 106200
    },
    {
      "epoch": 0.4273585882291454,
      "grad_norm": 2.977822780609131,
      "learning_rate": 4.2877416709214955e-05,
      "loss": 1.7454,
      "step": 106300
    },
    {
      "epoch": 0.42776061888599315,
      "grad_norm": 3.1783483028411865,
      "learning_rate": 4.287071619153296e-05,
      "loss": 1.7683,
      "step": 106400
    },
    {
      "epoch": 0.4281626495428409,
      "grad_norm": 3.4865353107452393,
      "learning_rate": 4.2864015673850965e-05,
      "loss": 1.7684,
      "step": 106500
    },
    {
      "epoch": 0.42856468019968863,
      "grad_norm": 3.167940855026245,
      "learning_rate": 4.285731515616897e-05,
      "loss": 1.7842,
      "step": 106600
    },
    {
      "epoch": 0.42896671085653637,
      "grad_norm": 2.7068817615509033,
      "learning_rate": 4.285061463848697e-05,
      "loss": 1.7726,
      "step": 106700
    },
    {
      "epoch": 0.4293687415133841,
      "grad_norm": 4.439551830291748,
      "learning_rate": 4.2843914120804974e-05,
      "loss": 1.8051,
      "step": 106800
    },
    {
      "epoch": 0.42977077217023185,
      "grad_norm": 3.043166160583496,
      "learning_rate": 4.283721360312298e-05,
      "loss": 1.7774,
      "step": 106900
    },
    {
      "epoch": 0.4301728028270796,
      "grad_norm": 2.9765331745147705,
      "learning_rate": 4.2830513085440984e-05,
      "loss": 1.7845,
      "step": 107000
    },
    {
      "epoch": 0.43057483348392733,
      "grad_norm": 3.493936538696289,
      "learning_rate": 4.282381256775899e-05,
      "loss": 1.828,
      "step": 107100
    },
    {
      "epoch": 0.4309768641407751,
      "grad_norm": 3.125889778137207,
      "learning_rate": 4.281711205007699e-05,
      "loss": 1.7806,
      "step": 107200
    },
    {
      "epoch": 0.4313788947976228,
      "grad_norm": 3.5563502311706543,
      "learning_rate": 4.281041153239499e-05,
      "loss": 1.79,
      "step": 107300
    },
    {
      "epoch": 0.43178092545447055,
      "grad_norm": 2.6132266521453857,
      "learning_rate": 4.2803711014713e-05,
      "loss": 1.7824,
      "step": 107400
    },
    {
      "epoch": 0.4321829561113183,
      "grad_norm": 3.5280284881591797,
      "learning_rate": 4.2797010497031e-05,
      "loss": 1.7921,
      "step": 107500
    },
    {
      "epoch": 0.432584986768166,
      "grad_norm": 3.844860076904297,
      "learning_rate": 4.279030997934901e-05,
      "loss": 1.812,
      "step": 107600
    },
    {
      "epoch": 0.4329870174250137,
      "grad_norm": 3.277545213699341,
      "learning_rate": 4.278360946166701e-05,
      "loss": 1.8006,
      "step": 107700
    },
    {
      "epoch": 0.43338904808186146,
      "grad_norm": 3.4432833194732666,
      "learning_rate": 4.277690894398502e-05,
      "loss": 1.7935,
      "step": 107800
    },
    {
      "epoch": 0.4337910787387092,
      "grad_norm": 3.060973644256592,
      "learning_rate": 4.277020842630302e-05,
      "loss": 1.7905,
      "step": 107900
    },
    {
      "epoch": 0.43419310939555694,
      "grad_norm": 3.4667088985443115,
      "learning_rate": 4.276350790862102e-05,
      "loss": 1.6945,
      "step": 108000
    },
    {
      "epoch": 0.4345951400524047,
      "grad_norm": 3.4445295333862305,
      "learning_rate": 4.275680739093903e-05,
      "loss": 1.7421,
      "step": 108100
    },
    {
      "epoch": 0.4349971707092524,
      "grad_norm": 3.0647132396698,
      "learning_rate": 4.275010687325703e-05,
      "loss": 1.7508,
      "step": 108200
    },
    {
      "epoch": 0.43539920136610016,
      "grad_norm": 2.976205348968506,
      "learning_rate": 4.274340635557503e-05,
      "loss": 1.7943,
      "step": 108300
    },
    {
      "epoch": 0.4358012320229479,
      "grad_norm": 3.3831310272216797,
      "learning_rate": 4.2736705837893035e-05,
      "loss": 1.7985,
      "step": 108400
    },
    {
      "epoch": 0.43620326267979564,
      "grad_norm": 2.9447405338287354,
      "learning_rate": 4.273000532021104e-05,
      "loss": 1.7926,
      "step": 108500
    },
    {
      "epoch": 0.4366052933366434,
      "grad_norm": 2.5949647426605225,
      "learning_rate": 4.2723304802529046e-05,
      "loss": 1.7869,
      "step": 108600
    },
    {
      "epoch": 0.4370073239934911,
      "grad_norm": 3.359738349914551,
      "learning_rate": 4.271660428484705e-05,
      "loss": 1.771,
      "step": 108700
    },
    {
      "epoch": 0.43740935465033887,
      "grad_norm": 3.1383771896362305,
      "learning_rate": 4.2709903767165056e-05,
      "loss": 1.8131,
      "step": 108800
    },
    {
      "epoch": 0.4378113853071866,
      "grad_norm": 2.870131015777588,
      "learning_rate": 4.270320324948306e-05,
      "loss": 1.7478,
      "step": 108900
    },
    {
      "epoch": 0.43821341596403435,
      "grad_norm": 3.4822943210601807,
      "learning_rate": 4.2696502731801066e-05,
      "loss": 1.8012,
      "step": 109000
    },
    {
      "epoch": 0.4386154466208821,
      "grad_norm": 3.445251941680908,
      "learning_rate": 4.2689802214119065e-05,
      "loss": 1.7702,
      "step": 109100
    },
    {
      "epoch": 0.43901747727772983,
      "grad_norm": 3.1632277965545654,
      "learning_rate": 4.268310169643707e-05,
      "loss": 1.7843,
      "step": 109200
    },
    {
      "epoch": 0.43941950793457757,
      "grad_norm": 3.2457115650177,
      "learning_rate": 4.267640117875507e-05,
      "loss": 1.7971,
      "step": 109300
    },
    {
      "epoch": 0.4398215385914253,
      "grad_norm": 3.2022886276245117,
      "learning_rate": 4.266970066107307e-05,
      "loss": 1.7547,
      "step": 109400
    },
    {
      "epoch": 0.44022356924827305,
      "grad_norm": 2.453667640686035,
      "learning_rate": 4.266300014339108e-05,
      "loss": 1.7707,
      "step": 109500
    },
    {
      "epoch": 0.4406255999051208,
      "grad_norm": 2.333726644515991,
      "learning_rate": 4.2656299625709083e-05,
      "loss": 1.7423,
      "step": 109600
    },
    {
      "epoch": 0.4410276305619685,
      "grad_norm": 3.3171422481536865,
      "learning_rate": 4.264959910802709e-05,
      "loss": 1.7451,
      "step": 109700
    },
    {
      "epoch": 0.4414296612188162,
      "grad_norm": 3.5582451820373535,
      "learning_rate": 4.2642898590345094e-05,
      "loss": 1.7853,
      "step": 109800
    },
    {
      "epoch": 0.44183169187566396,
      "grad_norm": 3.9716854095458984,
      "learning_rate": 4.26361980726631e-05,
      "loss": 1.7512,
      "step": 109900
    },
    {
      "epoch": 0.4422337225325117,
      "grad_norm": 3.537580728530884,
      "learning_rate": 4.2629497554981104e-05,
      "loss": 1.7451,
      "step": 110000
    },
    {
      "epoch": 0.44263575318935944,
      "grad_norm": 3.4425370693206787,
      "learning_rate": 4.262279703729911e-05,
      "loss": 1.8159,
      "step": 110100
    },
    {
      "epoch": 0.4430377838462072,
      "grad_norm": 3.411088466644287,
      "learning_rate": 4.261609651961711e-05,
      "loss": 1.7125,
      "step": 110200
    },
    {
      "epoch": 0.4434398145030549,
      "grad_norm": 3.5784826278686523,
      "learning_rate": 4.2609396001935106e-05,
      "loss": 1.7858,
      "step": 110300
    },
    {
      "epoch": 0.44384184515990266,
      "grad_norm": 3.1305816173553467,
      "learning_rate": 4.260269548425311e-05,
      "loss": 1.7375,
      "step": 110400
    },
    {
      "epoch": 0.4442438758167504,
      "grad_norm": 3.0209407806396484,
      "learning_rate": 4.2595994966571116e-05,
      "loss": 1.7836,
      "step": 110500
    },
    {
      "epoch": 0.44464590647359814,
      "grad_norm": 3.1603622436523438,
      "learning_rate": 4.258929444888912e-05,
      "loss": 1.7646,
      "step": 110600
    },
    {
      "epoch": 0.4450479371304459,
      "grad_norm": 3.984053611755371,
      "learning_rate": 4.2582593931207126e-05,
      "loss": 1.7395,
      "step": 110700
    },
    {
      "epoch": 0.4454499677872936,
      "grad_norm": 3.24119234085083,
      "learning_rate": 4.257589341352513e-05,
      "loss": 1.7744,
      "step": 110800
    },
    {
      "epoch": 0.44585199844414136,
      "grad_norm": 2.9965169429779053,
      "learning_rate": 4.2569192895843137e-05,
      "loss": 1.7205,
      "step": 110900
    },
    {
      "epoch": 0.4462540291009891,
      "grad_norm": 3.2686634063720703,
      "learning_rate": 4.256249237816114e-05,
      "loss": 1.7949,
      "step": 111000
    },
    {
      "epoch": 0.44665605975783684,
      "grad_norm": 3.6155943870544434,
      "learning_rate": 4.255579186047915e-05,
      "loss": 1.8035,
      "step": 111100
    },
    {
      "epoch": 0.4470580904146846,
      "grad_norm": 2.7630105018615723,
      "learning_rate": 4.2549091342797145e-05,
      "loss": 1.7416,
      "step": 111200
    },
    {
      "epoch": 0.4474601210715323,
      "grad_norm": 3.1718714237213135,
      "learning_rate": 4.254239082511515e-05,
      "loss": 1.7652,
      "step": 111300
    },
    {
      "epoch": 0.44786215172838006,
      "grad_norm": 2.6092910766601562,
      "learning_rate": 4.2535690307433155e-05,
      "loss": 1.8025,
      "step": 111400
    },
    {
      "epoch": 0.4482641823852278,
      "grad_norm": 3.586480140686035,
      "learning_rate": 4.2528989789751154e-05,
      "loss": 1.7774,
      "step": 111500
    },
    {
      "epoch": 0.44866621304207555,
      "grad_norm": 3.131269693374634,
      "learning_rate": 4.252228927206916e-05,
      "loss": 1.8019,
      "step": 111600
    },
    {
      "epoch": 0.4490682436989233,
      "grad_norm": 3.108201026916504,
      "learning_rate": 4.2515588754387164e-05,
      "loss": 1.7984,
      "step": 111700
    },
    {
      "epoch": 0.449470274355771,
      "grad_norm": 3.664534091949463,
      "learning_rate": 4.250888823670517e-05,
      "loss": 1.7975,
      "step": 111800
    },
    {
      "epoch": 0.4498723050126187,
      "grad_norm": 3.0803298950195312,
      "learning_rate": 4.2502187719023174e-05,
      "loss": 1.7577,
      "step": 111900
    },
    {
      "epoch": 0.45027433566946645,
      "grad_norm": 3.6929519176483154,
      "learning_rate": 4.249548720134118e-05,
      "loss": 1.7999,
      "step": 112000
    },
    {
      "epoch": 0.4506763663263142,
      "grad_norm": 3.26412034034729,
      "learning_rate": 4.2488786683659185e-05,
      "loss": 1.7404,
      "step": 112100
    },
    {
      "epoch": 0.45107839698316193,
      "grad_norm": 3.327955961227417,
      "learning_rate": 4.248208616597719e-05,
      "loss": 1.8112,
      "step": 112200
    },
    {
      "epoch": 0.4514804276400097,
      "grad_norm": 3.0636448860168457,
      "learning_rate": 4.247538564829519e-05,
      "loss": 1.794,
      "step": 112300
    },
    {
      "epoch": 0.4518824582968574,
      "grad_norm": 3.20741605758667,
      "learning_rate": 4.246868513061319e-05,
      "loss": 1.7879,
      "step": 112400
    },
    {
      "epoch": 0.45228448895370515,
      "grad_norm": 2.943671226501465,
      "learning_rate": 4.24619846129312e-05,
      "loss": 1.7929,
      "step": 112500
    },
    {
      "epoch": 0.4526865196105529,
      "grad_norm": 3.664513111114502,
      "learning_rate": 4.24552840952492e-05,
      "loss": 1.7789,
      "step": 112600
    },
    {
      "epoch": 0.45308855026740064,
      "grad_norm": 3.3859381675720215,
      "learning_rate": 4.24485835775672e-05,
      "loss": 1.7298,
      "step": 112700
    },
    {
      "epoch": 0.4534905809242484,
      "grad_norm": 3.3900809288024902,
      "learning_rate": 4.244188305988521e-05,
      "loss": 1.7801,
      "step": 112800
    },
    {
      "epoch": 0.4538926115810961,
      "grad_norm": 4.028761863708496,
      "learning_rate": 4.243518254220321e-05,
      "loss": 1.7422,
      "step": 112900
    },
    {
      "epoch": 0.45429464223794386,
      "grad_norm": 3.1422791481018066,
      "learning_rate": 4.242848202452122e-05,
      "loss": 1.7696,
      "step": 113000
    },
    {
      "epoch": 0.4546966728947916,
      "grad_norm": 3.699190855026245,
      "learning_rate": 4.242178150683922e-05,
      "loss": 1.7766,
      "step": 113100
    },
    {
      "epoch": 0.45509870355163934,
      "grad_norm": 3.0257790088653564,
      "learning_rate": 4.241508098915723e-05,
      "loss": 1.7649,
      "step": 113200
    },
    {
      "epoch": 0.4555007342084871,
      "grad_norm": 3.3359484672546387,
      "learning_rate": 4.2408380471475226e-05,
      "loss": 1.8291,
      "step": 113300
    },
    {
      "epoch": 0.4559027648653348,
      "grad_norm": 3.295969009399414,
      "learning_rate": 4.240167995379323e-05,
      "loss": 1.7652,
      "step": 113400
    },
    {
      "epoch": 0.45630479552218256,
      "grad_norm": 2.6819584369659424,
      "learning_rate": 4.2394979436111236e-05,
      "loss": 1.7168,
      "step": 113500
    },
    {
      "epoch": 0.4567068261790303,
      "grad_norm": 2.9972259998321533,
      "learning_rate": 4.238827891842924e-05,
      "loss": 1.7826,
      "step": 113600
    },
    {
      "epoch": 0.45710885683587804,
      "grad_norm": 3.182366132736206,
      "learning_rate": 4.2381578400747246e-05,
      "loss": 1.7709,
      "step": 113700
    },
    {
      "epoch": 0.4575108874927258,
      "grad_norm": 3.269637107849121,
      "learning_rate": 4.237487788306525e-05,
      "loss": 1.7732,
      "step": 113800
    },
    {
      "epoch": 0.4579129181495735,
      "grad_norm": 3.331895589828491,
      "learning_rate": 4.236817736538325e-05,
      "loss": 1.7979,
      "step": 113900
    },
    {
      "epoch": 0.4583149488064212,
      "grad_norm": 2.8017055988311768,
      "learning_rate": 4.2361476847701255e-05,
      "loss": 1.7333,
      "step": 114000
    },
    {
      "epoch": 0.45871697946326895,
      "grad_norm": 2.9042272567749023,
      "learning_rate": 4.235477633001926e-05,
      "loss": 1.7623,
      "step": 114100
    },
    {
      "epoch": 0.4591190101201167,
      "grad_norm": 2.744853973388672,
      "learning_rate": 4.2348075812337265e-05,
      "loss": 1.7251,
      "step": 114200
    },
    {
      "epoch": 0.45952104077696443,
      "grad_norm": 2.995959758758545,
      "learning_rate": 4.2341375294655263e-05,
      "loss": 1.7239,
      "step": 114300
    },
    {
      "epoch": 0.45992307143381217,
      "grad_norm": 3.2256600856781006,
      "learning_rate": 4.233467477697327e-05,
      "loss": 1.775,
      "step": 114400
    },
    {
      "epoch": 0.4603251020906599,
      "grad_norm": 3.1829726696014404,
      "learning_rate": 4.2327974259291274e-05,
      "loss": 1.8024,
      "step": 114500
    },
    {
      "epoch": 0.46072713274750765,
      "grad_norm": 3.0459415912628174,
      "learning_rate": 4.232127374160928e-05,
      "loss": 1.7708,
      "step": 114600
    },
    {
      "epoch": 0.4611291634043554,
      "grad_norm": 3.4562630653381348,
      "learning_rate": 4.2314573223927284e-05,
      "loss": 1.7535,
      "step": 114700
    },
    {
      "epoch": 0.46153119406120313,
      "grad_norm": 2.8961219787597656,
      "learning_rate": 4.230787270624529e-05,
      "loss": 1.7802,
      "step": 114800
    },
    {
      "epoch": 0.46193322471805087,
      "grad_norm": 3.4426510334014893,
      "learning_rate": 4.2301172188563294e-05,
      "loss": 1.7514,
      "step": 114900
    },
    {
      "epoch": 0.4623352553748986,
      "grad_norm": 3.6086301803588867,
      "learning_rate": 4.229447167088129e-05,
      "loss": 1.7677,
      "step": 115000
    },
    {
      "epoch": 0.46273728603174635,
      "grad_norm": 3.0623831748962402,
      "learning_rate": 4.22877711531993e-05,
      "loss": 1.7704,
      "step": 115100
    },
    {
      "epoch": 0.4631393166885941,
      "grad_norm": 3.1433537006378174,
      "learning_rate": 4.22810706355173e-05,
      "loss": 1.7609,
      "step": 115200
    },
    {
      "epoch": 0.46354134734544183,
      "grad_norm": 2.770667552947998,
      "learning_rate": 4.227437011783531e-05,
      "loss": 1.7446,
      "step": 115300
    },
    {
      "epoch": 0.4639433780022896,
      "grad_norm": 3.1626837253570557,
      "learning_rate": 4.2267669600153306e-05,
      "loss": 1.751,
      "step": 115400
    },
    {
      "epoch": 0.4643454086591373,
      "grad_norm": 3.032489538192749,
      "learning_rate": 4.226096908247131e-05,
      "loss": 1.7322,
      "step": 115500
    },
    {
      "epoch": 0.46474743931598506,
      "grad_norm": 3.1698107719421387,
      "learning_rate": 4.2254268564789316e-05,
      "loss": 1.7681,
      "step": 115600
    },
    {
      "epoch": 0.4651494699728328,
      "grad_norm": 2.8233110904693604,
      "learning_rate": 4.224756804710732e-05,
      "loss": 1.7613,
      "step": 115700
    },
    {
      "epoch": 0.46555150062968054,
      "grad_norm": 2.9677813053131104,
      "learning_rate": 4.224086752942533e-05,
      "loss": 1.7679,
      "step": 115800
    },
    {
      "epoch": 0.4659535312865283,
      "grad_norm": 3.141658067703247,
      "learning_rate": 4.223416701174333e-05,
      "loss": 1.7609,
      "step": 115900
    },
    {
      "epoch": 0.466355561943376,
      "grad_norm": 2.9646034240722656,
      "learning_rate": 4.222746649406134e-05,
      "loss": 1.772,
      "step": 116000
    },
    {
      "epoch": 0.4667575926002237,
      "grad_norm": 2.725761651992798,
      "learning_rate": 4.222076597637934e-05,
      "loss": 1.7859,
      "step": 116100
    },
    {
      "epoch": 0.46715962325707144,
      "grad_norm": 3.1380443572998047,
      "learning_rate": 4.221406545869734e-05,
      "loss": 1.7687,
      "step": 116200
    },
    {
      "epoch": 0.4675616539139192,
      "grad_norm": 2.6949315071105957,
      "learning_rate": 4.2207364941015346e-05,
      "loss": 1.7908,
      "step": 116300
    },
    {
      "epoch": 0.4679636845707669,
      "grad_norm": 3.0031611919403076,
      "learning_rate": 4.2200664423333344e-05,
      "loss": 1.7823,
      "step": 116400
    },
    {
      "epoch": 0.46836571522761467,
      "grad_norm": 3.860628843307495,
      "learning_rate": 4.219396390565135e-05,
      "loss": 1.8079,
      "step": 116500
    },
    {
      "epoch": 0.4687677458844624,
      "grad_norm": 3.5991337299346924,
      "learning_rate": 4.2187263387969354e-05,
      "loss": 1.7963,
      "step": 116600
    },
    {
      "epoch": 0.46916977654131015,
      "grad_norm": 2.709282636642456,
      "learning_rate": 4.218056287028736e-05,
      "loss": 1.7562,
      "step": 116700
    },
    {
      "epoch": 0.4695718071981579,
      "grad_norm": 3.0871286392211914,
      "learning_rate": 4.2173862352605364e-05,
      "loss": 1.765,
      "step": 116800
    },
    {
      "epoch": 0.4699738378550056,
      "grad_norm": 2.8928070068359375,
      "learning_rate": 4.216716183492337e-05,
      "loss": 1.7523,
      "step": 116900
    },
    {
      "epoch": 0.47037586851185337,
      "grad_norm": 3.0833418369293213,
      "learning_rate": 4.2160461317241375e-05,
      "loss": 1.7959,
      "step": 117000
    },
    {
      "epoch": 0.4707778991687011,
      "grad_norm": 3.4789023399353027,
      "learning_rate": 4.215376079955938e-05,
      "loss": 1.7438,
      "step": 117100
    },
    {
      "epoch": 0.47117992982554885,
      "grad_norm": 4.074592590332031,
      "learning_rate": 4.2147060281877385e-05,
      "loss": 1.7613,
      "step": 117200
    },
    {
      "epoch": 0.4715819604823966,
      "grad_norm": 2.908097743988037,
      "learning_rate": 4.214035976419538e-05,
      "loss": 1.7407,
      "step": 117300
    },
    {
      "epoch": 0.47198399113924433,
      "grad_norm": 2.7902729511260986,
      "learning_rate": 4.213365924651338e-05,
      "loss": 1.7828,
      "step": 117400
    },
    {
      "epoch": 0.47238602179609207,
      "grad_norm": 2.9359333515167236,
      "learning_rate": 4.212695872883139e-05,
      "loss": 1.7797,
      "step": 117500
    },
    {
      "epoch": 0.4727880524529398,
      "grad_norm": 3.2475414276123047,
      "learning_rate": 4.212025821114939e-05,
      "loss": 1.7853,
      "step": 117600
    },
    {
      "epoch": 0.47319008310978755,
      "grad_norm": 2.8194544315338135,
      "learning_rate": 4.21135576934674e-05,
      "loss": 1.7515,
      "step": 117700
    },
    {
      "epoch": 0.4735921137666353,
      "grad_norm": 3.1752448081970215,
      "learning_rate": 4.21068571757854e-05,
      "loss": 1.727,
      "step": 117800
    },
    {
      "epoch": 0.47399414442348303,
      "grad_norm": 3.465487480163574,
      "learning_rate": 4.210015665810341e-05,
      "loss": 1.741,
      "step": 117900
    },
    {
      "epoch": 0.4743961750803308,
      "grad_norm": 2.84594464302063,
      "learning_rate": 4.209345614042141e-05,
      "loss": 1.7699,
      "step": 118000
    },
    {
      "epoch": 0.4747982057371785,
      "grad_norm": 2.6902573108673096,
      "learning_rate": 4.208675562273942e-05,
      "loss": 1.7997,
      "step": 118100
    },
    {
      "epoch": 0.47520023639402625,
      "grad_norm": 3.0391173362731934,
      "learning_rate": 4.208005510505742e-05,
      "loss": 1.7415,
      "step": 118200
    },
    {
      "epoch": 0.47560226705087394,
      "grad_norm": 2.780961513519287,
      "learning_rate": 4.207335458737542e-05,
      "loss": 1.7484,
      "step": 118300
    },
    {
      "epoch": 0.4760042977077217,
      "grad_norm": 3.3127360343933105,
      "learning_rate": 4.2066654069693426e-05,
      "loss": 1.7225,
      "step": 118400
    },
    {
      "epoch": 0.4764063283645694,
      "grad_norm": 3.3838424682617188,
      "learning_rate": 4.205995355201143e-05,
      "loss": 1.7471,
      "step": 118500
    },
    {
      "epoch": 0.47680835902141716,
      "grad_norm": 3.5251927375793457,
      "learning_rate": 4.205325303432943e-05,
      "loss": 1.7605,
      "step": 118600
    },
    {
      "epoch": 0.4772103896782649,
      "grad_norm": 3.3695261478424072,
      "learning_rate": 4.2046552516647435e-05,
      "loss": 1.7728,
      "step": 118700
    },
    {
      "epoch": 0.47761242033511264,
      "grad_norm": 3.47489333152771,
      "learning_rate": 4.203985199896544e-05,
      "loss": 1.7623,
      "step": 118800
    },
    {
      "epoch": 0.4780144509919604,
      "grad_norm": 3.071451187133789,
      "learning_rate": 4.2033151481283445e-05,
      "loss": 1.7207,
      "step": 118900
    },
    {
      "epoch": 0.4784164816488081,
      "grad_norm": 3.198798418045044,
      "learning_rate": 4.202645096360145e-05,
      "loss": 1.7574,
      "step": 119000
    },
    {
      "epoch": 0.47881851230565586,
      "grad_norm": 2.676152229309082,
      "learning_rate": 4.2019750445919455e-05,
      "loss": 1.7111,
      "step": 119100
    },
    {
      "epoch": 0.4792205429625036,
      "grad_norm": 2.869650363922119,
      "learning_rate": 4.201304992823746e-05,
      "loss": 1.7478,
      "step": 119200
    },
    {
      "epoch": 0.47962257361935134,
      "grad_norm": 3.524693727493286,
      "learning_rate": 4.2006349410555466e-05,
      "loss": 1.7646,
      "step": 119300
    },
    {
      "epoch": 0.4800246042761991,
      "grad_norm": 3.2246761322021484,
      "learning_rate": 4.1999648892873464e-05,
      "loss": 1.7262,
      "step": 119400
    },
    {
      "epoch": 0.4804266349330468,
      "grad_norm": 3.4084010124206543,
      "learning_rate": 4.199294837519147e-05,
      "loss": 1.7109,
      "step": 119500
    },
    {
      "epoch": 0.48082866558989457,
      "grad_norm": 3.0394206047058105,
      "learning_rate": 4.1986247857509474e-05,
      "loss": 1.7557,
      "step": 119600
    },
    {
      "epoch": 0.4812306962467423,
      "grad_norm": 2.7831497192382812,
      "learning_rate": 4.197954733982748e-05,
      "loss": 1.7058,
      "step": 119700
    },
    {
      "epoch": 0.48163272690359005,
      "grad_norm": 2.9380006790161133,
      "learning_rate": 4.197284682214548e-05,
      "loss": 1.714,
      "step": 119800
    },
    {
      "epoch": 0.4820347575604378,
      "grad_norm": 3.002058267593384,
      "learning_rate": 4.196614630446348e-05,
      "loss": 1.7476,
      "step": 119900
    },
    {
      "epoch": 0.48243678821728553,
      "grad_norm": 2.958350896835327,
      "learning_rate": 4.195944578678149e-05,
      "loss": 1.7451,
      "step": 120000
    },
    {
      "epoch": 0.48283881887413327,
      "grad_norm": 3.4142231941223145,
      "learning_rate": 4.195274526909949e-05,
      "loss": 1.6941,
      "step": 120100
    },
    {
      "epoch": 0.483240849530981,
      "grad_norm": 3.0693228244781494,
      "learning_rate": 4.19460447514175e-05,
      "loss": 1.7497,
      "step": 120200
    },
    {
      "epoch": 0.48364288018782875,
      "grad_norm": 3.4250049591064453,
      "learning_rate": 4.19393442337355e-05,
      "loss": 1.7316,
      "step": 120300
    },
    {
      "epoch": 0.48404491084467643,
      "grad_norm": 3.4475250244140625,
      "learning_rate": 4.19326437160535e-05,
      "loss": 1.7894,
      "step": 120400
    },
    {
      "epoch": 0.4844469415015242,
      "grad_norm": 3.202866792678833,
      "learning_rate": 4.192594319837151e-05,
      "loss": 1.742,
      "step": 120500
    },
    {
      "epoch": 0.4848489721583719,
      "grad_norm": 3.3441123962402344,
      "learning_rate": 4.191924268068951e-05,
      "loss": 1.7446,
      "step": 120600
    },
    {
      "epoch": 0.48525100281521966,
      "grad_norm": 2.887819290161133,
      "learning_rate": 4.191254216300752e-05,
      "loss": 1.7361,
      "step": 120700
    },
    {
      "epoch": 0.4856530334720674,
      "grad_norm": 3.0614395141601562,
      "learning_rate": 4.190584164532552e-05,
      "loss": 1.7307,
      "step": 120800
    },
    {
      "epoch": 0.48605506412891514,
      "grad_norm": 3.075875759124756,
      "learning_rate": 4.189914112764353e-05,
      "loss": 1.7105,
      "step": 120900
    },
    {
      "epoch": 0.4864570947857629,
      "grad_norm": 2.968712091445923,
      "learning_rate": 4.1892440609961526e-05,
      "loss": 1.7708,
      "step": 121000
    },
    {
      "epoch": 0.4868591254426106,
      "grad_norm": 3.069005250930786,
      "learning_rate": 4.188574009227953e-05,
      "loss": 1.7695,
      "step": 121100
    },
    {
      "epoch": 0.48726115609945836,
      "grad_norm": 3.5204200744628906,
      "learning_rate": 4.1879039574597536e-05,
      "loss": 1.7567,
      "step": 121200
    },
    {
      "epoch": 0.4876631867563061,
      "grad_norm": 3.1703038215637207,
      "learning_rate": 4.187233905691554e-05,
      "loss": 1.7202,
      "step": 121300
    },
    {
      "epoch": 0.48806521741315384,
      "grad_norm": 3.1756808757781982,
      "learning_rate": 4.186563853923354e-05,
      "loss": 1.7196,
      "step": 121400
    },
    {
      "epoch": 0.4884672480700016,
      "grad_norm": 3.1494593620300293,
      "learning_rate": 4.1858938021551544e-05,
      "loss": 1.7567,
      "step": 121500
    },
    {
      "epoch": 0.4888692787268493,
      "grad_norm": 3.524096727371216,
      "learning_rate": 4.185223750386955e-05,
      "loss": 1.7036,
      "step": 121600
    },
    {
      "epoch": 0.48927130938369706,
      "grad_norm": 3.2965471744537354,
      "learning_rate": 4.1845536986187555e-05,
      "loss": 1.7853,
      "step": 121700
    },
    {
      "epoch": 0.4896733400405448,
      "grad_norm": 3.2665083408355713,
      "learning_rate": 4.183883646850556e-05,
      "loss": 1.6853,
      "step": 121800
    },
    {
      "epoch": 0.49007537069739254,
      "grad_norm": 2.809431552886963,
      "learning_rate": 4.1832135950823565e-05,
      "loss": 1.8035,
      "step": 121900
    },
    {
      "epoch": 0.4904774013542403,
      "grad_norm": 3.771193742752075,
      "learning_rate": 4.182543543314157e-05,
      "loss": 1.7725,
      "step": 122000
    },
    {
      "epoch": 0.490879432011088,
      "grad_norm": 3.4267075061798096,
      "learning_rate": 4.1818734915459575e-05,
      "loss": 1.7764,
      "step": 122100
    },
    {
      "epoch": 0.49128146266793576,
      "grad_norm": 3.4238955974578857,
      "learning_rate": 4.1812034397777574e-05,
      "loss": 1.7707,
      "step": 122200
    },
    {
      "epoch": 0.4916834933247835,
      "grad_norm": 3.081538200378418,
      "learning_rate": 4.180533388009558e-05,
      "loss": 1.7284,
      "step": 122300
    },
    {
      "epoch": 0.49208552398163125,
      "grad_norm": 3.783576726913452,
      "learning_rate": 4.1798633362413584e-05,
      "loss": 1.7325,
      "step": 122400
    },
    {
      "epoch": 0.49248755463847893,
      "grad_norm": 2.978440523147583,
      "learning_rate": 4.179193284473158e-05,
      "loss": 1.8028,
      "step": 122500
    },
    {
      "epoch": 0.49288958529532667,
      "grad_norm": 3.6709768772125244,
      "learning_rate": 4.178523232704959e-05,
      "loss": 1.7379,
      "step": 122600
    },
    {
      "epoch": 0.4932916159521744,
      "grad_norm": 3.1534857749938965,
      "learning_rate": 4.177853180936759e-05,
      "loss": 1.7334,
      "step": 122700
    },
    {
      "epoch": 0.49369364660902215,
      "grad_norm": 2.959597110748291,
      "learning_rate": 4.17718312916856e-05,
      "loss": 1.74,
      "step": 122800
    },
    {
      "epoch": 0.4940956772658699,
      "grad_norm": 3.8790745735168457,
      "learning_rate": 4.17651307740036e-05,
      "loss": 1.7489,
      "step": 122900
    },
    {
      "epoch": 0.49449770792271763,
      "grad_norm": 3.3067984580993652,
      "learning_rate": 4.175843025632161e-05,
      "loss": 1.7555,
      "step": 123000
    },
    {
      "epoch": 0.4948997385795654,
      "grad_norm": 3.458765745162964,
      "learning_rate": 4.175172973863961e-05,
      "loss": 1.7256,
      "step": 123100
    },
    {
      "epoch": 0.4953017692364131,
      "grad_norm": 2.9761905670166016,
      "learning_rate": 4.174502922095762e-05,
      "loss": 1.7782,
      "step": 123200
    },
    {
      "epoch": 0.49570379989326085,
      "grad_norm": 2.9264533519744873,
      "learning_rate": 4.173832870327562e-05,
      "loss": 1.7941,
      "step": 123300
    },
    {
      "epoch": 0.4961058305501086,
      "grad_norm": 3.547785997390747,
      "learning_rate": 4.173162818559362e-05,
      "loss": 1.7935,
      "step": 123400
    },
    {
      "epoch": 0.49650786120695634,
      "grad_norm": 2.9625437259674072,
      "learning_rate": 4.172492766791162e-05,
      "loss": 1.7603,
      "step": 123500
    },
    {
      "epoch": 0.4969098918638041,
      "grad_norm": 3.0063364505767822,
      "learning_rate": 4.1718227150229625e-05,
      "loss": 1.7452,
      "step": 123600
    },
    {
      "epoch": 0.4973119225206518,
      "grad_norm": 3.2361438274383545,
      "learning_rate": 4.171152663254763e-05,
      "loss": 1.7264,
      "step": 123700
    },
    {
      "epoch": 0.49771395317749956,
      "grad_norm": 3.318920135498047,
      "learning_rate": 4.1704826114865635e-05,
      "loss": 1.7538,
      "step": 123800
    },
    {
      "epoch": 0.4981159838343473,
      "grad_norm": 2.4317054748535156,
      "learning_rate": 4.169812559718364e-05,
      "loss": 1.7705,
      "step": 123900
    },
    {
      "epoch": 0.49851801449119504,
      "grad_norm": 3.081684112548828,
      "learning_rate": 4.1691425079501645e-05,
      "loss": 1.755,
      "step": 124000
    },
    {
      "epoch": 0.4989200451480428,
      "grad_norm": 3.994189977645874,
      "learning_rate": 4.168472456181965e-05,
      "loss": 1.7786,
      "step": 124100
    },
    {
      "epoch": 0.4993220758048905,
      "grad_norm": 3.4402523040771484,
      "learning_rate": 4.1678024044137656e-05,
      "loss": 1.7253,
      "step": 124200
    },
    {
      "epoch": 0.49972410646173826,
      "grad_norm": 2.911402463912964,
      "learning_rate": 4.167132352645566e-05,
      "loss": 1.7174,
      "step": 124300
    },
    {
      "epoch": 0.500126137118586,
      "grad_norm": 2.980804443359375,
      "learning_rate": 4.166462300877366e-05,
      "loss": 1.7386,
      "step": 124400
    },
    {
      "epoch": 0.5005281677754337,
      "grad_norm": 3.3994600772857666,
      "learning_rate": 4.1657922491091664e-05,
      "loss": 1.7838,
      "step": 124500
    },
    {
      "epoch": 0.5009301984322815,
      "grad_norm": 3.2573397159576416,
      "learning_rate": 4.165122197340966e-05,
      "loss": 1.7496,
      "step": 124600
    },
    {
      "epoch": 0.5013322290891292,
      "grad_norm": 3.4179227352142334,
      "learning_rate": 4.164452145572767e-05,
      "loss": 1.7655,
      "step": 124700
    },
    {
      "epoch": 0.501734259745977,
      "grad_norm": 3.086466073989868,
      "learning_rate": 4.163782093804567e-05,
      "loss": 1.7716,
      "step": 124800
    },
    {
      "epoch": 0.5021362904028247,
      "grad_norm": 2.5287694931030273,
      "learning_rate": 4.163112042036368e-05,
      "loss": 1.702,
      "step": 124900
    },
    {
      "epoch": 0.5025383210596724,
      "grad_norm": 3.013420581817627,
      "learning_rate": 4.162441990268168e-05,
      "loss": 1.7443,
      "step": 125000
    },
    {
      "epoch": 0.5029403517165202,
      "grad_norm": 3.4330060482025146,
      "learning_rate": 4.161771938499969e-05,
      "loss": 1.6986,
      "step": 125100
    },
    {
      "epoch": 0.5033423823733679,
      "grad_norm": 2.917484760284424,
      "learning_rate": 4.1611018867317693e-05,
      "loss": 1.7986,
      "step": 125200
    },
    {
      "epoch": 0.5037444130302157,
      "grad_norm": 3.491528034210205,
      "learning_rate": 4.16043183496357e-05,
      "loss": 1.716,
      "step": 125300
    },
    {
      "epoch": 0.5041464436870634,
      "grad_norm": 3.4556407928466797,
      "learning_rate": 4.15976178319537e-05,
      "loss": 1.7246,
      "step": 125400
    },
    {
      "epoch": 0.5045484743439111,
      "grad_norm": 2.6827075481414795,
      "learning_rate": 4.15909173142717e-05,
      "loss": 1.7315,
      "step": 125500
    },
    {
      "epoch": 0.5049505050007589,
      "grad_norm": 3.6937005519866943,
      "learning_rate": 4.158421679658971e-05,
      "loss": 1.7433,
      "step": 125600
    },
    {
      "epoch": 0.5053525356576065,
      "grad_norm": 3.74568772315979,
      "learning_rate": 4.1577516278907705e-05,
      "loss": 1.7542,
      "step": 125700
    },
    {
      "epoch": 0.5057545663144543,
      "grad_norm": 4.106205940246582,
      "learning_rate": 4.157081576122571e-05,
      "loss": 1.7445,
      "step": 125800
    },
    {
      "epoch": 0.506156596971302,
      "grad_norm": 3.5398900508880615,
      "learning_rate": 4.1564115243543716e-05,
      "loss": 1.7803,
      "step": 125900
    },
    {
      "epoch": 0.5065586276281497,
      "grad_norm": 3.2636280059814453,
      "learning_rate": 4.155741472586172e-05,
      "loss": 1.7253,
      "step": 126000
    },
    {
      "epoch": 0.5069606582849975,
      "grad_norm": 3.504011631011963,
      "learning_rate": 4.1550714208179726e-05,
      "loss": 1.7447,
      "step": 126100
    },
    {
      "epoch": 0.5073626889418452,
      "grad_norm": 3.2416505813598633,
      "learning_rate": 4.154401369049773e-05,
      "loss": 1.7437,
      "step": 126200
    },
    {
      "epoch": 0.507764719598693,
      "grad_norm": 2.7163913249969482,
      "learning_rate": 4.1537313172815736e-05,
      "loss": 1.767,
      "step": 126300
    },
    {
      "epoch": 0.5081667502555407,
      "grad_norm": 3.6537632942199707,
      "learning_rate": 4.153061265513374e-05,
      "loss": 1.7348,
      "step": 126400
    },
    {
      "epoch": 0.5085687809123884,
      "grad_norm": 3.061197519302368,
      "learning_rate": 4.152391213745174e-05,
      "loss": 1.7534,
      "step": 126500
    },
    {
      "epoch": 0.5089708115692362,
      "grad_norm": 3.220986843109131,
      "learning_rate": 4.1517211619769745e-05,
      "loss": 1.7207,
      "step": 126600
    },
    {
      "epoch": 0.5093728422260839,
      "grad_norm": 3.1139841079711914,
      "learning_rate": 4.151051110208775e-05,
      "loss": 1.754,
      "step": 126700
    },
    {
      "epoch": 0.5097748728829317,
      "grad_norm": 2.8698086738586426,
      "learning_rate": 4.1503810584405755e-05,
      "loss": 1.7353,
      "step": 126800
    },
    {
      "epoch": 0.5101769035397794,
      "grad_norm": 3.234926700592041,
      "learning_rate": 4.1497110066723753e-05,
      "loss": 1.7748,
      "step": 126900
    },
    {
      "epoch": 0.5105789341966271,
      "grad_norm": 3.1826891899108887,
      "learning_rate": 4.149040954904176e-05,
      "loss": 1.7375,
      "step": 127000
    },
    {
      "epoch": 0.5109809648534749,
      "grad_norm": 2.778557538986206,
      "learning_rate": 4.1483709031359764e-05,
      "loss": 1.7386,
      "step": 127100
    },
    {
      "epoch": 0.5113829955103226,
      "grad_norm": 3.5718650817871094,
      "learning_rate": 4.147700851367777e-05,
      "loss": 1.756,
      "step": 127200
    },
    {
      "epoch": 0.5117850261671704,
      "grad_norm": 3.524609327316284,
      "learning_rate": 4.1470307995995774e-05,
      "loss": 1.7883,
      "step": 127300
    },
    {
      "epoch": 0.5121870568240181,
      "grad_norm": 3.366783857345581,
      "learning_rate": 4.146360747831378e-05,
      "loss": 1.7698,
      "step": 127400
    },
    {
      "epoch": 0.5125890874808658,
      "grad_norm": 2.612945556640625,
      "learning_rate": 4.145690696063178e-05,
      "loss": 1.6823,
      "step": 127500
    },
    {
      "epoch": 0.5129911181377136,
      "grad_norm": 3.0775086879730225,
      "learning_rate": 4.145020644294978e-05,
      "loss": 1.775,
      "step": 127600
    },
    {
      "epoch": 0.5133931487945613,
      "grad_norm": 3.0277116298675537,
      "learning_rate": 4.144350592526779e-05,
      "loss": 1.7042,
      "step": 127700
    },
    {
      "epoch": 0.5137951794514091,
      "grad_norm": 3.3174943923950195,
      "learning_rate": 4.143680540758579e-05,
      "loss": 1.7877,
      "step": 127800
    },
    {
      "epoch": 0.5141972101082568,
      "grad_norm": 2.90087628364563,
      "learning_rate": 4.14301048899038e-05,
      "loss": 1.6939,
      "step": 127900
    },
    {
      "epoch": 0.5145992407651045,
      "grad_norm": 3.6455817222595215,
      "learning_rate": 4.14234043722218e-05,
      "loss": 1.6843,
      "step": 128000
    },
    {
      "epoch": 0.5150012714219523,
      "grad_norm": 3.1273069381713867,
      "learning_rate": 4.14167038545398e-05,
      "loss": 1.7042,
      "step": 128100
    },
    {
      "epoch": 0.5154033020788,
      "grad_norm": 3.4026682376861572,
      "learning_rate": 4.1410003336857807e-05,
      "loss": 1.7244,
      "step": 128200
    },
    {
      "epoch": 0.5158053327356478,
      "grad_norm": 3.0916290283203125,
      "learning_rate": 4.140330281917581e-05,
      "loss": 1.7797,
      "step": 128300
    },
    {
      "epoch": 0.5162073633924955,
      "grad_norm": 2.610964059829712,
      "learning_rate": 4.139660230149382e-05,
      "loss": 1.7189,
      "step": 128400
    },
    {
      "epoch": 0.5166093940493433,
      "grad_norm": 2.8522746562957764,
      "learning_rate": 4.138990178381182e-05,
      "loss": 1.7421,
      "step": 128500
    },
    {
      "epoch": 0.517011424706191,
      "grad_norm": 3.3437254428863525,
      "learning_rate": 4.138320126612982e-05,
      "loss": 1.7335,
      "step": 128600
    },
    {
      "epoch": 0.5174134553630387,
      "grad_norm": 2.799677848815918,
      "learning_rate": 4.1376500748447825e-05,
      "loss": 1.7288,
      "step": 128700
    },
    {
      "epoch": 0.5178154860198865,
      "grad_norm": 3.281759262084961,
      "learning_rate": 4.136980023076583e-05,
      "loss": 1.7327,
      "step": 128800
    },
    {
      "epoch": 0.5182175166767342,
      "grad_norm": 3.116884231567383,
      "learning_rate": 4.1363099713083836e-05,
      "loss": 1.7603,
      "step": 128900
    },
    {
      "epoch": 0.518619547333582,
      "grad_norm": 2.9478843212127686,
      "learning_rate": 4.135639919540184e-05,
      "loss": 1.8008,
      "step": 129000
    },
    {
      "epoch": 0.5190215779904297,
      "grad_norm": 3.0093283653259277,
      "learning_rate": 4.1349698677719846e-05,
      "loss": 1.7322,
      "step": 129100
    },
    {
      "epoch": 0.5194236086472774,
      "grad_norm": 3.109283685684204,
      "learning_rate": 4.134299816003785e-05,
      "loss": 1.7228,
      "step": 129200
    },
    {
      "epoch": 0.5198256393041252,
      "grad_norm": 3.325460910797119,
      "learning_rate": 4.133629764235585e-05,
      "loss": 1.7452,
      "step": 129300
    },
    {
      "epoch": 0.5202276699609729,
      "grad_norm": 2.798638105392456,
      "learning_rate": 4.1329597124673855e-05,
      "loss": 1.7462,
      "step": 129400
    },
    {
      "epoch": 0.5206297006178207,
      "grad_norm": 3.0874898433685303,
      "learning_rate": 4.132289660699186e-05,
      "loss": 1.7139,
      "step": 129500
    },
    {
      "epoch": 0.5210317312746684,
      "grad_norm": 3.144412040710449,
      "learning_rate": 4.131619608930986e-05,
      "loss": 1.7763,
      "step": 129600
    },
    {
      "epoch": 0.5214337619315161,
      "grad_norm": 2.7404043674468994,
      "learning_rate": 4.130949557162786e-05,
      "loss": 1.7363,
      "step": 129700
    },
    {
      "epoch": 0.5218357925883639,
      "grad_norm": 3.618819236755371,
      "learning_rate": 4.130279505394587e-05,
      "loss": 1.7889,
      "step": 129800
    },
    {
      "epoch": 0.5222378232452116,
      "grad_norm": 2.9085536003112793,
      "learning_rate": 4.129609453626387e-05,
      "loss": 1.7364,
      "step": 129900
    },
    {
      "epoch": 0.5226398539020592,
      "grad_norm": 2.8752481937408447,
      "learning_rate": 4.128939401858188e-05,
      "loss": 1.7617,
      "step": 130000
    },
    {
      "epoch": 0.523041884558907,
      "grad_norm": 3.3388617038726807,
      "learning_rate": 4.1282693500899884e-05,
      "loss": 1.6928,
      "step": 130100
    },
    {
      "epoch": 0.5234439152157547,
      "grad_norm": 2.87909197807312,
      "learning_rate": 4.127599298321789e-05,
      "loss": 1.7253,
      "step": 130200
    },
    {
      "epoch": 0.5238459458726025,
      "grad_norm": 3.0662055015563965,
      "learning_rate": 4.1269292465535894e-05,
      "loss": 1.7667,
      "step": 130300
    },
    {
      "epoch": 0.5242479765294502,
      "grad_norm": 3.9348843097686768,
      "learning_rate": 4.12625919478539e-05,
      "loss": 1.6495,
      "step": 130400
    },
    {
      "epoch": 0.524650007186298,
      "grad_norm": 3.140972852706909,
      "learning_rate": 4.12558914301719e-05,
      "loss": 1.7703,
      "step": 130500
    },
    {
      "epoch": 0.5250520378431457,
      "grad_norm": 3.455533504486084,
      "learning_rate": 4.1249190912489896e-05,
      "loss": 1.8005,
      "step": 130600
    },
    {
      "epoch": 0.5254540684999934,
      "grad_norm": 3.4325387477874756,
      "learning_rate": 4.12424903948079e-05,
      "loss": 1.7633,
      "step": 130700
    },
    {
      "epoch": 0.5258560991568412,
      "grad_norm": 2.797250747680664,
      "learning_rate": 4.1235789877125906e-05,
      "loss": 1.7017,
      "step": 130800
    },
    {
      "epoch": 0.5262581298136889,
      "grad_norm": 2.735037088394165,
      "learning_rate": 4.122908935944391e-05,
      "loss": 1.7129,
      "step": 130900
    },
    {
      "epoch": 0.5266601604705367,
      "grad_norm": 3.0383148193359375,
      "learning_rate": 4.1222388841761916e-05,
      "loss": 1.7509,
      "step": 131000
    },
    {
      "epoch": 0.5270621911273844,
      "grad_norm": 3.329005479812622,
      "learning_rate": 4.121568832407992e-05,
      "loss": 1.7569,
      "step": 131100
    },
    {
      "epoch": 0.5274642217842321,
      "grad_norm": 3.712111711502075,
      "learning_rate": 4.1208987806397926e-05,
      "loss": 1.761,
      "step": 131200
    },
    {
      "epoch": 0.5278662524410799,
      "grad_norm": 3.308751344680786,
      "learning_rate": 4.120228728871593e-05,
      "loss": 1.7556,
      "step": 131300
    },
    {
      "epoch": 0.5282682830979276,
      "grad_norm": 2.8452491760253906,
      "learning_rate": 4.119558677103394e-05,
      "loss": 1.753,
      "step": 131400
    },
    {
      "epoch": 0.5286703137547754,
      "grad_norm": 2.692922592163086,
      "learning_rate": 4.1188886253351935e-05,
      "loss": 1.7164,
      "step": 131500
    },
    {
      "epoch": 0.5290723444116231,
      "grad_norm": 2.550121307373047,
      "learning_rate": 4.118218573566994e-05,
      "loss": 1.6898,
      "step": 131600
    },
    {
      "epoch": 0.5294743750684708,
      "grad_norm": 2.6789958477020264,
      "learning_rate": 4.117548521798794e-05,
      "loss": 1.693,
      "step": 131700
    },
    {
      "epoch": 0.5298764057253186,
      "grad_norm": 2.7214012145996094,
      "learning_rate": 4.1168784700305944e-05,
      "loss": 1.7432,
      "step": 131800
    },
    {
      "epoch": 0.5302784363821663,
      "grad_norm": 3.8506407737731934,
      "learning_rate": 4.116208418262395e-05,
      "loss": 1.7181,
      "step": 131900
    },
    {
      "epoch": 0.5306804670390141,
      "grad_norm": 3.5909197330474854,
      "learning_rate": 4.1155383664941954e-05,
      "loss": 1.7058,
      "step": 132000
    },
    {
      "epoch": 0.5310824976958618,
      "grad_norm": 2.772094249725342,
      "learning_rate": 4.114868314725996e-05,
      "loss": 1.7463,
      "step": 132100
    },
    {
      "epoch": 0.5314845283527095,
      "grad_norm": 2.884617567062378,
      "learning_rate": 4.1141982629577964e-05,
      "loss": 1.7178,
      "step": 132200
    },
    {
      "epoch": 0.5318865590095573,
      "grad_norm": 2.985788583755493,
      "learning_rate": 4.113528211189597e-05,
      "loss": 1.7202,
      "step": 132300
    },
    {
      "epoch": 0.532288589666405,
      "grad_norm": 3.48930287361145,
      "learning_rate": 4.1128581594213974e-05,
      "loss": 1.736,
      "step": 132400
    },
    {
      "epoch": 0.5326906203232528,
      "grad_norm": 2.851400852203369,
      "learning_rate": 4.112188107653198e-05,
      "loss": 1.7235,
      "step": 132500
    },
    {
      "epoch": 0.5330926509801005,
      "grad_norm": 2.9580297470092773,
      "learning_rate": 4.111518055884998e-05,
      "loss": 1.7431,
      "step": 132600
    },
    {
      "epoch": 0.5334946816369482,
      "grad_norm": 3.085552215576172,
      "learning_rate": 4.110848004116798e-05,
      "loss": 1.7526,
      "step": 132700
    },
    {
      "epoch": 0.533896712293796,
      "grad_norm": 3.325627326965332,
      "learning_rate": 4.110177952348599e-05,
      "loss": 1.7806,
      "step": 132800
    },
    {
      "epoch": 0.5342987429506437,
      "grad_norm": 2.977254867553711,
      "learning_rate": 4.1095079005803986e-05,
      "loss": 1.7227,
      "step": 132900
    },
    {
      "epoch": 0.5347007736074915,
      "grad_norm": 3.322308301925659,
      "learning_rate": 4.108837848812199e-05,
      "loss": 1.7224,
      "step": 133000
    },
    {
      "epoch": 0.5351028042643392,
      "grad_norm": 3.3059797286987305,
      "learning_rate": 4.108167797044e-05,
      "loss": 1.7931,
      "step": 133100
    },
    {
      "epoch": 0.535504834921187,
      "grad_norm": 2.8021156787872314,
      "learning_rate": 4.1074977452758e-05,
      "loss": 1.7618,
      "step": 133200
    },
    {
      "epoch": 0.5359068655780347,
      "grad_norm": 2.9020473957061768,
      "learning_rate": 4.106827693507601e-05,
      "loss": 1.7087,
      "step": 133300
    },
    {
      "epoch": 0.5363088962348824,
      "grad_norm": 2.593155860900879,
      "learning_rate": 4.106157641739401e-05,
      "loss": 1.7155,
      "step": 133400
    },
    {
      "epoch": 0.5367109268917302,
      "grad_norm": 3.5037965774536133,
      "learning_rate": 4.105487589971202e-05,
      "loss": 1.7245,
      "step": 133500
    },
    {
      "epoch": 0.5371129575485779,
      "grad_norm": 2.8465993404388428,
      "learning_rate": 4.1048175382030016e-05,
      "loss": 1.7561,
      "step": 133600
    },
    {
      "epoch": 0.5375149882054256,
      "grad_norm": 2.8947196006774902,
      "learning_rate": 4.104147486434802e-05,
      "loss": 1.7627,
      "step": 133700
    },
    {
      "epoch": 0.5379170188622734,
      "grad_norm": 3.5374906063079834,
      "learning_rate": 4.1034774346666026e-05,
      "loss": 1.7067,
      "step": 133800
    },
    {
      "epoch": 0.5383190495191211,
      "grad_norm": 3.099050998687744,
      "learning_rate": 4.102807382898403e-05,
      "loss": 1.681,
      "step": 133900
    },
    {
      "epoch": 0.5387210801759689,
      "grad_norm": 2.787604570388794,
      "learning_rate": 4.102137331130203e-05,
      "loss": 1.7266,
      "step": 134000
    },
    {
      "epoch": 0.5391231108328166,
      "grad_norm": 3.3708975315093994,
      "learning_rate": 4.1014672793620034e-05,
      "loss": 1.704,
      "step": 134100
    },
    {
      "epoch": 0.5395251414896642,
      "grad_norm": 3.62248158454895,
      "learning_rate": 4.100797227593804e-05,
      "loss": 1.7483,
      "step": 134200
    },
    {
      "epoch": 0.539927172146512,
      "grad_norm": 3.383829116821289,
      "learning_rate": 4.1001271758256045e-05,
      "loss": 1.7447,
      "step": 134300
    },
    {
      "epoch": 0.5403292028033597,
      "grad_norm": 3.5251247882843018,
      "learning_rate": 4.099457124057405e-05,
      "loss": 1.7192,
      "step": 134400
    },
    {
      "epoch": 0.5407312334602075,
      "grad_norm": 3.4765355587005615,
      "learning_rate": 4.0987870722892055e-05,
      "loss": 1.6866,
      "step": 134500
    },
    {
      "epoch": 0.5411332641170552,
      "grad_norm": 3.2996580600738525,
      "learning_rate": 4.098117020521005e-05,
      "loss": 1.7586,
      "step": 134600
    },
    {
      "epoch": 0.5415352947739029,
      "grad_norm": 2.8971495628356934,
      "learning_rate": 4.097446968752806e-05,
      "loss": 1.7099,
      "step": 134700
    },
    {
      "epoch": 0.5419373254307507,
      "grad_norm": 3.1405763626098633,
      "learning_rate": 4.0967769169846064e-05,
      "loss": 1.7042,
      "step": 134800
    },
    {
      "epoch": 0.5423393560875984,
      "grad_norm": 3.03086256980896,
      "learning_rate": 4.096106865216407e-05,
      "loss": 1.7106,
      "step": 134900
    },
    {
      "epoch": 0.5427413867444462,
      "grad_norm": 3.618765115737915,
      "learning_rate": 4.0954368134482074e-05,
      "loss": 1.6858,
      "step": 135000
    },
    {
      "epoch": 0.5431434174012939,
      "grad_norm": 3.207864284515381,
      "learning_rate": 4.094766761680008e-05,
      "loss": 1.7242,
      "step": 135100
    },
    {
      "epoch": 0.5435454480581416,
      "grad_norm": 2.7836148738861084,
      "learning_rate": 4.094096709911808e-05,
      "loss": 1.7207,
      "step": 135200
    },
    {
      "epoch": 0.5439474787149894,
      "grad_norm": 3.1225998401641846,
      "learning_rate": 4.093426658143608e-05,
      "loss": 1.7158,
      "step": 135300
    },
    {
      "epoch": 0.5443495093718371,
      "grad_norm": 3.0789060592651367,
      "learning_rate": 4.092756606375409e-05,
      "loss": 1.7174,
      "step": 135400
    },
    {
      "epoch": 0.5447515400286849,
      "grad_norm": 2.921935796737671,
      "learning_rate": 4.092086554607209e-05,
      "loss": 1.7271,
      "step": 135500
    },
    {
      "epoch": 0.5451535706855326,
      "grad_norm": 2.543922185897827,
      "learning_rate": 4.09141650283901e-05,
      "loss": 1.7494,
      "step": 135600
    },
    {
      "epoch": 0.5455556013423803,
      "grad_norm": 3.0747902393341064,
      "learning_rate": 4.0907464510708096e-05,
      "loss": 1.7435,
      "step": 135700
    },
    {
      "epoch": 0.5459576319992281,
      "grad_norm": 3.118062973022461,
      "learning_rate": 4.09007639930261e-05,
      "loss": 1.756,
      "step": 135800
    },
    {
      "epoch": 0.5463596626560758,
      "grad_norm": 3.5246469974517822,
      "learning_rate": 4.0894063475344106e-05,
      "loss": 1.7275,
      "step": 135900
    },
    {
      "epoch": 0.5467616933129236,
      "grad_norm": 2.9314913749694824,
      "learning_rate": 4.088736295766211e-05,
      "loss": 1.6777,
      "step": 136000
    },
    {
      "epoch": 0.5471637239697713,
      "grad_norm": 3.0796618461608887,
      "learning_rate": 4.088066243998012e-05,
      "loss": 1.7151,
      "step": 136100
    },
    {
      "epoch": 0.547565754626619,
      "grad_norm": 3.3466978073120117,
      "learning_rate": 4.087396192229812e-05,
      "loss": 1.7504,
      "step": 136200
    },
    {
      "epoch": 0.5479677852834668,
      "grad_norm": 3.149118423461914,
      "learning_rate": 4.086726140461613e-05,
      "loss": 1.6913,
      "step": 136300
    },
    {
      "epoch": 0.5483698159403145,
      "grad_norm": 3.3316824436187744,
      "learning_rate": 4.0860560886934125e-05,
      "loss": 1.7576,
      "step": 136400
    },
    {
      "epoch": 0.5487718465971623,
      "grad_norm": 2.963566541671753,
      "learning_rate": 4.085386036925213e-05,
      "loss": 1.7167,
      "step": 136500
    },
    {
      "epoch": 0.54917387725401,
      "grad_norm": 3.4680778980255127,
      "learning_rate": 4.0847159851570135e-05,
      "loss": 1.7557,
      "step": 136600
    },
    {
      "epoch": 0.5495759079108578,
      "grad_norm": 3.417358875274658,
      "learning_rate": 4.0840459333888134e-05,
      "loss": 1.7185,
      "step": 136700
    },
    {
      "epoch": 0.5499779385677055,
      "grad_norm": 3.4807488918304443,
      "learning_rate": 4.083375881620614e-05,
      "loss": 1.7127,
      "step": 136800
    },
    {
      "epoch": 0.5503799692245532,
      "grad_norm": 3.297884464263916,
      "learning_rate": 4.0827058298524144e-05,
      "loss": 1.6637,
      "step": 136900
    },
    {
      "epoch": 0.550781999881401,
      "grad_norm": 3.5963826179504395,
      "learning_rate": 4.082035778084215e-05,
      "loss": 1.6824,
      "step": 137000
    },
    {
      "epoch": 0.5511840305382487,
      "grad_norm": 3.23077654838562,
      "learning_rate": 4.0813657263160154e-05,
      "loss": 1.6753,
      "step": 137100
    },
    {
      "epoch": 0.5515860611950965,
      "grad_norm": 2.9621996879577637,
      "learning_rate": 4.080695674547816e-05,
      "loss": 1.7121,
      "step": 137200
    },
    {
      "epoch": 0.5519880918519442,
      "grad_norm": 3.2729668617248535,
      "learning_rate": 4.0800256227796165e-05,
      "loss": 1.786,
      "step": 137300
    },
    {
      "epoch": 0.5523901225087919,
      "grad_norm": 3.4071717262268066,
      "learning_rate": 4.079355571011417e-05,
      "loss": 1.7543,
      "step": 137400
    },
    {
      "epoch": 0.5527921531656397,
      "grad_norm": 3.2603769302368164,
      "learning_rate": 4.0786855192432175e-05,
      "loss": 1.7485,
      "step": 137500
    },
    {
      "epoch": 0.5531941838224874,
      "grad_norm": 3.5002429485321045,
      "learning_rate": 4.078015467475017e-05,
      "loss": 1.693,
      "step": 137600
    },
    {
      "epoch": 0.5535962144793352,
      "grad_norm": 2.7978954315185547,
      "learning_rate": 4.077345415706818e-05,
      "loss": 1.7366,
      "step": 137700
    },
    {
      "epoch": 0.5539982451361829,
      "grad_norm": 3.1683084964752197,
      "learning_rate": 4.076675363938618e-05,
      "loss": 1.7306,
      "step": 137800
    },
    {
      "epoch": 0.5544002757930306,
      "grad_norm": 3.2646570205688477,
      "learning_rate": 4.076005312170418e-05,
      "loss": 1.6962,
      "step": 137900
    },
    {
      "epoch": 0.5548023064498784,
      "grad_norm": 3.6298117637634277,
      "learning_rate": 4.075335260402219e-05,
      "loss": 1.6861,
      "step": 138000
    },
    {
      "epoch": 0.5552043371067261,
      "grad_norm": 3.002753496170044,
      "learning_rate": 4.074665208634019e-05,
      "loss": 1.6798,
      "step": 138100
    },
    {
      "epoch": 0.5556063677635739,
      "grad_norm": 3.2089383602142334,
      "learning_rate": 4.07399515686582e-05,
      "loss": 1.6969,
      "step": 138200
    },
    {
      "epoch": 0.5560083984204216,
      "grad_norm": 2.9051146507263184,
      "learning_rate": 4.07332510509762e-05,
      "loss": 1.7318,
      "step": 138300
    },
    {
      "epoch": 0.5564104290772693,
      "grad_norm": 3.287947654724121,
      "learning_rate": 4.072655053329421e-05,
      "loss": 1.726,
      "step": 138400
    },
    {
      "epoch": 0.556812459734117,
      "grad_norm": 2.9572439193725586,
      "learning_rate": 4.071985001561221e-05,
      "loss": 1.7633,
      "step": 138500
    },
    {
      "epoch": 0.5572144903909647,
      "grad_norm": 3.291290283203125,
      "learning_rate": 4.071314949793021e-05,
      "loss": 1.6949,
      "step": 138600
    },
    {
      "epoch": 0.5576165210478125,
      "grad_norm": 3.1377551555633545,
      "learning_rate": 4.0706448980248216e-05,
      "loss": 1.7073,
      "step": 138700
    },
    {
      "epoch": 0.5580185517046602,
      "grad_norm": 3.3963890075683594,
      "learning_rate": 4.0699748462566214e-05,
      "loss": 1.7322,
      "step": 138800
    },
    {
      "epoch": 0.5584205823615079,
      "grad_norm": 3.015850067138672,
      "learning_rate": 4.069304794488422e-05,
      "loss": 1.7415,
      "step": 138900
    },
    {
      "epoch": 0.5588226130183557,
      "grad_norm": 3.359441041946411,
      "learning_rate": 4.0686347427202225e-05,
      "loss": 1.7345,
      "step": 139000
    },
    {
      "epoch": 0.5592246436752034,
      "grad_norm": 3.5141804218292236,
      "learning_rate": 4.067964690952023e-05,
      "loss": 1.7595,
      "step": 139100
    },
    {
      "epoch": 0.5596266743320512,
      "grad_norm": 3.5060269832611084,
      "learning_rate": 4.0672946391838235e-05,
      "loss": 1.7472,
      "step": 139200
    },
    {
      "epoch": 0.5600287049888989,
      "grad_norm": 2.991726875305176,
      "learning_rate": 4.066624587415624e-05,
      "loss": 1.7266,
      "step": 139300
    },
    {
      "epoch": 0.5604307356457466,
      "grad_norm": 2.984241008758545,
      "learning_rate": 4.0659545356474245e-05,
      "loss": 1.7163,
      "step": 139400
    },
    {
      "epoch": 0.5608327663025944,
      "grad_norm": 3.265169143676758,
      "learning_rate": 4.065284483879225e-05,
      "loss": 1.6948,
      "step": 139500
    },
    {
      "epoch": 0.5612347969594421,
      "grad_norm": 3.001819610595703,
      "learning_rate": 4.0646144321110255e-05,
      "loss": 1.745,
      "step": 139600
    },
    {
      "epoch": 0.5616368276162899,
      "grad_norm": 3.6347503662109375,
      "learning_rate": 4.0639443803428254e-05,
      "loss": 1.6757,
      "step": 139700
    },
    {
      "epoch": 0.5620388582731376,
      "grad_norm": 3.08465576171875,
      "learning_rate": 4.063274328574626e-05,
      "loss": 1.6602,
      "step": 139800
    },
    {
      "epoch": 0.5624408889299853,
      "grad_norm": 3.3497540950775146,
      "learning_rate": 4.0626042768064264e-05,
      "loss": 1.7596,
      "step": 139900
    },
    {
      "epoch": 0.5628429195868331,
      "grad_norm": 3.399250030517578,
      "learning_rate": 4.061934225038226e-05,
      "loss": 1.7042,
      "step": 140000
    },
    {
      "epoch": 0.5632449502436808,
      "grad_norm": 3.189434766769409,
      "learning_rate": 4.061264173270027e-05,
      "loss": 1.6928,
      "step": 140100
    },
    {
      "epoch": 0.5636469809005286,
      "grad_norm": 3.130190372467041,
      "learning_rate": 4.060594121501827e-05,
      "loss": 1.7033,
      "step": 140200
    },
    {
      "epoch": 0.5640490115573763,
      "grad_norm": 3.473907470703125,
      "learning_rate": 4.059924069733628e-05,
      "loss": 1.7348,
      "step": 140300
    },
    {
      "epoch": 0.564451042214224,
      "grad_norm": 4.205036163330078,
      "learning_rate": 4.059254017965428e-05,
      "loss": 1.7177,
      "step": 140400
    },
    {
      "epoch": 0.5648530728710718,
      "grad_norm": 2.763483762741089,
      "learning_rate": 4.058583966197229e-05,
      "loss": 1.7138,
      "step": 140500
    },
    {
      "epoch": 0.5652551035279195,
      "grad_norm": 2.8259541988372803,
      "learning_rate": 4.057913914429029e-05,
      "loss": 1.7271,
      "step": 140600
    },
    {
      "epoch": 0.5656571341847673,
      "grad_norm": 3.0493195056915283,
      "learning_rate": 4.057243862660829e-05,
      "loss": 1.7361,
      "step": 140700
    },
    {
      "epoch": 0.566059164841615,
      "grad_norm": 3.1916987895965576,
      "learning_rate": 4.0565738108926297e-05,
      "loss": 1.7155,
      "step": 140800
    },
    {
      "epoch": 0.5664611954984627,
      "grad_norm": 3.6432032585144043,
      "learning_rate": 4.05590375912443e-05,
      "loss": 1.7108,
      "step": 140900
    },
    {
      "epoch": 0.5668632261553105,
      "grad_norm": 3.0389564037323,
      "learning_rate": 4.055233707356231e-05,
      "loss": 1.7434,
      "step": 141000
    },
    {
      "epoch": 0.5672652568121582,
      "grad_norm": 3.202347993850708,
      "learning_rate": 4.054563655588031e-05,
      "loss": 1.6891,
      "step": 141100
    },
    {
      "epoch": 0.567667287469006,
      "grad_norm": 3.409252643585205,
      "learning_rate": 4.053893603819831e-05,
      "loss": 1.7162,
      "step": 141200
    },
    {
      "epoch": 0.5680693181258537,
      "grad_norm": 3.5441837310791016,
      "learning_rate": 4.0532235520516315e-05,
      "loss": 1.6909,
      "step": 141300
    },
    {
      "epoch": 0.5684713487827014,
      "grad_norm": 3.1092917919158936,
      "learning_rate": 4.052553500283432e-05,
      "loss": 1.7371,
      "step": 141400
    },
    {
      "epoch": 0.5688733794395492,
      "grad_norm": 2.845907211303711,
      "learning_rate": 4.0518834485152326e-05,
      "loss": 1.704,
      "step": 141500
    },
    {
      "epoch": 0.5692754100963969,
      "grad_norm": 3.206308603286743,
      "learning_rate": 4.051213396747033e-05,
      "loss": 1.7036,
      "step": 141600
    },
    {
      "epoch": 0.5696774407532447,
      "grad_norm": 3.2594399452209473,
      "learning_rate": 4.050543344978833e-05,
      "loss": 1.7497,
      "step": 141700
    },
    {
      "epoch": 0.5700794714100924,
      "grad_norm": 3.2758615016937256,
      "learning_rate": 4.0498732932106334e-05,
      "loss": 1.73,
      "step": 141800
    },
    {
      "epoch": 0.5704815020669401,
      "grad_norm": 3.1408259868621826,
      "learning_rate": 4.049203241442434e-05,
      "loss": 1.6707,
      "step": 141900
    },
    {
      "epoch": 0.5708835327237879,
      "grad_norm": 2.9343018531799316,
      "learning_rate": 4.0485331896742345e-05,
      "loss": 1.7289,
      "step": 142000
    },
    {
      "epoch": 0.5712855633806356,
      "grad_norm": 4.036001205444336,
      "learning_rate": 4.047863137906035e-05,
      "loss": 1.7184,
      "step": 142100
    },
    {
      "epoch": 0.5716875940374834,
      "grad_norm": 3.4829373359680176,
      "learning_rate": 4.0471930861378355e-05,
      "loss": 1.7259,
      "step": 142200
    },
    {
      "epoch": 0.5720896246943311,
      "grad_norm": 3.2417101860046387,
      "learning_rate": 4.046523034369636e-05,
      "loss": 1.734,
      "step": 142300
    },
    {
      "epoch": 0.5724916553511789,
      "grad_norm": 3.437957525253296,
      "learning_rate": 4.045852982601436e-05,
      "loss": 1.7188,
      "step": 142400
    },
    {
      "epoch": 0.5728936860080266,
      "grad_norm": 3.717068672180176,
      "learning_rate": 4.0451829308332363e-05,
      "loss": 1.7022,
      "step": 142500
    },
    {
      "epoch": 0.5732957166648743,
      "grad_norm": 2.956130266189575,
      "learning_rate": 4.044512879065037e-05,
      "loss": 1.748,
      "step": 142600
    },
    {
      "epoch": 0.573697747321722,
      "grad_norm": 3.287761688232422,
      "learning_rate": 4.0438428272968374e-05,
      "loss": 1.7811,
      "step": 142700
    },
    {
      "epoch": 0.5740997779785697,
      "grad_norm": 3.397853136062622,
      "learning_rate": 4.043172775528637e-05,
      "loss": 1.7376,
      "step": 142800
    },
    {
      "epoch": 0.5745018086354174,
      "grad_norm": 3.4967215061187744,
      "learning_rate": 4.042502723760438e-05,
      "loss": 1.7444,
      "step": 142900
    },
    {
      "epoch": 0.5749038392922652,
      "grad_norm": 3.2167367935180664,
      "learning_rate": 4.041832671992238e-05,
      "loss": 1.6922,
      "step": 143000
    },
    {
      "epoch": 0.5753058699491129,
      "grad_norm": 3.2878615856170654,
      "learning_rate": 4.041162620224039e-05,
      "loss": 1.6694,
      "step": 143100
    },
    {
      "epoch": 0.5757079006059607,
      "grad_norm": 3.1196937561035156,
      "learning_rate": 4.040492568455839e-05,
      "loss": 1.7082,
      "step": 143200
    },
    {
      "epoch": 0.5761099312628084,
      "grad_norm": 2.965543031692505,
      "learning_rate": 4.03982251668764e-05,
      "loss": 1.7339,
      "step": 143300
    },
    {
      "epoch": 0.5765119619196561,
      "grad_norm": 3.2887625694274902,
      "learning_rate": 4.03915246491944e-05,
      "loss": 1.7395,
      "step": 143400
    },
    {
      "epoch": 0.5769139925765039,
      "grad_norm": 3.2855563163757324,
      "learning_rate": 4.03848241315124e-05,
      "loss": 1.7398,
      "step": 143500
    },
    {
      "epoch": 0.5773160232333516,
      "grad_norm": 3.2612125873565674,
      "learning_rate": 4.0378123613830406e-05,
      "loss": 1.6983,
      "step": 143600
    },
    {
      "epoch": 0.5777180538901994,
      "grad_norm": 3.2762272357940674,
      "learning_rate": 4.037142309614841e-05,
      "loss": 1.7091,
      "step": 143700
    },
    {
      "epoch": 0.5781200845470471,
      "grad_norm": 3.298269510269165,
      "learning_rate": 4.036472257846641e-05,
      "loss": 1.7591,
      "step": 143800
    },
    {
      "epoch": 0.5785221152038948,
      "grad_norm": 3.1206209659576416,
      "learning_rate": 4.0358022060784415e-05,
      "loss": 1.7542,
      "step": 143900
    },
    {
      "epoch": 0.5789241458607426,
      "grad_norm": 3.2207584381103516,
      "learning_rate": 4.035132154310242e-05,
      "loss": 1.6849,
      "step": 144000
    },
    {
      "epoch": 0.5793261765175903,
      "grad_norm": 2.9364049434661865,
      "learning_rate": 4.0344621025420425e-05,
      "loss": 1.7191,
      "step": 144100
    },
    {
      "epoch": 0.5797282071744381,
      "grad_norm": 3.274076461791992,
      "learning_rate": 4.033792050773843e-05,
      "loss": 1.7244,
      "step": 144200
    },
    {
      "epoch": 0.5801302378312858,
      "grad_norm": 3.300076723098755,
      "learning_rate": 4.0331219990056435e-05,
      "loss": 1.7254,
      "step": 144300
    },
    {
      "epoch": 0.5805322684881336,
      "grad_norm": 3.222949504852295,
      "learning_rate": 4.032451947237444e-05,
      "loss": 1.7232,
      "step": 144400
    },
    {
      "epoch": 0.5809342991449813,
      "grad_norm": 2.8699727058410645,
      "learning_rate": 4.0317818954692446e-05,
      "loss": 1.705,
      "step": 144500
    },
    {
      "epoch": 0.581336329801829,
      "grad_norm": 3.1653659343719482,
      "learning_rate": 4.031111843701045e-05,
      "loss": 1.7013,
      "step": 144600
    },
    {
      "epoch": 0.5817383604586768,
      "grad_norm": 2.897172689437866,
      "learning_rate": 4.030441791932845e-05,
      "loss": 1.7019,
      "step": 144700
    },
    {
      "epoch": 0.5821403911155245,
      "grad_norm": 3.4277822971343994,
      "learning_rate": 4.0297717401646454e-05,
      "loss": 1.7,
      "step": 144800
    },
    {
      "epoch": 0.5825424217723723,
      "grad_norm": 3.125152349472046,
      "learning_rate": 4.029101688396445e-05,
      "loss": 1.744,
      "step": 144900
    },
    {
      "epoch": 0.58294445242922,
      "grad_norm": 2.9982051849365234,
      "learning_rate": 4.028431636628246e-05,
      "loss": 1.7234,
      "step": 145000
    },
    {
      "epoch": 0.5833464830860677,
      "grad_norm": 2.692754030227661,
      "learning_rate": 4.027761584860046e-05,
      "loss": 1.716,
      "step": 145100
    },
    {
      "epoch": 0.5837485137429155,
      "grad_norm": 3.1762635707855225,
      "learning_rate": 4.027091533091847e-05,
      "loss": 1.7127,
      "step": 145200
    },
    {
      "epoch": 0.5841505443997632,
      "grad_norm": 3.811171293258667,
      "learning_rate": 4.026421481323647e-05,
      "loss": 1.7103,
      "step": 145300
    },
    {
      "epoch": 0.584552575056611,
      "grad_norm": 2.975484609603882,
      "learning_rate": 4.025751429555448e-05,
      "loss": 1.7288,
      "step": 145400
    },
    {
      "epoch": 0.5849546057134587,
      "grad_norm": 2.7754266262054443,
      "learning_rate": 4.025081377787248e-05,
      "loss": 1.6925,
      "step": 145500
    },
    {
      "epoch": 0.5853566363703064,
      "grad_norm": 2.485738515853882,
      "learning_rate": 4.024411326019049e-05,
      "loss": 1.7309,
      "step": 145600
    },
    {
      "epoch": 0.5857586670271542,
      "grad_norm": 3.1205384731292725,
      "learning_rate": 4.023741274250849e-05,
      "loss": 1.719,
      "step": 145700
    },
    {
      "epoch": 0.5861606976840019,
      "grad_norm": 3.4412941932678223,
      "learning_rate": 4.023071222482649e-05,
      "loss": 1.6804,
      "step": 145800
    },
    {
      "epoch": 0.5865627283408497,
      "grad_norm": 3.4767606258392334,
      "learning_rate": 4.022401170714449e-05,
      "loss": 1.6631,
      "step": 145900
    },
    {
      "epoch": 0.5869647589976974,
      "grad_norm": 3.285339117050171,
      "learning_rate": 4.0217311189462495e-05,
      "loss": 1.7135,
      "step": 146000
    },
    {
      "epoch": 0.5873667896545451,
      "grad_norm": 3.4893126487731934,
      "learning_rate": 4.02106106717805e-05,
      "loss": 1.7191,
      "step": 146100
    },
    {
      "epoch": 0.5877688203113929,
      "grad_norm": 3.3511979579925537,
      "learning_rate": 4.0203910154098506e-05,
      "loss": 1.7199,
      "step": 146200
    },
    {
      "epoch": 0.5881708509682406,
      "grad_norm": 3.121049404144287,
      "learning_rate": 4.019720963641651e-05,
      "loss": 1.6795,
      "step": 146300
    },
    {
      "epoch": 0.5885728816250884,
      "grad_norm": 2.85467267036438,
      "learning_rate": 4.0190509118734516e-05,
      "loss": 1.7064,
      "step": 146400
    },
    {
      "epoch": 0.5889749122819361,
      "grad_norm": 2.966094970703125,
      "learning_rate": 4.018380860105252e-05,
      "loss": 1.7186,
      "step": 146500
    },
    {
      "epoch": 0.5893769429387838,
      "grad_norm": 3.1782689094543457,
      "learning_rate": 4.0177108083370526e-05,
      "loss": 1.7242,
      "step": 146600
    },
    {
      "epoch": 0.5897789735956316,
      "grad_norm": 3.3414437770843506,
      "learning_rate": 4.017040756568853e-05,
      "loss": 1.7384,
      "step": 146700
    },
    {
      "epoch": 0.5901810042524793,
      "grad_norm": 3.0599777698516846,
      "learning_rate": 4.016370704800653e-05,
      "loss": 1.6764,
      "step": 146800
    },
    {
      "epoch": 0.5905830349093271,
      "grad_norm": 3.5880444049835205,
      "learning_rate": 4.0157006530324535e-05,
      "loss": 1.7693,
      "step": 146900
    },
    {
      "epoch": 0.5909850655661747,
      "grad_norm": 2.822026014328003,
      "learning_rate": 4.015030601264254e-05,
      "loss": 1.7401,
      "step": 147000
    },
    {
      "epoch": 0.5913870962230224,
      "grad_norm": 2.887180805206299,
      "learning_rate": 4.014360549496054e-05,
      "loss": 1.7026,
      "step": 147100
    },
    {
      "epoch": 0.5917891268798702,
      "grad_norm": 3.099565029144287,
      "learning_rate": 4.013690497727854e-05,
      "loss": 1.7066,
      "step": 147200
    },
    {
      "epoch": 0.5921911575367179,
      "grad_norm": 3.039043664932251,
      "learning_rate": 4.013020445959655e-05,
      "loss": 1.7229,
      "step": 147300
    },
    {
      "epoch": 0.5925931881935657,
      "grad_norm": 2.8971121311187744,
      "learning_rate": 4.0123503941914554e-05,
      "loss": 1.6906,
      "step": 147400
    },
    {
      "epoch": 0.5929952188504134,
      "grad_norm": 3.0524322986602783,
      "learning_rate": 4.011680342423256e-05,
      "loss": 1.7128,
      "step": 147500
    },
    {
      "epoch": 0.5933972495072611,
      "grad_norm": 2.60146164894104,
      "learning_rate": 4.0110102906550564e-05,
      "loss": 1.6811,
      "step": 147600
    },
    {
      "epoch": 0.5937992801641089,
      "grad_norm": 3.8046231269836426,
      "learning_rate": 4.010340238886857e-05,
      "loss": 1.7165,
      "step": 147700
    },
    {
      "epoch": 0.5942013108209566,
      "grad_norm": 3.064314126968384,
      "learning_rate": 4.009670187118657e-05,
      "loss": 1.702,
      "step": 147800
    },
    {
      "epoch": 0.5946033414778044,
      "grad_norm": 2.5155930519104004,
      "learning_rate": 4.009000135350457e-05,
      "loss": 1.6865,
      "step": 147900
    },
    {
      "epoch": 0.5950053721346521,
      "grad_norm": 3.202018976211548,
      "learning_rate": 4.008330083582258e-05,
      "loss": 1.7462,
      "step": 148000
    },
    {
      "epoch": 0.5954074027914998,
      "grad_norm": 3.3451449871063232,
      "learning_rate": 4.007660031814058e-05,
      "loss": 1.7323,
      "step": 148100
    },
    {
      "epoch": 0.5958094334483476,
      "grad_norm": 2.7148501873016357,
      "learning_rate": 4.006989980045859e-05,
      "loss": 1.705,
      "step": 148200
    },
    {
      "epoch": 0.5962114641051953,
      "grad_norm": 3.0041911602020264,
      "learning_rate": 4.0063199282776586e-05,
      "loss": 1.7011,
      "step": 148300
    },
    {
      "epoch": 0.5966134947620431,
      "grad_norm": 2.880805015563965,
      "learning_rate": 4.005649876509459e-05,
      "loss": 1.6679,
      "step": 148400
    },
    {
      "epoch": 0.5970155254188908,
      "grad_norm": 3.147616147994995,
      "learning_rate": 4.0049798247412596e-05,
      "loss": 1.6921,
      "step": 148500
    },
    {
      "epoch": 0.5974175560757385,
      "grad_norm": 3.122706413269043,
      "learning_rate": 4.00430977297306e-05,
      "loss": 1.712,
      "step": 148600
    },
    {
      "epoch": 0.5978195867325863,
      "grad_norm": 2.69006085395813,
      "learning_rate": 4.003639721204861e-05,
      "loss": 1.7199,
      "step": 148700
    },
    {
      "epoch": 0.598221617389434,
      "grad_norm": 2.5228493213653564,
      "learning_rate": 4.002969669436661e-05,
      "loss": 1.6597,
      "step": 148800
    },
    {
      "epoch": 0.5986236480462818,
      "grad_norm": 3.045949935913086,
      "learning_rate": 4.002299617668461e-05,
      "loss": 1.7499,
      "step": 148900
    },
    {
      "epoch": 0.5990256787031295,
      "grad_norm": 2.899042844772339,
      "learning_rate": 4.0016295659002615e-05,
      "loss": 1.7601,
      "step": 149000
    },
    {
      "epoch": 0.5994277093599772,
      "grad_norm": 2.9794511795043945,
      "learning_rate": 4.000959514132062e-05,
      "loss": 1.6797,
      "step": 149100
    },
    {
      "epoch": 0.599829740016825,
      "grad_norm": 3.2819535732269287,
      "learning_rate": 4.0002894623638626e-05,
      "loss": 1.6972,
      "step": 149200
    },
    {
      "epoch": 0.6002317706736727,
      "grad_norm": 3.4321160316467285,
      "learning_rate": 3.999619410595663e-05,
      "loss": 1.7048,
      "step": 149300
    },
    {
      "epoch": 0.6006338013305205,
      "grad_norm": 3.4307069778442383,
      "learning_rate": 3.9989493588274636e-05,
      "loss": 1.6937,
      "step": 149400
    },
    {
      "epoch": 0.6010358319873682,
      "grad_norm": 2.770198106765747,
      "learning_rate": 3.9982793070592634e-05,
      "loss": 1.717,
      "step": 149500
    },
    {
      "epoch": 0.601437862644216,
      "grad_norm": 3.503105878829956,
      "learning_rate": 3.997609255291064e-05,
      "loss": 1.6722,
      "step": 149600
    },
    {
      "epoch": 0.6018398933010637,
      "grad_norm": 3.946051836013794,
      "learning_rate": 3.9969392035228644e-05,
      "loss": 1.7255,
      "step": 149700
    },
    {
      "epoch": 0.6022419239579114,
      "grad_norm": 3.3563554286956787,
      "learning_rate": 3.996269151754665e-05,
      "loss": 1.6969,
      "step": 149800
    },
    {
      "epoch": 0.6026439546147592,
      "grad_norm": 3.0492827892303467,
      "learning_rate": 3.995599099986465e-05,
      "loss": 1.7224,
      "step": 149900
    },
    {
      "epoch": 0.6030459852716069,
      "grad_norm": 3.6604487895965576,
      "learning_rate": 3.994929048218265e-05,
      "loss": 1.7315,
      "step": 150000
    },
    {
      "epoch": 0.6034480159284547,
      "grad_norm": 2.527961015701294,
      "learning_rate": 3.994258996450066e-05,
      "loss": 1.6804,
      "step": 150100
    },
    {
      "epoch": 0.6038500465853024,
      "grad_norm": 3.6791751384735107,
      "learning_rate": 3.993588944681866e-05,
      "loss": 1.6817,
      "step": 150200
    },
    {
      "epoch": 0.6042520772421501,
      "grad_norm": 3.289559841156006,
      "learning_rate": 3.992918892913667e-05,
      "loss": 1.6966,
      "step": 150300
    },
    {
      "epoch": 0.6046541078989979,
      "grad_norm": 4.014800548553467,
      "learning_rate": 3.9922488411454674e-05,
      "loss": 1.6699,
      "step": 150400
    },
    {
      "epoch": 0.6050561385558456,
      "grad_norm": 3.3002686500549316,
      "learning_rate": 3.991578789377268e-05,
      "loss": 1.742,
      "step": 150500
    },
    {
      "epoch": 0.6054581692126934,
      "grad_norm": 3.469834327697754,
      "learning_rate": 3.9909087376090684e-05,
      "loss": 1.7405,
      "step": 150600
    },
    {
      "epoch": 0.6058601998695411,
      "grad_norm": 2.9146552085876465,
      "learning_rate": 3.990238685840868e-05,
      "loss": 1.7229,
      "step": 150700
    },
    {
      "epoch": 0.6062622305263888,
      "grad_norm": 3.903472900390625,
      "learning_rate": 3.989568634072669e-05,
      "loss": 1.7261,
      "step": 150800
    },
    {
      "epoch": 0.6066642611832366,
      "grad_norm": 3.157172679901123,
      "learning_rate": 3.9888985823044686e-05,
      "loss": 1.6919,
      "step": 150900
    },
    {
      "epoch": 0.6070662918400843,
      "grad_norm": 2.9421048164367676,
      "learning_rate": 3.988228530536269e-05,
      "loss": 1.7014,
      "step": 151000
    },
    {
      "epoch": 0.6074683224969321,
      "grad_norm": 3.1713130474090576,
      "learning_rate": 3.9875584787680696e-05,
      "loss": 1.68,
      "step": 151100
    },
    {
      "epoch": 0.6078703531537798,
      "grad_norm": 3.348330497741699,
      "learning_rate": 3.98688842699987e-05,
      "loss": 1.7331,
      "step": 151200
    },
    {
      "epoch": 0.6082723838106274,
      "grad_norm": 2.3959763050079346,
      "learning_rate": 3.9862183752316706e-05,
      "loss": 1.7271,
      "step": 151300
    },
    {
      "epoch": 0.6086744144674752,
      "grad_norm": 3.241459608078003,
      "learning_rate": 3.985548323463471e-05,
      "loss": 1.6725,
      "step": 151400
    },
    {
      "epoch": 0.6090764451243229,
      "grad_norm": 3.8647043704986572,
      "learning_rate": 3.9848782716952716e-05,
      "loss": 1.7376,
      "step": 151500
    },
    {
      "epoch": 0.6094784757811706,
      "grad_norm": 3.497217893600464,
      "learning_rate": 3.984208219927072e-05,
      "loss": 1.6905,
      "step": 151600
    },
    {
      "epoch": 0.6098805064380184,
      "grad_norm": 2.628258228302002,
      "learning_rate": 3.9835381681588727e-05,
      "loss": 1.7057,
      "step": 151700
    },
    {
      "epoch": 0.6102825370948661,
      "grad_norm": 3.161156415939331,
      "learning_rate": 3.9828681163906725e-05,
      "loss": 1.7072,
      "step": 151800
    },
    {
      "epoch": 0.6106845677517139,
      "grad_norm": 3.4555625915527344,
      "learning_rate": 3.982198064622473e-05,
      "loss": 1.7266,
      "step": 151900
    },
    {
      "epoch": 0.6110865984085616,
      "grad_norm": 3.942324161529541,
      "learning_rate": 3.981528012854273e-05,
      "loss": 1.7215,
      "step": 152000
    },
    {
      "epoch": 0.6114886290654094,
      "grad_norm": 3.997232437133789,
      "learning_rate": 3.9808579610860734e-05,
      "loss": 1.6788,
      "step": 152100
    },
    {
      "epoch": 0.6118906597222571,
      "grad_norm": 3.197831630706787,
      "learning_rate": 3.980187909317874e-05,
      "loss": 1.6812,
      "step": 152200
    },
    {
      "epoch": 0.6122926903791048,
      "grad_norm": 2.7713401317596436,
      "learning_rate": 3.9795178575496744e-05,
      "loss": 1.7029,
      "step": 152300
    },
    {
      "epoch": 0.6126947210359526,
      "grad_norm": 3.088886260986328,
      "learning_rate": 3.978847805781475e-05,
      "loss": 1.6529,
      "step": 152400
    },
    {
      "epoch": 0.6130967516928003,
      "grad_norm": 3.345311164855957,
      "learning_rate": 3.9781777540132754e-05,
      "loss": 1.7144,
      "step": 152500
    },
    {
      "epoch": 0.613498782349648,
      "grad_norm": 3.3780288696289062,
      "learning_rate": 3.977507702245076e-05,
      "loss": 1.6967,
      "step": 152600
    },
    {
      "epoch": 0.6139008130064958,
      "grad_norm": 3.0371904373168945,
      "learning_rate": 3.9768376504768764e-05,
      "loss": 1.6942,
      "step": 152700
    },
    {
      "epoch": 0.6143028436633435,
      "grad_norm": 2.9390134811401367,
      "learning_rate": 3.976167598708677e-05,
      "loss": 1.6856,
      "step": 152800
    },
    {
      "epoch": 0.6147048743201913,
      "grad_norm": 3.30649471282959,
      "learning_rate": 3.975497546940477e-05,
      "loss": 1.7334,
      "step": 152900
    },
    {
      "epoch": 0.615106904977039,
      "grad_norm": 3.392474412918091,
      "learning_rate": 3.9748274951722766e-05,
      "loss": 1.742,
      "step": 153000
    },
    {
      "epoch": 0.6155089356338868,
      "grad_norm": 2.780789852142334,
      "learning_rate": 3.974157443404077e-05,
      "loss": 1.7143,
      "step": 153100
    },
    {
      "epoch": 0.6159109662907345,
      "grad_norm": 3.745422601699829,
      "learning_rate": 3.9734873916358776e-05,
      "loss": 1.6948,
      "step": 153200
    },
    {
      "epoch": 0.6163129969475822,
      "grad_norm": 3.1640465259552,
      "learning_rate": 3.972817339867678e-05,
      "loss": 1.6795,
      "step": 153300
    },
    {
      "epoch": 0.61671502760443,
      "grad_norm": 2.734034776687622,
      "learning_rate": 3.972147288099479e-05,
      "loss": 1.652,
      "step": 153400
    },
    {
      "epoch": 0.6171170582612777,
      "grad_norm": 3.1511876583099365,
      "learning_rate": 3.971477236331279e-05,
      "loss": 1.7404,
      "step": 153500
    },
    {
      "epoch": 0.6175190889181255,
      "grad_norm": 3.320919990539551,
      "learning_rate": 3.97080718456308e-05,
      "loss": 1.7335,
      "step": 153600
    },
    {
      "epoch": 0.6179211195749732,
      "grad_norm": 3.1738150119781494,
      "learning_rate": 3.97013713279488e-05,
      "loss": 1.6949,
      "step": 153700
    },
    {
      "epoch": 0.6183231502318209,
      "grad_norm": 3.411576271057129,
      "learning_rate": 3.969467081026681e-05,
      "loss": 1.6702,
      "step": 153800
    },
    {
      "epoch": 0.6187251808886687,
      "grad_norm": 3.9043495655059814,
      "learning_rate": 3.9687970292584805e-05,
      "loss": 1.7279,
      "step": 153900
    },
    {
      "epoch": 0.6191272115455164,
      "grad_norm": 3.1198410987854004,
      "learning_rate": 3.968126977490281e-05,
      "loss": 1.6973,
      "step": 154000
    },
    {
      "epoch": 0.6195292422023642,
      "grad_norm": 3.7064626216888428,
      "learning_rate": 3.9674569257220816e-05,
      "loss": 1.7196,
      "step": 154100
    },
    {
      "epoch": 0.6199312728592119,
      "grad_norm": 3.0924267768859863,
      "learning_rate": 3.9667868739538814e-05,
      "loss": 1.7568,
      "step": 154200
    },
    {
      "epoch": 0.6203333035160596,
      "grad_norm": 3.1148691177368164,
      "learning_rate": 3.966116822185682e-05,
      "loss": 1.6862,
      "step": 154300
    },
    {
      "epoch": 0.6207353341729074,
      "grad_norm": 3.241666555404663,
      "learning_rate": 3.9654467704174824e-05,
      "loss": 1.7118,
      "step": 154400
    },
    {
      "epoch": 0.6211373648297551,
      "grad_norm": 3.0525615215301514,
      "learning_rate": 3.964776718649283e-05,
      "loss": 1.726,
      "step": 154500
    },
    {
      "epoch": 0.6215393954866029,
      "grad_norm": 3.49513578414917,
      "learning_rate": 3.9641066668810835e-05,
      "loss": 1.6755,
      "step": 154600
    },
    {
      "epoch": 0.6219414261434506,
      "grad_norm": 3.527881145477295,
      "learning_rate": 3.963436615112884e-05,
      "loss": 1.7055,
      "step": 154700
    },
    {
      "epoch": 0.6223434568002983,
      "grad_norm": 3.2793989181518555,
      "learning_rate": 3.9627665633446845e-05,
      "loss": 1.7146,
      "step": 154800
    },
    {
      "epoch": 0.6227454874571461,
      "grad_norm": 2.838491439819336,
      "learning_rate": 3.962096511576484e-05,
      "loss": 1.6913,
      "step": 154900
    },
    {
      "epoch": 0.6231475181139938,
      "grad_norm": 2.7755346298217773,
      "learning_rate": 3.961426459808285e-05,
      "loss": 1.7095,
      "step": 155000
    },
    {
      "epoch": 0.6235495487708416,
      "grad_norm": 2.963224172592163,
      "learning_rate": 3.9607564080400853e-05,
      "loss": 1.7128,
      "step": 155100
    },
    {
      "epoch": 0.6239515794276893,
      "grad_norm": 3.4148001670837402,
      "learning_rate": 3.960086356271886e-05,
      "loss": 1.6822,
      "step": 155200
    },
    {
      "epoch": 0.624353610084537,
      "grad_norm": 3.949843406677246,
      "learning_rate": 3.9594163045036864e-05,
      "loss": 1.7,
      "step": 155300
    },
    {
      "epoch": 0.6247556407413848,
      "grad_norm": 2.972722291946411,
      "learning_rate": 3.958746252735486e-05,
      "loss": 1.7193,
      "step": 155400
    },
    {
      "epoch": 0.6251576713982324,
      "grad_norm": 3.2000386714935303,
      "learning_rate": 3.958076200967287e-05,
      "loss": 1.6846,
      "step": 155500
    },
    {
      "epoch": 0.6255597020550802,
      "grad_norm": 3.6364946365356445,
      "learning_rate": 3.957406149199087e-05,
      "loss": 1.7318,
      "step": 155600
    },
    {
      "epoch": 0.6259617327119279,
      "grad_norm": 2.9799652099609375,
      "learning_rate": 3.956736097430888e-05,
      "loss": 1.6787,
      "step": 155700
    },
    {
      "epoch": 0.6263637633687756,
      "grad_norm": 3.3761003017425537,
      "learning_rate": 3.956066045662688e-05,
      "loss": 1.6998,
      "step": 155800
    },
    {
      "epoch": 0.6267657940256234,
      "grad_norm": 3.666652202606201,
      "learning_rate": 3.955395993894489e-05,
      "loss": 1.7079,
      "step": 155900
    },
    {
      "epoch": 0.6271678246824711,
      "grad_norm": 3.4563820362091064,
      "learning_rate": 3.9547259421262886e-05,
      "loss": 1.689,
      "step": 156000
    },
    {
      "epoch": 0.6275698553393189,
      "grad_norm": 2.8941962718963623,
      "learning_rate": 3.954055890358089e-05,
      "loss": 1.6686,
      "step": 156100
    },
    {
      "epoch": 0.6279718859961666,
      "grad_norm": 3.6684484481811523,
      "learning_rate": 3.9533858385898896e-05,
      "loss": 1.6853,
      "step": 156200
    },
    {
      "epoch": 0.6283739166530143,
      "grad_norm": 3.271775007247925,
      "learning_rate": 3.95271578682169e-05,
      "loss": 1.6879,
      "step": 156300
    },
    {
      "epoch": 0.6287759473098621,
      "grad_norm": 2.6003921031951904,
      "learning_rate": 3.9520457350534907e-05,
      "loss": 1.7102,
      "step": 156400
    },
    {
      "epoch": 0.6291779779667098,
      "grad_norm": 2.9336130619049072,
      "learning_rate": 3.951375683285291e-05,
      "loss": 1.7077,
      "step": 156500
    },
    {
      "epoch": 0.6295800086235576,
      "grad_norm": 3.27453351020813,
      "learning_rate": 3.950705631517091e-05,
      "loss": 1.6788,
      "step": 156600
    },
    {
      "epoch": 0.6299820392804053,
      "grad_norm": 3.544491767883301,
      "learning_rate": 3.9500355797488915e-05,
      "loss": 1.7338,
      "step": 156700
    },
    {
      "epoch": 0.630384069937253,
      "grad_norm": 3.1185715198516846,
      "learning_rate": 3.949365527980692e-05,
      "loss": 1.6852,
      "step": 156800
    },
    {
      "epoch": 0.6307861005941008,
      "grad_norm": 3.5992398262023926,
      "learning_rate": 3.9486954762124925e-05,
      "loss": 1.6914,
      "step": 156900
    },
    {
      "epoch": 0.6311881312509485,
      "grad_norm": 3.1741693019866943,
      "learning_rate": 3.9480254244442924e-05,
      "loss": 1.6448,
      "step": 157000
    },
    {
      "epoch": 0.6315901619077963,
      "grad_norm": 3.45399808883667,
      "learning_rate": 3.947355372676093e-05,
      "loss": 1.7134,
      "step": 157100
    },
    {
      "epoch": 0.631992192564644,
      "grad_norm": 3.432375907897949,
      "learning_rate": 3.9466853209078934e-05,
      "loss": 1.7067,
      "step": 157200
    },
    {
      "epoch": 0.6323942232214917,
      "grad_norm": 2.775611639022827,
      "learning_rate": 3.946015269139694e-05,
      "loss": 1.6875,
      "step": 157300
    },
    {
      "epoch": 0.6327962538783395,
      "grad_norm": 3.6658575534820557,
      "learning_rate": 3.9453452173714944e-05,
      "loss": 1.7173,
      "step": 157400
    },
    {
      "epoch": 0.6331982845351872,
      "grad_norm": 3.2697057723999023,
      "learning_rate": 3.944675165603295e-05,
      "loss": 1.7129,
      "step": 157500
    },
    {
      "epoch": 0.633600315192035,
      "grad_norm": 3.181318759918213,
      "learning_rate": 3.9440051138350955e-05,
      "loss": 1.7108,
      "step": 157600
    },
    {
      "epoch": 0.6340023458488827,
      "grad_norm": 3.028078079223633,
      "learning_rate": 3.943335062066896e-05,
      "loss": 1.7157,
      "step": 157700
    },
    {
      "epoch": 0.6344043765057304,
      "grad_norm": 3.2217578887939453,
      "learning_rate": 3.942665010298696e-05,
      "loss": 1.7143,
      "step": 157800
    },
    {
      "epoch": 0.6348064071625782,
      "grad_norm": 3.1903936862945557,
      "learning_rate": 3.941994958530496e-05,
      "loss": 1.6871,
      "step": 157900
    },
    {
      "epoch": 0.6352084378194259,
      "grad_norm": 3.381608724594116,
      "learning_rate": 3.941324906762297e-05,
      "loss": 1.6824,
      "step": 158000
    },
    {
      "epoch": 0.6356104684762737,
      "grad_norm": 2.6983323097229004,
      "learning_rate": 3.9406548549940967e-05,
      "loss": 1.693,
      "step": 158100
    },
    {
      "epoch": 0.6360124991331214,
      "grad_norm": 3.0154101848602295,
      "learning_rate": 3.939984803225897e-05,
      "loss": 1.6574,
      "step": 158200
    },
    {
      "epoch": 0.6364145297899692,
      "grad_norm": 4.3575873374938965,
      "learning_rate": 3.939314751457698e-05,
      "loss": 1.7034,
      "step": 158300
    },
    {
      "epoch": 0.6368165604468169,
      "grad_norm": 2.9510984420776367,
      "learning_rate": 3.938644699689498e-05,
      "loss": 1.6955,
      "step": 158400
    },
    {
      "epoch": 0.6372185911036646,
      "grad_norm": 3.722923517227173,
      "learning_rate": 3.937974647921299e-05,
      "loss": 1.6554,
      "step": 158500
    },
    {
      "epoch": 0.6376206217605124,
      "grad_norm": 2.772622585296631,
      "learning_rate": 3.937304596153099e-05,
      "loss": 1.7172,
      "step": 158600
    },
    {
      "epoch": 0.6380226524173601,
      "grad_norm": 2.869992733001709,
      "learning_rate": 3.9366345443849e-05,
      "loss": 1.6592,
      "step": 158700
    },
    {
      "epoch": 0.6384246830742079,
      "grad_norm": 2.882303237915039,
      "learning_rate": 3.9359644926167e-05,
      "loss": 1.7029,
      "step": 158800
    },
    {
      "epoch": 0.6388267137310556,
      "grad_norm": 3.12570858001709,
      "learning_rate": 3.9352944408485e-05,
      "loss": 1.6814,
      "step": 158900
    },
    {
      "epoch": 0.6392287443879033,
      "grad_norm": 3.9116666316986084,
      "learning_rate": 3.9346243890803006e-05,
      "loss": 1.7071,
      "step": 159000
    },
    {
      "epoch": 0.6396307750447511,
      "grad_norm": 3.070852041244507,
      "learning_rate": 3.9339543373121004e-05,
      "loss": 1.683,
      "step": 159100
    },
    {
      "epoch": 0.6400328057015988,
      "grad_norm": 2.881822109222412,
      "learning_rate": 3.933284285543901e-05,
      "loss": 1.6741,
      "step": 159200
    },
    {
      "epoch": 0.6404348363584466,
      "grad_norm": 3.1294214725494385,
      "learning_rate": 3.9326142337757015e-05,
      "loss": 1.6318,
      "step": 159300
    },
    {
      "epoch": 0.6408368670152943,
      "grad_norm": 3.471639394760132,
      "learning_rate": 3.931944182007502e-05,
      "loss": 1.6874,
      "step": 159400
    },
    {
      "epoch": 0.641238897672142,
      "grad_norm": 4.045228958129883,
      "learning_rate": 3.9312741302393025e-05,
      "loss": 1.7238,
      "step": 159500
    },
    {
      "epoch": 0.6416409283289898,
      "grad_norm": 3.540618419647217,
      "learning_rate": 3.930604078471103e-05,
      "loss": 1.7232,
      "step": 159600
    },
    {
      "epoch": 0.6420429589858375,
      "grad_norm": 4.420628070831299,
      "learning_rate": 3.9299340267029035e-05,
      "loss": 1.6575,
      "step": 159700
    },
    {
      "epoch": 0.6424449896426851,
      "grad_norm": 2.6976044178009033,
      "learning_rate": 3.929263974934704e-05,
      "loss": 1.6873,
      "step": 159800
    },
    {
      "epoch": 0.6428470202995329,
      "grad_norm": 3.1672797203063965,
      "learning_rate": 3.9285939231665045e-05,
      "loss": 1.7286,
      "step": 159900
    },
    {
      "epoch": 0.6432490509563806,
      "grad_norm": 2.5716254711151123,
      "learning_rate": 3.9279238713983044e-05,
      "loss": 1.7249,
      "step": 160000
    },
    {
      "epoch": 0.6436510816132284,
      "grad_norm": 2.7145140171051025,
      "learning_rate": 3.927253819630105e-05,
      "loss": 1.6878,
      "step": 160100
    },
    {
      "epoch": 0.6440531122700761,
      "grad_norm": 2.9178097248077393,
      "learning_rate": 3.926583767861905e-05,
      "loss": 1.6986,
      "step": 160200
    },
    {
      "epoch": 0.6444551429269239,
      "grad_norm": 2.9969322681427,
      "learning_rate": 3.925913716093705e-05,
      "loss": 1.7331,
      "step": 160300
    },
    {
      "epoch": 0.6448571735837716,
      "grad_norm": 3.3715569972991943,
      "learning_rate": 3.925243664325506e-05,
      "loss": 1.7661,
      "step": 160400
    },
    {
      "epoch": 0.6452592042406193,
      "grad_norm": 3.1435136795043945,
      "learning_rate": 3.924573612557306e-05,
      "loss": 1.6817,
      "step": 160500
    },
    {
      "epoch": 0.6456612348974671,
      "grad_norm": 3.367588758468628,
      "learning_rate": 3.923903560789107e-05,
      "loss": 1.6899,
      "step": 160600
    },
    {
      "epoch": 0.6460632655543148,
      "grad_norm": 3.1950905323028564,
      "learning_rate": 3.923233509020907e-05,
      "loss": 1.6649,
      "step": 160700
    },
    {
      "epoch": 0.6464652962111626,
      "grad_norm": 3.1655259132385254,
      "learning_rate": 3.922563457252708e-05,
      "loss": 1.6329,
      "step": 160800
    },
    {
      "epoch": 0.6468673268680103,
      "grad_norm": 3.0298593044281006,
      "learning_rate": 3.921893405484508e-05,
      "loss": 1.6693,
      "step": 160900
    },
    {
      "epoch": 0.647269357524858,
      "grad_norm": 3.245682954788208,
      "learning_rate": 3.921223353716308e-05,
      "loss": 1.7309,
      "step": 161000
    },
    {
      "epoch": 0.6476713881817058,
      "grad_norm": 3.613541841506958,
      "learning_rate": 3.9205533019481086e-05,
      "loss": 1.687,
      "step": 161100
    },
    {
      "epoch": 0.6480734188385535,
      "grad_norm": 3.1440165042877197,
      "learning_rate": 3.919883250179909e-05,
      "loss": 1.7075,
      "step": 161200
    },
    {
      "epoch": 0.6484754494954013,
      "grad_norm": 3.167848587036133,
      "learning_rate": 3.91921319841171e-05,
      "loss": 1.6774,
      "step": 161300
    },
    {
      "epoch": 0.648877480152249,
      "grad_norm": 2.8263368606567383,
      "learning_rate": 3.9185431466435095e-05,
      "loss": 1.6811,
      "step": 161400
    },
    {
      "epoch": 0.6492795108090967,
      "grad_norm": 3.6796348094940186,
      "learning_rate": 3.91787309487531e-05,
      "loss": 1.6525,
      "step": 161500
    },
    {
      "epoch": 0.6496815414659445,
      "grad_norm": 2.833789348602295,
      "learning_rate": 3.9172030431071105e-05,
      "loss": 1.6515,
      "step": 161600
    },
    {
      "epoch": 0.6500835721227922,
      "grad_norm": 2.9957802295684814,
      "learning_rate": 3.916532991338911e-05,
      "loss": 1.6623,
      "step": 161700
    },
    {
      "epoch": 0.65048560277964,
      "grad_norm": 3.3847594261169434,
      "learning_rate": 3.9158629395707116e-05,
      "loss": 1.7188,
      "step": 161800
    },
    {
      "epoch": 0.6508876334364877,
      "grad_norm": 3.1151392459869385,
      "learning_rate": 3.915192887802512e-05,
      "loss": 1.7214,
      "step": 161900
    },
    {
      "epoch": 0.6512896640933354,
      "grad_norm": 3.1112215518951416,
      "learning_rate": 3.9145228360343126e-05,
      "loss": 1.6733,
      "step": 162000
    },
    {
      "epoch": 0.6516916947501832,
      "grad_norm": 3.1481668949127197,
      "learning_rate": 3.9138527842661124e-05,
      "loss": 1.6854,
      "step": 162100
    },
    {
      "epoch": 0.6520937254070309,
      "grad_norm": 2.5383243560791016,
      "learning_rate": 3.913182732497913e-05,
      "loss": 1.6923,
      "step": 162200
    },
    {
      "epoch": 0.6524957560638787,
      "grad_norm": 3.1959245204925537,
      "learning_rate": 3.9125126807297134e-05,
      "loss": 1.6891,
      "step": 162300
    },
    {
      "epoch": 0.6528977867207264,
      "grad_norm": 2.915672540664673,
      "learning_rate": 3.911842628961514e-05,
      "loss": 1.7133,
      "step": 162400
    },
    {
      "epoch": 0.6532998173775741,
      "grad_norm": 3.2455008029937744,
      "learning_rate": 3.911172577193314e-05,
      "loss": 1.7278,
      "step": 162500
    },
    {
      "epoch": 0.6537018480344219,
      "grad_norm": 2.8012535572052,
      "learning_rate": 3.910502525425114e-05,
      "loss": 1.687,
      "step": 162600
    },
    {
      "epoch": 0.6541038786912696,
      "grad_norm": 3.506273031234741,
      "learning_rate": 3.909832473656915e-05,
      "loss": 1.6469,
      "step": 162700
    },
    {
      "epoch": 0.6545059093481174,
      "grad_norm": 3.526571750640869,
      "learning_rate": 3.909162421888715e-05,
      "loss": 1.6386,
      "step": 162800
    },
    {
      "epoch": 0.6549079400049651,
      "grad_norm": 2.402888774871826,
      "learning_rate": 3.908492370120516e-05,
      "loss": 1.6552,
      "step": 162900
    },
    {
      "epoch": 0.6553099706618128,
      "grad_norm": 3.30584979057312,
      "learning_rate": 3.9078223183523164e-05,
      "loss": 1.6987,
      "step": 163000
    },
    {
      "epoch": 0.6557120013186606,
      "grad_norm": 3.2170093059539795,
      "learning_rate": 3.907152266584116e-05,
      "loss": 1.6969,
      "step": 163100
    },
    {
      "epoch": 0.6561140319755083,
      "grad_norm": 2.8096861839294434,
      "learning_rate": 3.906482214815917e-05,
      "loss": 1.7055,
      "step": 163200
    },
    {
      "epoch": 0.6565160626323561,
      "grad_norm": 3.7176918983459473,
      "learning_rate": 3.905812163047717e-05,
      "loss": 1.7173,
      "step": 163300
    },
    {
      "epoch": 0.6569180932892038,
      "grad_norm": 3.231675148010254,
      "learning_rate": 3.905142111279518e-05,
      "loss": 1.6985,
      "step": 163400
    },
    {
      "epoch": 0.6573201239460515,
      "grad_norm": 2.8809492588043213,
      "learning_rate": 3.904472059511318e-05,
      "loss": 1.6709,
      "step": 163500
    },
    {
      "epoch": 0.6577221546028993,
      "grad_norm": 3.1393685340881348,
      "learning_rate": 3.903802007743119e-05,
      "loss": 1.7284,
      "step": 163600
    },
    {
      "epoch": 0.658124185259747,
      "grad_norm": 2.6556406021118164,
      "learning_rate": 3.9031319559749186e-05,
      "loss": 1.6968,
      "step": 163700
    },
    {
      "epoch": 0.6585262159165948,
      "grad_norm": 3.5301883220672607,
      "learning_rate": 3.902461904206719e-05,
      "loss": 1.6856,
      "step": 163800
    },
    {
      "epoch": 0.6589282465734425,
      "grad_norm": 2.800527334213257,
      "learning_rate": 3.9017918524385196e-05,
      "loss": 1.7053,
      "step": 163900
    },
    {
      "epoch": 0.6593302772302901,
      "grad_norm": 3.4893648624420166,
      "learning_rate": 3.90112180067032e-05,
      "loss": 1.6869,
      "step": 164000
    },
    {
      "epoch": 0.6597323078871379,
      "grad_norm": 3.3909223079681396,
      "learning_rate": 3.90045174890212e-05,
      "loss": 1.6988,
      "step": 164100
    },
    {
      "epoch": 0.6601343385439856,
      "grad_norm": 2.7872326374053955,
      "learning_rate": 3.8997816971339205e-05,
      "loss": 1.6796,
      "step": 164200
    },
    {
      "epoch": 0.6605363692008334,
      "grad_norm": 3.332543134689331,
      "learning_rate": 3.899111645365721e-05,
      "loss": 1.7018,
      "step": 164300
    },
    {
      "epoch": 0.6609383998576811,
      "grad_norm": 3.3422770500183105,
      "learning_rate": 3.8984415935975215e-05,
      "loss": 1.6819,
      "step": 164400
    },
    {
      "epoch": 0.6613404305145288,
      "grad_norm": 5.324060916900635,
      "learning_rate": 3.897771541829322e-05,
      "loss": 1.682,
      "step": 164500
    },
    {
      "epoch": 0.6617424611713766,
      "grad_norm": 3.07249116897583,
      "learning_rate": 3.8971014900611225e-05,
      "loss": 1.7078,
      "step": 164600
    },
    {
      "epoch": 0.6621444918282243,
      "grad_norm": 3.517540693283081,
      "learning_rate": 3.896431438292923e-05,
      "loss": 1.6822,
      "step": 164700
    },
    {
      "epoch": 0.6625465224850721,
      "grad_norm": 3.2444188594818115,
      "learning_rate": 3.8957613865247235e-05,
      "loss": 1.6927,
      "step": 164800
    },
    {
      "epoch": 0.6629485531419198,
      "grad_norm": 2.7712106704711914,
      "learning_rate": 3.8950913347565234e-05,
      "loss": 1.6709,
      "step": 164900
    },
    {
      "epoch": 0.6633505837987675,
      "grad_norm": 2.8942723274230957,
      "learning_rate": 3.894421282988324e-05,
      "loss": 1.6748,
      "step": 165000
    },
    {
      "epoch": 0.6637526144556153,
      "grad_norm": 3.172173500061035,
      "learning_rate": 3.8937512312201244e-05,
      "loss": 1.6646,
      "step": 165100
    },
    {
      "epoch": 0.664154645112463,
      "grad_norm": 3.567885398864746,
      "learning_rate": 3.893081179451924e-05,
      "loss": 1.6992,
      "step": 165200
    },
    {
      "epoch": 0.6645566757693108,
      "grad_norm": 3.3455324172973633,
      "learning_rate": 3.892411127683725e-05,
      "loss": 1.7494,
      "step": 165300
    },
    {
      "epoch": 0.6649587064261585,
      "grad_norm": 3.7246015071868896,
      "learning_rate": 3.891741075915525e-05,
      "loss": 1.6874,
      "step": 165400
    },
    {
      "epoch": 0.6653607370830062,
      "grad_norm": 2.6631720066070557,
      "learning_rate": 3.891071024147326e-05,
      "loss": 1.678,
      "step": 165500
    },
    {
      "epoch": 0.665762767739854,
      "grad_norm": 3.503347396850586,
      "learning_rate": 3.890400972379126e-05,
      "loss": 1.7135,
      "step": 165600
    },
    {
      "epoch": 0.6661647983967017,
      "grad_norm": 2.898815631866455,
      "learning_rate": 3.889730920610927e-05,
      "loss": 1.7294,
      "step": 165700
    },
    {
      "epoch": 0.6665668290535495,
      "grad_norm": 3.880350351333618,
      "learning_rate": 3.889060868842727e-05,
      "loss": 1.7364,
      "step": 165800
    },
    {
      "epoch": 0.6669688597103972,
      "grad_norm": 3.3059439659118652,
      "learning_rate": 3.888390817074528e-05,
      "loss": 1.692,
      "step": 165900
    },
    {
      "epoch": 0.667370890367245,
      "grad_norm": 3.4449574947357178,
      "learning_rate": 3.887720765306328e-05,
      "loss": 1.6892,
      "step": 166000
    },
    {
      "epoch": 0.6677729210240927,
      "grad_norm": 2.8816120624542236,
      "learning_rate": 3.887050713538128e-05,
      "loss": 1.6849,
      "step": 166100
    },
    {
      "epoch": 0.6681749516809404,
      "grad_norm": 3.531365156173706,
      "learning_rate": 3.886380661769928e-05,
      "loss": 1.7141,
      "step": 166200
    },
    {
      "epoch": 0.6685769823377882,
      "grad_norm": 3.6241488456726074,
      "learning_rate": 3.8857106100017285e-05,
      "loss": 1.6789,
      "step": 166300
    },
    {
      "epoch": 0.6689790129946359,
      "grad_norm": 3.344231605529785,
      "learning_rate": 3.885040558233529e-05,
      "loss": 1.6574,
      "step": 166400
    },
    {
      "epoch": 0.6693810436514837,
      "grad_norm": 3.001770496368408,
      "learning_rate": 3.8843705064653296e-05,
      "loss": 1.6738,
      "step": 166500
    },
    {
      "epoch": 0.6697830743083314,
      "grad_norm": 3.418743371963501,
      "learning_rate": 3.88370045469713e-05,
      "loss": 1.7205,
      "step": 166600
    },
    {
      "epoch": 0.6701851049651791,
      "grad_norm": 3.1623780727386475,
      "learning_rate": 3.8830304029289306e-05,
      "loss": 1.7113,
      "step": 166700
    },
    {
      "epoch": 0.6705871356220269,
      "grad_norm": 3.2390995025634766,
      "learning_rate": 3.882360351160731e-05,
      "loss": 1.6415,
      "step": 166800
    },
    {
      "epoch": 0.6709891662788746,
      "grad_norm": 3.4446778297424316,
      "learning_rate": 3.8816902993925316e-05,
      "loss": 1.6815,
      "step": 166900
    },
    {
      "epoch": 0.6713911969357224,
      "grad_norm": 2.6794283390045166,
      "learning_rate": 3.881020247624332e-05,
      "loss": 1.7162,
      "step": 167000
    },
    {
      "epoch": 0.6717932275925701,
      "grad_norm": 3.0859017372131348,
      "learning_rate": 3.880350195856132e-05,
      "loss": 1.6883,
      "step": 167100
    },
    {
      "epoch": 0.6721952582494178,
      "grad_norm": 3.993783473968506,
      "learning_rate": 3.8796801440879325e-05,
      "loss": 1.6968,
      "step": 167200
    },
    {
      "epoch": 0.6725972889062656,
      "grad_norm": 3.069892406463623,
      "learning_rate": 3.879010092319732e-05,
      "loss": 1.6924,
      "step": 167300
    },
    {
      "epoch": 0.6729993195631133,
      "grad_norm": 2.991931915283203,
      "learning_rate": 3.878340040551533e-05,
      "loss": 1.7211,
      "step": 167400
    },
    {
      "epoch": 0.6734013502199611,
      "grad_norm": 3.0297038555145264,
      "learning_rate": 3.877669988783333e-05,
      "loss": 1.6859,
      "step": 167500
    },
    {
      "epoch": 0.6738033808768088,
      "grad_norm": 3.4592795372009277,
      "learning_rate": 3.876999937015134e-05,
      "loss": 1.6941,
      "step": 167600
    },
    {
      "epoch": 0.6742054115336565,
      "grad_norm": 3.162193775177002,
      "learning_rate": 3.8763298852469344e-05,
      "loss": 1.6803,
      "step": 167700
    },
    {
      "epoch": 0.6746074421905043,
      "grad_norm": 3.0804944038391113,
      "learning_rate": 3.875659833478735e-05,
      "loss": 1.6858,
      "step": 167800
    },
    {
      "epoch": 0.675009472847352,
      "grad_norm": 2.843778610229492,
      "learning_rate": 3.8749897817105354e-05,
      "loss": 1.6847,
      "step": 167900
    },
    {
      "epoch": 0.6754115035041998,
      "grad_norm": 3.038562297821045,
      "learning_rate": 3.874319729942336e-05,
      "loss": 1.6705,
      "step": 168000
    },
    {
      "epoch": 0.6758135341610475,
      "grad_norm": 3.3086097240448,
      "learning_rate": 3.873649678174136e-05,
      "loss": 1.7429,
      "step": 168100
    },
    {
      "epoch": 0.6762155648178952,
      "grad_norm": 2.998128890991211,
      "learning_rate": 3.872979626405936e-05,
      "loss": 1.6774,
      "step": 168200
    },
    {
      "epoch": 0.6766175954747429,
      "grad_norm": 3.031810998916626,
      "learning_rate": 3.872309574637737e-05,
      "loss": 1.7432,
      "step": 168300
    },
    {
      "epoch": 0.6770196261315906,
      "grad_norm": 3.1866166591644287,
      "learning_rate": 3.871639522869537e-05,
      "loss": 1.6953,
      "step": 168400
    },
    {
      "epoch": 0.6774216567884384,
      "grad_norm": 3.0104598999023438,
      "learning_rate": 3.870969471101337e-05,
      "loss": 1.6829,
      "step": 168500
    },
    {
      "epoch": 0.6778236874452861,
      "grad_norm": 3.263295888900757,
      "learning_rate": 3.8702994193331376e-05,
      "loss": 1.7069,
      "step": 168600
    },
    {
      "epoch": 0.6782257181021338,
      "grad_norm": 3.2560489177703857,
      "learning_rate": 3.869629367564938e-05,
      "loss": 1.7127,
      "step": 168700
    },
    {
      "epoch": 0.6786277487589816,
      "grad_norm": 3.1403720378875732,
      "learning_rate": 3.8689593157967386e-05,
      "loss": 1.667,
      "step": 168800
    },
    {
      "epoch": 0.6790297794158293,
      "grad_norm": 3.0639853477478027,
      "learning_rate": 3.868289264028539e-05,
      "loss": 1.6697,
      "step": 168900
    },
    {
      "epoch": 0.6794318100726771,
      "grad_norm": 2.9323244094848633,
      "learning_rate": 3.8676192122603397e-05,
      "loss": 1.6854,
      "step": 169000
    },
    {
      "epoch": 0.6798338407295248,
      "grad_norm": 3.247725248336792,
      "learning_rate": 3.86694916049214e-05,
      "loss": 1.6776,
      "step": 169100
    },
    {
      "epoch": 0.6802358713863725,
      "grad_norm": 3.395927906036377,
      "learning_rate": 3.86627910872394e-05,
      "loss": 1.7371,
      "step": 169200
    },
    {
      "epoch": 0.6806379020432203,
      "grad_norm": 3.345348834991455,
      "learning_rate": 3.8656090569557405e-05,
      "loss": 1.6795,
      "step": 169300
    },
    {
      "epoch": 0.681039932700068,
      "grad_norm": 2.469642400741577,
      "learning_rate": 3.864939005187541e-05,
      "loss": 1.6542,
      "step": 169400
    },
    {
      "epoch": 0.6814419633569158,
      "grad_norm": 3.929825782775879,
      "learning_rate": 3.8642689534193415e-05,
      "loss": 1.6729,
      "step": 169500
    },
    {
      "epoch": 0.6818439940137635,
      "grad_norm": 2.9317522048950195,
      "learning_rate": 3.863598901651142e-05,
      "loss": 1.6755,
      "step": 169600
    },
    {
      "epoch": 0.6822460246706112,
      "grad_norm": 2.7102460861206055,
      "learning_rate": 3.862928849882942e-05,
      "loss": 1.7489,
      "step": 169700
    },
    {
      "epoch": 0.682648055327459,
      "grad_norm": 2.8018298149108887,
      "learning_rate": 3.8622587981147424e-05,
      "loss": 1.6928,
      "step": 169800
    },
    {
      "epoch": 0.6830500859843067,
      "grad_norm": 2.890021562576294,
      "learning_rate": 3.861588746346543e-05,
      "loss": 1.6661,
      "step": 169900
    },
    {
      "epoch": 0.6834521166411545,
      "grad_norm": 3.915796995162964,
      "learning_rate": 3.8609186945783434e-05,
      "loss": 1.7101,
      "step": 170000
    },
    {
      "epoch": 0.6838541472980022,
      "grad_norm": 2.9337778091430664,
      "learning_rate": 3.860248642810144e-05,
      "loss": 1.6661,
      "step": 170100
    },
    {
      "epoch": 0.6842561779548499,
      "grad_norm": 4.012371063232422,
      "learning_rate": 3.859578591041944e-05,
      "loss": 1.6114,
      "step": 170200
    },
    {
      "epoch": 0.6846582086116977,
      "grad_norm": 3.857825994491577,
      "learning_rate": 3.858908539273744e-05,
      "loss": 1.6684,
      "step": 170300
    },
    {
      "epoch": 0.6850602392685454,
      "grad_norm": 2.949723482131958,
      "learning_rate": 3.858238487505545e-05,
      "loss": 1.7071,
      "step": 170400
    },
    {
      "epoch": 0.6854622699253932,
      "grad_norm": 2.7082629203796387,
      "learning_rate": 3.857568435737345e-05,
      "loss": 1.6922,
      "step": 170500
    },
    {
      "epoch": 0.6858643005822409,
      "grad_norm": 3.045466899871826,
      "learning_rate": 3.856898383969146e-05,
      "loss": 1.72,
      "step": 170600
    },
    {
      "epoch": 0.6862663312390886,
      "grad_norm": 3.321746349334717,
      "learning_rate": 3.8562283322009463e-05,
      "loss": 1.6334,
      "step": 170700
    },
    {
      "epoch": 0.6866683618959364,
      "grad_norm": 2.661973237991333,
      "learning_rate": 3.855558280432747e-05,
      "loss": 1.7276,
      "step": 170800
    },
    {
      "epoch": 0.6870703925527841,
      "grad_norm": 3.1592445373535156,
      "learning_rate": 3.854888228664547e-05,
      "loss": 1.6557,
      "step": 170900
    },
    {
      "epoch": 0.6874724232096319,
      "grad_norm": 3.4468159675598145,
      "learning_rate": 3.854218176896347e-05,
      "loss": 1.6362,
      "step": 171000
    },
    {
      "epoch": 0.6878744538664796,
      "grad_norm": 2.5906763076782227,
      "learning_rate": 3.853548125128148e-05,
      "loss": 1.6546,
      "step": 171100
    },
    {
      "epoch": 0.6882764845233273,
      "grad_norm": 3.373521566390991,
      "learning_rate": 3.8528780733599475e-05,
      "loss": 1.6957,
      "step": 171200
    },
    {
      "epoch": 0.6886785151801751,
      "grad_norm": 2.897975206375122,
      "learning_rate": 3.852208021591748e-05,
      "loss": 1.6655,
      "step": 171300
    },
    {
      "epoch": 0.6890805458370228,
      "grad_norm": 3.073345422744751,
      "learning_rate": 3.8515379698235486e-05,
      "loss": 1.6551,
      "step": 171400
    },
    {
      "epoch": 0.6894825764938706,
      "grad_norm": 3.1252083778381348,
      "learning_rate": 3.850867918055349e-05,
      "loss": 1.6663,
      "step": 171500
    },
    {
      "epoch": 0.6898846071507183,
      "grad_norm": 3.4225828647613525,
      "learning_rate": 3.8501978662871496e-05,
      "loss": 1.7629,
      "step": 171600
    },
    {
      "epoch": 0.690286637807566,
      "grad_norm": 3.2033302783966064,
      "learning_rate": 3.84952781451895e-05,
      "loss": 1.6545,
      "step": 171700
    },
    {
      "epoch": 0.6906886684644138,
      "grad_norm": 3.1443889141082764,
      "learning_rate": 3.8488577627507506e-05,
      "loss": 1.7636,
      "step": 171800
    },
    {
      "epoch": 0.6910906991212615,
      "grad_norm": 2.9820716381073,
      "learning_rate": 3.848187710982551e-05,
      "loss": 1.6553,
      "step": 171900
    },
    {
      "epoch": 0.6914927297781093,
      "grad_norm": 2.895965814590454,
      "learning_rate": 3.847517659214351e-05,
      "loss": 1.6413,
      "step": 172000
    },
    {
      "epoch": 0.691894760434957,
      "grad_norm": 3.0632355213165283,
      "learning_rate": 3.8468476074461515e-05,
      "loss": 1.6711,
      "step": 172100
    },
    {
      "epoch": 0.6922967910918048,
      "grad_norm": 3.011404514312744,
      "learning_rate": 3.846177555677952e-05,
      "loss": 1.6438,
      "step": 172200
    },
    {
      "epoch": 0.6926988217486525,
      "grad_norm": 3.107849597930908,
      "learning_rate": 3.845507503909752e-05,
      "loss": 1.6856,
      "step": 172300
    },
    {
      "epoch": 0.6931008524055002,
      "grad_norm": 3.1021201610565186,
      "learning_rate": 3.8448374521415523e-05,
      "loss": 1.7038,
      "step": 172400
    },
    {
      "epoch": 0.693502883062348,
      "grad_norm": 3.138890266418457,
      "learning_rate": 3.844167400373353e-05,
      "loss": 1.6523,
      "step": 172500
    },
    {
      "epoch": 0.6939049137191956,
      "grad_norm": 2.7634119987487793,
      "learning_rate": 3.8434973486051534e-05,
      "loss": 1.6878,
      "step": 172600
    },
    {
      "epoch": 0.6943069443760433,
      "grad_norm": 2.969681739807129,
      "learning_rate": 3.842827296836954e-05,
      "loss": 1.683,
      "step": 172700
    },
    {
      "epoch": 0.6947089750328911,
      "grad_norm": 3.110100030899048,
      "learning_rate": 3.8421572450687544e-05,
      "loss": 1.7455,
      "step": 172800
    },
    {
      "epoch": 0.6951110056897388,
      "grad_norm": 3.0742874145507812,
      "learning_rate": 3.841487193300555e-05,
      "loss": 1.7112,
      "step": 172900
    },
    {
      "epoch": 0.6955130363465866,
      "grad_norm": 3.1009409427642822,
      "learning_rate": 3.8408171415323554e-05,
      "loss": 1.6747,
      "step": 173000
    },
    {
      "epoch": 0.6959150670034343,
      "grad_norm": 3.114596128463745,
      "learning_rate": 3.840147089764156e-05,
      "loss": 1.6595,
      "step": 173100
    },
    {
      "epoch": 0.696317097660282,
      "grad_norm": 3.734555721282959,
      "learning_rate": 3.839477037995956e-05,
      "loss": 1.6633,
      "step": 173200
    },
    {
      "epoch": 0.6967191283171298,
      "grad_norm": 3.390665054321289,
      "learning_rate": 3.8388069862277556e-05,
      "loss": 1.6205,
      "step": 173300
    },
    {
      "epoch": 0.6971211589739775,
      "grad_norm": 3.268596649169922,
      "learning_rate": 3.838136934459556e-05,
      "loss": 1.6429,
      "step": 173400
    },
    {
      "epoch": 0.6975231896308253,
      "grad_norm": 3.0194449424743652,
      "learning_rate": 3.8374668826913566e-05,
      "loss": 1.6294,
      "step": 173500
    },
    {
      "epoch": 0.697925220287673,
      "grad_norm": 3.5123038291931152,
      "learning_rate": 3.836796830923157e-05,
      "loss": 1.6858,
      "step": 173600
    },
    {
      "epoch": 0.6983272509445207,
      "grad_norm": 3.4538090229034424,
      "learning_rate": 3.8361267791549577e-05,
      "loss": 1.672,
      "step": 173700
    },
    {
      "epoch": 0.6987292816013685,
      "grad_norm": 3.3228979110717773,
      "learning_rate": 3.835456727386758e-05,
      "loss": 1.6557,
      "step": 173800
    },
    {
      "epoch": 0.6991313122582162,
      "grad_norm": 2.9457898139953613,
      "learning_rate": 3.834786675618559e-05,
      "loss": 1.6774,
      "step": 173900
    },
    {
      "epoch": 0.699533342915064,
      "grad_norm": 4.396070957183838,
      "learning_rate": 3.834116623850359e-05,
      "loss": 1.6771,
      "step": 174000
    },
    {
      "epoch": 0.6999353735719117,
      "grad_norm": 3.237196683883667,
      "learning_rate": 3.83344657208216e-05,
      "loss": 1.6493,
      "step": 174100
    },
    {
      "epoch": 0.7003374042287595,
      "grad_norm": 3.534224033355713,
      "learning_rate": 3.8327765203139595e-05,
      "loss": 1.6481,
      "step": 174200
    },
    {
      "epoch": 0.7007394348856072,
      "grad_norm": 3.3385684490203857,
      "learning_rate": 3.83210646854576e-05,
      "loss": 1.6627,
      "step": 174300
    },
    {
      "epoch": 0.7011414655424549,
      "grad_norm": 2.9820470809936523,
      "learning_rate": 3.83143641677756e-05,
      "loss": 1.6508,
      "step": 174400
    },
    {
      "epoch": 0.7015434961993027,
      "grad_norm": 2.77201247215271,
      "learning_rate": 3.8307663650093604e-05,
      "loss": 1.6968,
      "step": 174500
    },
    {
      "epoch": 0.7019455268561504,
      "grad_norm": 3.4871654510498047,
      "learning_rate": 3.830096313241161e-05,
      "loss": 1.7014,
      "step": 174600
    },
    {
      "epoch": 0.7023475575129982,
      "grad_norm": 2.7239582538604736,
      "learning_rate": 3.8294262614729614e-05,
      "loss": 1.6909,
      "step": 174700
    },
    {
      "epoch": 0.7027495881698459,
      "grad_norm": 3.7197322845458984,
      "learning_rate": 3.828756209704762e-05,
      "loss": 1.6972,
      "step": 174800
    },
    {
      "epoch": 0.7031516188266936,
      "grad_norm": 3.2169973850250244,
      "learning_rate": 3.8280861579365624e-05,
      "loss": 1.6734,
      "step": 174900
    },
    {
      "epoch": 0.7035536494835414,
      "grad_norm": 3.320847272872925,
      "learning_rate": 3.827416106168363e-05,
      "loss": 1.646,
      "step": 175000
    },
    {
      "epoch": 0.7039556801403891,
      "grad_norm": 3.393789529800415,
      "learning_rate": 3.8267460544001635e-05,
      "loss": 1.7194,
      "step": 175100
    },
    {
      "epoch": 0.7043577107972369,
      "grad_norm": 2.7824289798736572,
      "learning_rate": 3.826076002631963e-05,
      "loss": 1.6908,
      "step": 175200
    },
    {
      "epoch": 0.7047597414540846,
      "grad_norm": 2.6286685466766357,
      "learning_rate": 3.825405950863764e-05,
      "loss": 1.6847,
      "step": 175300
    },
    {
      "epoch": 0.7051617721109323,
      "grad_norm": 4.080735206604004,
      "learning_rate": 3.824735899095564e-05,
      "loss": 1.7062,
      "step": 175400
    },
    {
      "epoch": 0.7055638027677801,
      "grad_norm": 3.0133588314056396,
      "learning_rate": 3.824065847327365e-05,
      "loss": 1.6925,
      "step": 175500
    },
    {
      "epoch": 0.7059658334246278,
      "grad_norm": 3.2220702171325684,
      "learning_rate": 3.823395795559165e-05,
      "loss": 1.6446,
      "step": 175600
    },
    {
      "epoch": 0.7063678640814756,
      "grad_norm": 3.085270404815674,
      "learning_rate": 3.822725743790965e-05,
      "loss": 1.7085,
      "step": 175700
    },
    {
      "epoch": 0.7067698947383233,
      "grad_norm": 3.3545920848846436,
      "learning_rate": 3.822055692022766e-05,
      "loss": 1.7318,
      "step": 175800
    },
    {
      "epoch": 0.707171925395171,
      "grad_norm": 2.995243787765503,
      "learning_rate": 3.821385640254566e-05,
      "loss": 1.6944,
      "step": 175900
    },
    {
      "epoch": 0.7075739560520188,
      "grad_norm": 2.7205538749694824,
      "learning_rate": 3.820715588486367e-05,
      "loss": 1.7048,
      "step": 176000
    },
    {
      "epoch": 0.7079759867088665,
      "grad_norm": 3.4306492805480957,
      "learning_rate": 3.820045536718167e-05,
      "loss": 1.6881,
      "step": 176100
    },
    {
      "epoch": 0.7083780173657143,
      "grad_norm": 3.443296432495117,
      "learning_rate": 3.819375484949968e-05,
      "loss": 1.6869,
      "step": 176200
    },
    {
      "epoch": 0.708780048022562,
      "grad_norm": 3.0236287117004395,
      "learning_rate": 3.8187054331817676e-05,
      "loss": 1.6773,
      "step": 176300
    },
    {
      "epoch": 0.7091820786794097,
      "grad_norm": 3.0964183807373047,
      "learning_rate": 3.818035381413568e-05,
      "loss": 1.669,
      "step": 176400
    },
    {
      "epoch": 0.7095841093362575,
      "grad_norm": 2.818934679031372,
      "learning_rate": 3.8173653296453686e-05,
      "loss": 1.702,
      "step": 176500
    },
    {
      "epoch": 0.7099861399931052,
      "grad_norm": 3.36368465423584,
      "learning_rate": 3.816695277877169e-05,
      "loss": 1.6659,
      "step": 176600
    },
    {
      "epoch": 0.710388170649953,
      "grad_norm": 3.693195343017578,
      "learning_rate": 3.8160252261089696e-05,
      "loss": 1.6461,
      "step": 176700
    },
    {
      "epoch": 0.7107902013068006,
      "grad_norm": 3.5648961067199707,
      "learning_rate": 3.8153551743407695e-05,
      "loss": 1.6693,
      "step": 176800
    },
    {
      "epoch": 0.7111922319636483,
      "grad_norm": 3.478905200958252,
      "learning_rate": 3.81468512257257e-05,
      "loss": 1.6799,
      "step": 176900
    },
    {
      "epoch": 0.7115942626204961,
      "grad_norm": 3.7749335765838623,
      "learning_rate": 3.8140150708043705e-05,
      "loss": 1.6494,
      "step": 177000
    },
    {
      "epoch": 0.7119962932773438,
      "grad_norm": 3.6215713024139404,
      "learning_rate": 3.813345019036171e-05,
      "loss": 1.6558,
      "step": 177100
    },
    {
      "epoch": 0.7123983239341916,
      "grad_norm": 3.442946672439575,
      "learning_rate": 3.8126749672679715e-05,
      "loss": 1.662,
      "step": 177200
    },
    {
      "epoch": 0.7128003545910393,
      "grad_norm": 4.24252986907959,
      "learning_rate": 3.8120049154997714e-05,
      "loss": 1.6492,
      "step": 177300
    },
    {
      "epoch": 0.713202385247887,
      "grad_norm": 3.006714344024658,
      "learning_rate": 3.811334863731572e-05,
      "loss": 1.6789,
      "step": 177400
    },
    {
      "epoch": 0.7136044159047348,
      "grad_norm": 4.594377517700195,
      "learning_rate": 3.8106648119633724e-05,
      "loss": 1.7355,
      "step": 177500
    },
    {
      "epoch": 0.7140064465615825,
      "grad_norm": 2.8495066165924072,
      "learning_rate": 3.809994760195173e-05,
      "loss": 1.6263,
      "step": 177600
    },
    {
      "epoch": 0.7144084772184303,
      "grad_norm": 3.226896286010742,
      "learning_rate": 3.8093247084269734e-05,
      "loss": 1.697,
      "step": 177700
    },
    {
      "epoch": 0.714810507875278,
      "grad_norm": 3.2620811462402344,
      "learning_rate": 3.808654656658774e-05,
      "loss": 1.6495,
      "step": 177800
    },
    {
      "epoch": 0.7152125385321257,
      "grad_norm": 3.04982590675354,
      "learning_rate": 3.8079846048905744e-05,
      "loss": 1.6482,
      "step": 177900
    },
    {
      "epoch": 0.7156145691889735,
      "grad_norm": 3.149653911590576,
      "learning_rate": 3.807314553122374e-05,
      "loss": 1.6784,
      "step": 178000
    },
    {
      "epoch": 0.7160165998458212,
      "grad_norm": 3.577007293701172,
      "learning_rate": 3.806644501354175e-05,
      "loss": 1.7118,
      "step": 178100
    },
    {
      "epoch": 0.716418630502669,
      "grad_norm": 3.29565691947937,
      "learning_rate": 3.805974449585975e-05,
      "loss": 1.6941,
      "step": 178200
    },
    {
      "epoch": 0.7168206611595167,
      "grad_norm": 2.8780081272125244,
      "learning_rate": 3.805304397817776e-05,
      "loss": 1.7183,
      "step": 178300
    },
    {
      "epoch": 0.7172226918163644,
      "grad_norm": 4.53796911239624,
      "learning_rate": 3.8046343460495756e-05,
      "loss": 1.675,
      "step": 178400
    },
    {
      "epoch": 0.7176247224732122,
      "grad_norm": 2.6631062030792236,
      "learning_rate": 3.803964294281376e-05,
      "loss": 1.7087,
      "step": 178500
    },
    {
      "epoch": 0.7180267531300599,
      "grad_norm": 3.21515154838562,
      "learning_rate": 3.803294242513177e-05,
      "loss": 1.6539,
      "step": 178600
    },
    {
      "epoch": 0.7184287837869077,
      "grad_norm": 2.6800029277801514,
      "learning_rate": 3.802624190744977e-05,
      "loss": 1.6346,
      "step": 178700
    },
    {
      "epoch": 0.7188308144437554,
      "grad_norm": 3.439215898513794,
      "learning_rate": 3.801954138976778e-05,
      "loss": 1.7139,
      "step": 178800
    },
    {
      "epoch": 0.7192328451006031,
      "grad_norm": 3.1137473583221436,
      "learning_rate": 3.801284087208578e-05,
      "loss": 1.6338,
      "step": 178900
    },
    {
      "epoch": 0.7196348757574509,
      "grad_norm": 3.6179633140563965,
      "learning_rate": 3.800614035440379e-05,
      "loss": 1.6997,
      "step": 179000
    },
    {
      "epoch": 0.7200369064142986,
      "grad_norm": 3.1228654384613037,
      "learning_rate": 3.799943983672179e-05,
      "loss": 1.6441,
      "step": 179100
    },
    {
      "epoch": 0.7204389370711464,
      "grad_norm": 3.2613685131073,
      "learning_rate": 3.799273931903979e-05,
      "loss": 1.6447,
      "step": 179200
    },
    {
      "epoch": 0.7208409677279941,
      "grad_norm": 3.058802604675293,
      "learning_rate": 3.7986038801357796e-05,
      "loss": 1.6765,
      "step": 179300
    },
    {
      "epoch": 0.7212429983848418,
      "grad_norm": 3.620574474334717,
      "learning_rate": 3.7979338283675794e-05,
      "loss": 1.7228,
      "step": 179400
    },
    {
      "epoch": 0.7216450290416896,
      "grad_norm": 3.7244455814361572,
      "learning_rate": 3.79726377659938e-05,
      "loss": 1.6761,
      "step": 179500
    },
    {
      "epoch": 0.7220470596985373,
      "grad_norm": 2.339780569076538,
      "learning_rate": 3.7965937248311804e-05,
      "loss": 1.6736,
      "step": 179600
    },
    {
      "epoch": 0.7224490903553851,
      "grad_norm": 3.117095947265625,
      "learning_rate": 3.795923673062981e-05,
      "loss": 1.6434,
      "step": 179700
    },
    {
      "epoch": 0.7228511210122328,
      "grad_norm": 3.3756182193756104,
      "learning_rate": 3.7952536212947815e-05,
      "loss": 1.6513,
      "step": 179800
    },
    {
      "epoch": 0.7232531516690806,
      "grad_norm": 2.98825740814209,
      "learning_rate": 3.794583569526582e-05,
      "loss": 1.682,
      "step": 179900
    },
    {
      "epoch": 0.7236551823259283,
      "grad_norm": 3.693495512008667,
      "learning_rate": 3.7939135177583825e-05,
      "loss": 1.6516,
      "step": 180000
    },
    {
      "epoch": 0.724057212982776,
      "grad_norm": 3.5002224445343018,
      "learning_rate": 3.793243465990183e-05,
      "loss": 1.6541,
      "step": 180100
    },
    {
      "epoch": 0.7244592436396238,
      "grad_norm": 3.0108790397644043,
      "learning_rate": 3.7925734142219835e-05,
      "loss": 1.6378,
      "step": 180200
    },
    {
      "epoch": 0.7248612742964715,
      "grad_norm": 2.8888707160949707,
      "learning_rate": 3.7919033624537834e-05,
      "loss": 1.7051,
      "step": 180300
    },
    {
      "epoch": 0.7252633049533193,
      "grad_norm": 3.057263135910034,
      "learning_rate": 3.791233310685583e-05,
      "loss": 1.6896,
      "step": 180400
    },
    {
      "epoch": 0.725665335610167,
      "grad_norm": 2.9877166748046875,
      "learning_rate": 3.790563258917384e-05,
      "loss": 1.6755,
      "step": 180500
    },
    {
      "epoch": 0.7260673662670147,
      "grad_norm": 3.5715551376342773,
      "learning_rate": 3.789893207149184e-05,
      "loss": 1.6439,
      "step": 180600
    },
    {
      "epoch": 0.7264693969238625,
      "grad_norm": 3.3850626945495605,
      "learning_rate": 3.789223155380985e-05,
      "loss": 1.6685,
      "step": 180700
    },
    {
      "epoch": 0.7268714275807102,
      "grad_norm": 2.9845540523529053,
      "learning_rate": 3.788553103612785e-05,
      "loss": 1.6643,
      "step": 180800
    },
    {
      "epoch": 0.727273458237558,
      "grad_norm": 2.9646708965301514,
      "learning_rate": 3.787883051844586e-05,
      "loss": 1.6813,
      "step": 180900
    },
    {
      "epoch": 0.7276754888944057,
      "grad_norm": 3.6519124507904053,
      "learning_rate": 3.787213000076386e-05,
      "loss": 1.645,
      "step": 181000
    },
    {
      "epoch": 0.7280775195512533,
      "grad_norm": 2.4940102100372314,
      "learning_rate": 3.786542948308187e-05,
      "loss": 1.6628,
      "step": 181100
    },
    {
      "epoch": 0.7284795502081011,
      "grad_norm": 3.045520544052124,
      "learning_rate": 3.785872896539987e-05,
      "loss": 1.6259,
      "step": 181200
    },
    {
      "epoch": 0.7288815808649488,
      "grad_norm": 3.853144645690918,
      "learning_rate": 3.785202844771787e-05,
      "loss": 1.724,
      "step": 181300
    },
    {
      "epoch": 0.7292836115217965,
      "grad_norm": 3.2261040210723877,
      "learning_rate": 3.7845327930035876e-05,
      "loss": 1.6649,
      "step": 181400
    },
    {
      "epoch": 0.7296856421786443,
      "grad_norm": 2.696805953979492,
      "learning_rate": 3.7838627412353875e-05,
      "loss": 1.6755,
      "step": 181500
    },
    {
      "epoch": 0.730087672835492,
      "grad_norm": 2.996854066848755,
      "learning_rate": 3.783192689467188e-05,
      "loss": 1.6647,
      "step": 181600
    },
    {
      "epoch": 0.7304897034923398,
      "grad_norm": 2.7748148441314697,
      "learning_rate": 3.7825226376989885e-05,
      "loss": 1.6435,
      "step": 181700
    },
    {
      "epoch": 0.7308917341491875,
      "grad_norm": 3.623552083969116,
      "learning_rate": 3.781852585930789e-05,
      "loss": 1.6371,
      "step": 181800
    },
    {
      "epoch": 0.7312937648060353,
      "grad_norm": 3.349858045578003,
      "learning_rate": 3.7811825341625895e-05,
      "loss": 1.7157,
      "step": 181900
    },
    {
      "epoch": 0.731695795462883,
      "grad_norm": 2.840273141860962,
      "learning_rate": 3.78051248239439e-05,
      "loss": 1.6629,
      "step": 182000
    },
    {
      "epoch": 0.7320978261197307,
      "grad_norm": 3.467550039291382,
      "learning_rate": 3.7798424306261905e-05,
      "loss": 1.6881,
      "step": 182100
    },
    {
      "epoch": 0.7324998567765785,
      "grad_norm": 2.941981792449951,
      "learning_rate": 3.779172378857991e-05,
      "loss": 1.6498,
      "step": 182200
    },
    {
      "epoch": 0.7329018874334262,
      "grad_norm": 3.474472761154175,
      "learning_rate": 3.7785023270897916e-05,
      "loss": 1.637,
      "step": 182300
    },
    {
      "epoch": 0.733303918090274,
      "grad_norm": 3.460012435913086,
      "learning_rate": 3.7778322753215914e-05,
      "loss": 1.6355,
      "step": 182400
    },
    {
      "epoch": 0.7337059487471217,
      "grad_norm": 3.7883055210113525,
      "learning_rate": 3.777162223553392e-05,
      "loss": 1.6873,
      "step": 182500
    },
    {
      "epoch": 0.7341079794039694,
      "grad_norm": 2.570570468902588,
      "learning_rate": 3.7764921717851924e-05,
      "loss": 1.6617,
      "step": 182600
    },
    {
      "epoch": 0.7345100100608172,
      "grad_norm": 3.4083809852600098,
      "learning_rate": 3.775822120016992e-05,
      "loss": 1.7078,
      "step": 182700
    },
    {
      "epoch": 0.7349120407176649,
      "grad_norm": 3.725043296813965,
      "learning_rate": 3.775152068248793e-05,
      "loss": 1.6545,
      "step": 182800
    },
    {
      "epoch": 0.7353140713745127,
      "grad_norm": 4.743412971496582,
      "learning_rate": 3.774482016480593e-05,
      "loss": 1.6655,
      "step": 182900
    },
    {
      "epoch": 0.7357161020313604,
      "grad_norm": 3.479764938354492,
      "learning_rate": 3.773811964712394e-05,
      "loss": 1.7138,
      "step": 183000
    },
    {
      "epoch": 0.7361181326882081,
      "grad_norm": 3.621239185333252,
      "learning_rate": 3.773141912944194e-05,
      "loss": 1.6506,
      "step": 183100
    },
    {
      "epoch": 0.7365201633450559,
      "grad_norm": 3.441314220428467,
      "learning_rate": 3.772471861175995e-05,
      "loss": 1.6812,
      "step": 183200
    },
    {
      "epoch": 0.7369221940019036,
      "grad_norm": 3.4025442600250244,
      "learning_rate": 3.7718018094077953e-05,
      "loss": 1.678,
      "step": 183300
    },
    {
      "epoch": 0.7373242246587514,
      "grad_norm": 3.59450626373291,
      "learning_rate": 3.771131757639595e-05,
      "loss": 1.7027,
      "step": 183400
    },
    {
      "epoch": 0.7377262553155991,
      "grad_norm": 3.0691421031951904,
      "learning_rate": 3.770461705871396e-05,
      "loss": 1.6555,
      "step": 183500
    },
    {
      "epoch": 0.7381282859724468,
      "grad_norm": 3.138015031814575,
      "learning_rate": 3.769791654103196e-05,
      "loss": 1.6885,
      "step": 183600
    },
    {
      "epoch": 0.7385303166292946,
      "grad_norm": 3.4857337474823,
      "learning_rate": 3.769121602334997e-05,
      "loss": 1.6816,
      "step": 183700
    },
    {
      "epoch": 0.7389323472861423,
      "grad_norm": 3.2551839351654053,
      "learning_rate": 3.768451550566797e-05,
      "loss": 1.6444,
      "step": 183800
    },
    {
      "epoch": 0.7393343779429901,
      "grad_norm": 3.2651209831237793,
      "learning_rate": 3.767781498798597e-05,
      "loss": 1.6443,
      "step": 183900
    },
    {
      "epoch": 0.7397364085998378,
      "grad_norm": 3.479252338409424,
      "learning_rate": 3.7671114470303976e-05,
      "loss": 1.7006,
      "step": 184000
    },
    {
      "epoch": 0.7401384392566855,
      "grad_norm": 3.770761489868164,
      "learning_rate": 3.766441395262198e-05,
      "loss": 1.6692,
      "step": 184100
    },
    {
      "epoch": 0.7405404699135333,
      "grad_norm": 3.095853567123413,
      "learning_rate": 3.7657713434939986e-05,
      "loss": 1.6741,
      "step": 184200
    },
    {
      "epoch": 0.740942500570381,
      "grad_norm": 3.153748035430908,
      "learning_rate": 3.765101291725799e-05,
      "loss": 1.6482,
      "step": 184300
    },
    {
      "epoch": 0.7413445312272288,
      "grad_norm": 2.5554962158203125,
      "learning_rate": 3.764431239957599e-05,
      "loss": 1.6683,
      "step": 184400
    },
    {
      "epoch": 0.7417465618840765,
      "grad_norm": 2.9670372009277344,
      "learning_rate": 3.7637611881893995e-05,
      "loss": 1.6307,
      "step": 184500
    },
    {
      "epoch": 0.7421485925409242,
      "grad_norm": 3.2128496170043945,
      "learning_rate": 3.7630911364212e-05,
      "loss": 1.6641,
      "step": 184600
    },
    {
      "epoch": 0.742550623197772,
      "grad_norm": 3.3355813026428223,
      "learning_rate": 3.7624210846530005e-05,
      "loss": 1.6852,
      "step": 184700
    },
    {
      "epoch": 0.7429526538546197,
      "grad_norm": 3.5336692333221436,
      "learning_rate": 3.761751032884801e-05,
      "loss": 1.682,
      "step": 184800
    },
    {
      "epoch": 0.7433546845114675,
      "grad_norm": 3.4840950965881348,
      "learning_rate": 3.7610809811166015e-05,
      "loss": 1.6957,
      "step": 184900
    },
    {
      "epoch": 0.7437567151683152,
      "grad_norm": 3.06707501411438,
      "learning_rate": 3.760410929348402e-05,
      "loss": 1.6857,
      "step": 185000
    },
    {
      "epoch": 0.744158745825163,
      "grad_norm": 3.792717456817627,
      "learning_rate": 3.759740877580202e-05,
      "loss": 1.6407,
      "step": 185100
    },
    {
      "epoch": 0.7445607764820107,
      "grad_norm": 3.1811163425445557,
      "learning_rate": 3.7590708258120024e-05,
      "loss": 1.6765,
      "step": 185200
    },
    {
      "epoch": 0.7449628071388583,
      "grad_norm": 3.01662015914917,
      "learning_rate": 3.758400774043803e-05,
      "loss": 1.7323,
      "step": 185300
    },
    {
      "epoch": 0.7453648377957061,
      "grad_norm": 3.1465580463409424,
      "learning_rate": 3.7577307222756034e-05,
      "loss": 1.7066,
      "step": 185400
    },
    {
      "epoch": 0.7457668684525538,
      "grad_norm": 3.178363561630249,
      "learning_rate": 3.757060670507403e-05,
      "loss": 1.6878,
      "step": 185500
    },
    {
      "epoch": 0.7461688991094015,
      "grad_norm": 3.4900012016296387,
      "learning_rate": 3.756390618739204e-05,
      "loss": 1.6562,
      "step": 185600
    },
    {
      "epoch": 0.7465709297662493,
      "grad_norm": 3.3640475273132324,
      "learning_rate": 3.755720566971004e-05,
      "loss": 1.6953,
      "step": 185700
    },
    {
      "epoch": 0.746972960423097,
      "grad_norm": 3.7083017826080322,
      "learning_rate": 3.755050515202805e-05,
      "loss": 1.7165,
      "step": 185800
    },
    {
      "epoch": 0.7473749910799448,
      "grad_norm": 3.3001415729522705,
      "learning_rate": 3.754380463434605e-05,
      "loss": 1.6557,
      "step": 185900
    },
    {
      "epoch": 0.7477770217367925,
      "grad_norm": 3.7578554153442383,
      "learning_rate": 3.753710411666406e-05,
      "loss": 1.6646,
      "step": 186000
    },
    {
      "epoch": 0.7481790523936402,
      "grad_norm": 3.4642221927642822,
      "learning_rate": 3.753040359898206e-05,
      "loss": 1.7054,
      "step": 186100
    },
    {
      "epoch": 0.748581083050488,
      "grad_norm": 3.195803165435791,
      "learning_rate": 3.752370308130007e-05,
      "loss": 1.6927,
      "step": 186200
    },
    {
      "epoch": 0.7489831137073357,
      "grad_norm": 3.214517116546631,
      "learning_rate": 3.7517002563618067e-05,
      "loss": 1.6678,
      "step": 186300
    },
    {
      "epoch": 0.7493851443641835,
      "grad_norm": 4.009679794311523,
      "learning_rate": 3.751030204593607e-05,
      "loss": 1.6784,
      "step": 186400
    },
    {
      "epoch": 0.7497871750210312,
      "grad_norm": 2.5876898765563965,
      "learning_rate": 3.750360152825407e-05,
      "loss": 1.7088,
      "step": 186500
    },
    {
      "epoch": 0.750189205677879,
      "grad_norm": 3.2786693572998047,
      "learning_rate": 3.7496901010572075e-05,
      "loss": 1.62,
      "step": 186600
    },
    {
      "epoch": 0.7505912363347267,
      "grad_norm": 3.1082451343536377,
      "learning_rate": 3.749020049289008e-05,
      "loss": 1.63,
      "step": 186700
    },
    {
      "epoch": 0.7509932669915744,
      "grad_norm": 3.074690341949463,
      "learning_rate": 3.7483499975208085e-05,
      "loss": 1.6747,
      "step": 186800
    },
    {
      "epoch": 0.7513952976484222,
      "grad_norm": 3.0785927772521973,
      "learning_rate": 3.747679945752609e-05,
      "loss": 1.6535,
      "step": 186900
    },
    {
      "epoch": 0.7517973283052699,
      "grad_norm": 3.416872262954712,
      "learning_rate": 3.7470098939844096e-05,
      "loss": 1.7015,
      "step": 187000
    },
    {
      "epoch": 0.7521993589621176,
      "grad_norm": 3.2581803798675537,
      "learning_rate": 3.74633984221621e-05,
      "loss": 1.654,
      "step": 187100
    },
    {
      "epoch": 0.7526013896189654,
      "grad_norm": 3.2414066791534424,
      "learning_rate": 3.7456697904480106e-05,
      "loss": 1.6714,
      "step": 187200
    },
    {
      "epoch": 0.7530034202758131,
      "grad_norm": 3.2694737911224365,
      "learning_rate": 3.744999738679811e-05,
      "loss": 1.6758,
      "step": 187300
    },
    {
      "epoch": 0.7534054509326609,
      "grad_norm": 2.770564317703247,
      "learning_rate": 3.744329686911611e-05,
      "loss": 1.6684,
      "step": 187400
    },
    {
      "epoch": 0.7538074815895086,
      "grad_norm": 2.7736995220184326,
      "learning_rate": 3.743659635143411e-05,
      "loss": 1.7361,
      "step": 187500
    },
    {
      "epoch": 0.7542095122463564,
      "grad_norm": 3.875249147415161,
      "learning_rate": 3.742989583375211e-05,
      "loss": 1.6933,
      "step": 187600
    },
    {
      "epoch": 0.7546115429032041,
      "grad_norm": 3.7557902336120605,
      "learning_rate": 3.742319531607012e-05,
      "loss": 1.6414,
      "step": 187700
    },
    {
      "epoch": 0.7550135735600518,
      "grad_norm": 3.125437021255493,
      "learning_rate": 3.741649479838812e-05,
      "loss": 1.7202,
      "step": 187800
    },
    {
      "epoch": 0.7554156042168996,
      "grad_norm": 3.5333096981048584,
      "learning_rate": 3.740979428070613e-05,
      "loss": 1.6299,
      "step": 187900
    },
    {
      "epoch": 0.7558176348737473,
      "grad_norm": 3.0023858547210693,
      "learning_rate": 3.7403093763024133e-05,
      "loss": 1.6023,
      "step": 188000
    },
    {
      "epoch": 0.756219665530595,
      "grad_norm": 3.1287713050842285,
      "learning_rate": 3.739639324534214e-05,
      "loss": 1.6855,
      "step": 188100
    },
    {
      "epoch": 0.7566216961874428,
      "grad_norm": 3.099611282348633,
      "learning_rate": 3.7389692727660144e-05,
      "loss": 1.6474,
      "step": 188200
    },
    {
      "epoch": 0.7570237268442905,
      "grad_norm": 3.151970386505127,
      "learning_rate": 3.738299220997815e-05,
      "loss": 1.692,
      "step": 188300
    },
    {
      "epoch": 0.7574257575011383,
      "grad_norm": 2.8154826164245605,
      "learning_rate": 3.737629169229615e-05,
      "loss": 1.6342,
      "step": 188400
    },
    {
      "epoch": 0.757827788157986,
      "grad_norm": 3.365731716156006,
      "learning_rate": 3.736959117461415e-05,
      "loss": 1.6568,
      "step": 188500
    },
    {
      "epoch": 0.7582298188148338,
      "grad_norm": 2.803807497024536,
      "learning_rate": 3.736289065693216e-05,
      "loss": 1.669,
      "step": 188600
    },
    {
      "epoch": 0.7586318494716815,
      "grad_norm": 3.0154576301574707,
      "learning_rate": 3.7356190139250156e-05,
      "loss": 1.6736,
      "step": 188700
    },
    {
      "epoch": 0.7590338801285292,
      "grad_norm": 3.5383737087249756,
      "learning_rate": 3.734948962156816e-05,
      "loss": 1.6742,
      "step": 188800
    },
    {
      "epoch": 0.759435910785377,
      "grad_norm": 3.095233917236328,
      "learning_rate": 3.7342789103886166e-05,
      "loss": 1.6767,
      "step": 188900
    },
    {
      "epoch": 0.7598379414422247,
      "grad_norm": 3.2065272331237793,
      "learning_rate": 3.733608858620417e-05,
      "loss": 1.655,
      "step": 189000
    },
    {
      "epoch": 0.7602399720990725,
      "grad_norm": 3.476564407348633,
      "learning_rate": 3.7329388068522176e-05,
      "loss": 1.6242,
      "step": 189100
    },
    {
      "epoch": 0.7606420027559202,
      "grad_norm": 3.100297212600708,
      "learning_rate": 3.732268755084018e-05,
      "loss": 1.6369,
      "step": 189200
    },
    {
      "epoch": 0.7610440334127679,
      "grad_norm": 3.547546625137329,
      "learning_rate": 3.7315987033158186e-05,
      "loss": 1.6805,
      "step": 189300
    },
    {
      "epoch": 0.7614460640696157,
      "grad_norm": 3.2815651893615723,
      "learning_rate": 3.730928651547619e-05,
      "loss": 1.6377,
      "step": 189400
    },
    {
      "epoch": 0.7618480947264634,
      "grad_norm": 2.8253190517425537,
      "learning_rate": 3.730258599779419e-05,
      "loss": 1.6614,
      "step": 189500
    },
    {
      "epoch": 0.762250125383311,
      "grad_norm": 4.175668239593506,
      "learning_rate": 3.7295885480112195e-05,
      "loss": 1.7011,
      "step": 189600
    },
    {
      "epoch": 0.7626521560401588,
      "grad_norm": 3.449127674102783,
      "learning_rate": 3.72891849624302e-05,
      "loss": 1.6042,
      "step": 189700
    },
    {
      "epoch": 0.7630541866970065,
      "grad_norm": 3.266571521759033,
      "learning_rate": 3.7282484444748205e-05,
      "loss": 1.6857,
      "step": 189800
    },
    {
      "epoch": 0.7634562173538543,
      "grad_norm": 2.796799421310425,
      "learning_rate": 3.7275783927066204e-05,
      "loss": 1.7123,
      "step": 189900
    },
    {
      "epoch": 0.763858248010702,
      "grad_norm": 3.4195687770843506,
      "learning_rate": 3.726908340938421e-05,
      "loss": 1.6818,
      "step": 190000
    },
    {
      "epoch": 0.7642602786675498,
      "grad_norm": 3.1469664573669434,
      "learning_rate": 3.7262382891702214e-05,
      "loss": 1.668,
      "step": 190100
    },
    {
      "epoch": 0.7646623093243975,
      "grad_norm": 3.212528944015503,
      "learning_rate": 3.725568237402022e-05,
      "loss": 1.6731,
      "step": 190200
    },
    {
      "epoch": 0.7650643399812452,
      "grad_norm": 2.956028699874878,
      "learning_rate": 3.7248981856338224e-05,
      "loss": 1.6187,
      "step": 190300
    },
    {
      "epoch": 0.765466370638093,
      "grad_norm": 3.240145444869995,
      "learning_rate": 3.724228133865623e-05,
      "loss": 1.6824,
      "step": 190400
    },
    {
      "epoch": 0.7658684012949407,
      "grad_norm": 2.7008752822875977,
      "learning_rate": 3.723558082097423e-05,
      "loss": 1.6984,
      "step": 190500
    },
    {
      "epoch": 0.7662704319517885,
      "grad_norm": 3.4095845222473145,
      "learning_rate": 3.722888030329223e-05,
      "loss": 1.6473,
      "step": 190600
    },
    {
      "epoch": 0.7666724626086362,
      "grad_norm": 3.2949459552764893,
      "learning_rate": 3.722217978561024e-05,
      "loss": 1.677,
      "step": 190700
    },
    {
      "epoch": 0.7670744932654839,
      "grad_norm": 3.493662118911743,
      "learning_rate": 3.721547926792824e-05,
      "loss": 1.6378,
      "step": 190800
    },
    {
      "epoch": 0.7674765239223317,
      "grad_norm": 3.213277578353882,
      "learning_rate": 3.720877875024625e-05,
      "loss": 1.6585,
      "step": 190900
    },
    {
      "epoch": 0.7678785545791794,
      "grad_norm": 3.8841214179992676,
      "learning_rate": 3.7202078232564247e-05,
      "loss": 1.6965,
      "step": 191000
    },
    {
      "epoch": 0.7682805852360272,
      "grad_norm": 2.9445295333862305,
      "learning_rate": 3.719537771488225e-05,
      "loss": 1.6457,
      "step": 191100
    },
    {
      "epoch": 0.7686826158928749,
      "grad_norm": 3.240164279937744,
      "learning_rate": 3.718867719720026e-05,
      "loss": 1.6669,
      "step": 191200
    },
    {
      "epoch": 0.7690846465497226,
      "grad_norm": 2.8358237743377686,
      "learning_rate": 3.718197667951826e-05,
      "loss": 1.6442,
      "step": 191300
    },
    {
      "epoch": 0.7694866772065704,
      "grad_norm": 2.9989936351776123,
      "learning_rate": 3.717527616183627e-05,
      "loss": 1.7025,
      "step": 191400
    },
    {
      "epoch": 0.7698887078634181,
      "grad_norm": 3.814160108566284,
      "learning_rate": 3.7168575644154265e-05,
      "loss": 1.6657,
      "step": 191500
    },
    {
      "epoch": 0.7702907385202659,
      "grad_norm": 2.5060737133026123,
      "learning_rate": 3.716187512647227e-05,
      "loss": 1.6382,
      "step": 191600
    },
    {
      "epoch": 0.7706927691771136,
      "grad_norm": 4.0786638259887695,
      "learning_rate": 3.7155174608790276e-05,
      "loss": 1.6844,
      "step": 191700
    },
    {
      "epoch": 0.7710947998339613,
      "grad_norm": 2.7928669452667236,
      "learning_rate": 3.714847409110828e-05,
      "loss": 1.7029,
      "step": 191800
    },
    {
      "epoch": 0.7714968304908091,
      "grad_norm": 3.2675445079803467,
      "learning_rate": 3.7141773573426286e-05,
      "loss": 1.6439,
      "step": 191900
    },
    {
      "epoch": 0.7718988611476568,
      "grad_norm": 2.9879395961761475,
      "learning_rate": 3.713507305574429e-05,
      "loss": 1.7116,
      "step": 192000
    },
    {
      "epoch": 0.7723008918045046,
      "grad_norm": 3.372224807739258,
      "learning_rate": 3.7128372538062296e-05,
      "loss": 1.6394,
      "step": 192100
    },
    {
      "epoch": 0.7727029224613523,
      "grad_norm": 3.128023386001587,
      "learning_rate": 3.7121672020380294e-05,
      "loss": 1.6764,
      "step": 192200
    },
    {
      "epoch": 0.7731049531182,
      "grad_norm": 2.7757928371429443,
      "learning_rate": 3.71149715026983e-05,
      "loss": 1.6558,
      "step": 192300
    },
    {
      "epoch": 0.7735069837750478,
      "grad_norm": 3.068493127822876,
      "learning_rate": 3.7108270985016305e-05,
      "loss": 1.723,
      "step": 192400
    },
    {
      "epoch": 0.7739090144318955,
      "grad_norm": 3.104034185409546,
      "learning_rate": 3.710157046733431e-05,
      "loss": 1.7008,
      "step": 192500
    },
    {
      "epoch": 0.7743110450887433,
      "grad_norm": 4.18149471282959,
      "learning_rate": 3.709486994965231e-05,
      "loss": 1.6506,
      "step": 192600
    },
    {
      "epoch": 0.774713075745591,
      "grad_norm": 3.3317604064941406,
      "learning_rate": 3.708816943197031e-05,
      "loss": 1.6268,
      "step": 192700
    },
    {
      "epoch": 0.7751151064024387,
      "grad_norm": 3.2208497524261475,
      "learning_rate": 3.708146891428832e-05,
      "loss": 1.6351,
      "step": 192800
    },
    {
      "epoch": 0.7755171370592865,
      "grad_norm": 3.002600908279419,
      "learning_rate": 3.7074768396606324e-05,
      "loss": 1.6409,
      "step": 192900
    },
    {
      "epoch": 0.7759191677161342,
      "grad_norm": 2.6308212280273438,
      "learning_rate": 3.706806787892433e-05,
      "loss": 1.669,
      "step": 193000
    },
    {
      "epoch": 0.776321198372982,
      "grad_norm": 3.1612701416015625,
      "learning_rate": 3.7061367361242334e-05,
      "loss": 1.6691,
      "step": 193100
    },
    {
      "epoch": 0.7767232290298297,
      "grad_norm": 2.699456214904785,
      "learning_rate": 3.705466684356034e-05,
      "loss": 1.6704,
      "step": 193200
    },
    {
      "epoch": 0.7771252596866774,
      "grad_norm": 3.3871333599090576,
      "learning_rate": 3.7047966325878344e-05,
      "loss": 1.6469,
      "step": 193300
    },
    {
      "epoch": 0.7775272903435252,
      "grad_norm": 2.9389595985412598,
      "learning_rate": 3.704126580819634e-05,
      "loss": 1.6321,
      "step": 193400
    },
    {
      "epoch": 0.7779293210003729,
      "grad_norm": 2.7090821266174316,
      "learning_rate": 3.703456529051435e-05,
      "loss": 1.6221,
      "step": 193500
    },
    {
      "epoch": 0.7783313516572207,
      "grad_norm": 2.9649159908294678,
      "learning_rate": 3.7027864772832346e-05,
      "loss": 1.7106,
      "step": 193600
    },
    {
      "epoch": 0.7787333823140684,
      "grad_norm": 3.354698419570923,
      "learning_rate": 3.702116425515035e-05,
      "loss": 1.6651,
      "step": 193700
    },
    {
      "epoch": 0.7791354129709162,
      "grad_norm": 3.1038784980773926,
      "learning_rate": 3.7014463737468356e-05,
      "loss": 1.6314,
      "step": 193800
    },
    {
      "epoch": 0.7795374436277638,
      "grad_norm": 3.339552402496338,
      "learning_rate": 3.700776321978636e-05,
      "loss": 1.6461,
      "step": 193900
    },
    {
      "epoch": 0.7799394742846115,
      "grad_norm": 3.099658250808716,
      "learning_rate": 3.7001062702104366e-05,
      "loss": 1.6206,
      "step": 194000
    },
    {
      "epoch": 0.7803415049414593,
      "grad_norm": 3.2578186988830566,
      "learning_rate": 3.699436218442237e-05,
      "loss": 1.6533,
      "step": 194100
    },
    {
      "epoch": 0.780743535598307,
      "grad_norm": 3.540083408355713,
      "learning_rate": 3.698766166674038e-05,
      "loss": 1.6528,
      "step": 194200
    },
    {
      "epoch": 0.7811455662551547,
      "grad_norm": 3.0562353134155273,
      "learning_rate": 3.698096114905838e-05,
      "loss": 1.6236,
      "step": 194300
    },
    {
      "epoch": 0.7815475969120025,
      "grad_norm": 3.841200113296509,
      "learning_rate": 3.697426063137639e-05,
      "loss": 1.6958,
      "step": 194400
    },
    {
      "epoch": 0.7819496275688502,
      "grad_norm": 3.1334547996520996,
      "learning_rate": 3.6967560113694385e-05,
      "loss": 1.627,
      "step": 194500
    },
    {
      "epoch": 0.782351658225698,
      "grad_norm": 2.830721139907837,
      "learning_rate": 3.696085959601239e-05,
      "loss": 1.6477,
      "step": 194600
    },
    {
      "epoch": 0.7827536888825457,
      "grad_norm": 3.3809947967529297,
      "learning_rate": 3.695415907833039e-05,
      "loss": 1.6358,
      "step": 194700
    },
    {
      "epoch": 0.7831557195393934,
      "grad_norm": 3.5169460773468018,
      "learning_rate": 3.6947458560648394e-05,
      "loss": 1.6238,
      "step": 194800
    },
    {
      "epoch": 0.7835577501962412,
      "grad_norm": 3.0419704914093018,
      "learning_rate": 3.69407580429664e-05,
      "loss": 1.6802,
      "step": 194900
    },
    {
      "epoch": 0.7839597808530889,
      "grad_norm": 3.3275675773620605,
      "learning_rate": 3.6934057525284404e-05,
      "loss": 1.6599,
      "step": 195000
    },
    {
      "epoch": 0.7843618115099367,
      "grad_norm": 3.2964706420898438,
      "learning_rate": 3.692735700760241e-05,
      "loss": 1.687,
      "step": 195100
    },
    {
      "epoch": 0.7847638421667844,
      "grad_norm": 3.1882944107055664,
      "learning_rate": 3.6920656489920414e-05,
      "loss": 1.6723,
      "step": 195200
    },
    {
      "epoch": 0.7851658728236321,
      "grad_norm": 3.521672248840332,
      "learning_rate": 3.691395597223842e-05,
      "loss": 1.648,
      "step": 195300
    },
    {
      "epoch": 0.7855679034804799,
      "grad_norm": 3.1312601566314697,
      "learning_rate": 3.6907255454556425e-05,
      "loss": 1.6649,
      "step": 195400
    },
    {
      "epoch": 0.7859699341373276,
      "grad_norm": 3.829958438873291,
      "learning_rate": 3.690055493687442e-05,
      "loss": 1.6564,
      "step": 195500
    },
    {
      "epoch": 0.7863719647941754,
      "grad_norm": 2.533996343612671,
      "learning_rate": 3.689385441919243e-05,
      "loss": 1.6703,
      "step": 195600
    },
    {
      "epoch": 0.7867739954510231,
      "grad_norm": 2.9509689807891846,
      "learning_rate": 3.688715390151043e-05,
      "loss": 1.6542,
      "step": 195700
    },
    {
      "epoch": 0.7871760261078709,
      "grad_norm": 3.15016508102417,
      "learning_rate": 3.688045338382843e-05,
      "loss": 1.6396,
      "step": 195800
    },
    {
      "epoch": 0.7875780567647186,
      "grad_norm": 2.9696710109710693,
      "learning_rate": 3.687375286614644e-05,
      "loss": 1.6517,
      "step": 195900
    },
    {
      "epoch": 0.7879800874215663,
      "grad_norm": 2.9948318004608154,
      "learning_rate": 3.686705234846444e-05,
      "loss": 1.5841,
      "step": 196000
    },
    {
      "epoch": 0.7883821180784141,
      "grad_norm": 3.6804933547973633,
      "learning_rate": 3.686035183078245e-05,
      "loss": 1.6059,
      "step": 196100
    },
    {
      "epoch": 0.7887841487352618,
      "grad_norm": 3.629451274871826,
      "learning_rate": 3.685365131310045e-05,
      "loss": 1.6084,
      "step": 196200
    },
    {
      "epoch": 0.7891861793921096,
      "grad_norm": 4.025522708892822,
      "learning_rate": 3.684695079541846e-05,
      "loss": 1.6738,
      "step": 196300
    },
    {
      "epoch": 0.7895882100489573,
      "grad_norm": 3.2256381511688232,
      "learning_rate": 3.684025027773646e-05,
      "loss": 1.6348,
      "step": 196400
    },
    {
      "epoch": 0.789990240705805,
      "grad_norm": 4.225503921508789,
      "learning_rate": 3.683354976005447e-05,
      "loss": 1.7061,
      "step": 196500
    },
    {
      "epoch": 0.7903922713626528,
      "grad_norm": 3.382030725479126,
      "learning_rate": 3.6826849242372466e-05,
      "loss": 1.6831,
      "step": 196600
    },
    {
      "epoch": 0.7907943020195005,
      "grad_norm": 2.810885429382324,
      "learning_rate": 3.682014872469047e-05,
      "loss": 1.6723,
      "step": 196700
    },
    {
      "epoch": 0.7911963326763483,
      "grad_norm": 3.486063003540039,
      "learning_rate": 3.6813448207008476e-05,
      "loss": 1.6137,
      "step": 196800
    },
    {
      "epoch": 0.791598363333196,
      "grad_norm": 3.3071813583374023,
      "learning_rate": 3.680674768932648e-05,
      "loss": 1.681,
      "step": 196900
    },
    {
      "epoch": 0.7920003939900437,
      "grad_norm": 2.793774127960205,
      "learning_rate": 3.680004717164448e-05,
      "loss": 1.6474,
      "step": 197000
    },
    {
      "epoch": 0.7924024246468915,
      "grad_norm": 3.6452364921569824,
      "learning_rate": 3.6793346653962485e-05,
      "loss": 1.6712,
      "step": 197100
    },
    {
      "epoch": 0.7928044553037392,
      "grad_norm": 3.1670098304748535,
      "learning_rate": 3.678664613628049e-05,
      "loss": 1.6443,
      "step": 197200
    },
    {
      "epoch": 0.793206485960587,
      "grad_norm": 2.9511070251464844,
      "learning_rate": 3.6779945618598495e-05,
      "loss": 1.6648,
      "step": 197300
    },
    {
      "epoch": 0.7936085166174347,
      "grad_norm": 2.9327032566070557,
      "learning_rate": 3.67732451009165e-05,
      "loss": 1.6993,
      "step": 197400
    },
    {
      "epoch": 0.7940105472742824,
      "grad_norm": 3.5583009719848633,
      "learning_rate": 3.6766544583234505e-05,
      "loss": 1.6579,
      "step": 197500
    },
    {
      "epoch": 0.7944125779311302,
      "grad_norm": 3.993304491043091,
      "learning_rate": 3.6759844065552504e-05,
      "loss": 1.6499,
      "step": 197600
    },
    {
      "epoch": 0.7948146085879779,
      "grad_norm": 3.6010546684265137,
      "learning_rate": 3.675314354787051e-05,
      "loss": 1.6112,
      "step": 197700
    },
    {
      "epoch": 0.7952166392448257,
      "grad_norm": 3.5139262676239014,
      "learning_rate": 3.6746443030188514e-05,
      "loss": 1.6252,
      "step": 197800
    },
    {
      "epoch": 0.7956186699016734,
      "grad_norm": 3.0195441246032715,
      "learning_rate": 3.673974251250652e-05,
      "loss": 1.6007,
      "step": 197900
    },
    {
      "epoch": 0.7960207005585211,
      "grad_norm": 2.9165210723876953,
      "learning_rate": 3.6733041994824524e-05,
      "loss": 1.6415,
      "step": 198000
    },
    {
      "epoch": 0.7964227312153688,
      "grad_norm": 3.0022315979003906,
      "learning_rate": 3.672634147714253e-05,
      "loss": 1.6931,
      "step": 198100
    },
    {
      "epoch": 0.7968247618722165,
      "grad_norm": 3.2959601879119873,
      "learning_rate": 3.671964095946053e-05,
      "loss": 1.6659,
      "step": 198200
    },
    {
      "epoch": 0.7972267925290643,
      "grad_norm": 3.470548391342163,
      "learning_rate": 3.671294044177853e-05,
      "loss": 1.6584,
      "step": 198300
    },
    {
      "epoch": 0.797628823185912,
      "grad_norm": 2.9659364223480225,
      "learning_rate": 3.670623992409654e-05,
      "loss": 1.6575,
      "step": 198400
    },
    {
      "epoch": 0.7980308538427597,
      "grad_norm": 3.1364097595214844,
      "learning_rate": 3.669953940641454e-05,
      "loss": 1.6633,
      "step": 198500
    },
    {
      "epoch": 0.7984328844996075,
      "grad_norm": 3.366758346557617,
      "learning_rate": 3.669283888873255e-05,
      "loss": 1.6501,
      "step": 198600
    },
    {
      "epoch": 0.7988349151564552,
      "grad_norm": 4.091748237609863,
      "learning_rate": 3.6686138371050546e-05,
      "loss": 1.6422,
      "step": 198700
    },
    {
      "epoch": 0.799236945813303,
      "grad_norm": 3.2969424724578857,
      "learning_rate": 3.667943785336855e-05,
      "loss": 1.684,
      "step": 198800
    },
    {
      "epoch": 0.7996389764701507,
      "grad_norm": 3.7460553646087646,
      "learning_rate": 3.667273733568656e-05,
      "loss": 1.643,
      "step": 198900
    },
    {
      "epoch": 0.8000410071269984,
      "grad_norm": 2.852729558944702,
      "learning_rate": 3.666603681800456e-05,
      "loss": 1.6491,
      "step": 199000
    },
    {
      "epoch": 0.8004430377838462,
      "grad_norm": 3.2619709968566895,
      "learning_rate": 3.665933630032257e-05,
      "loss": 1.6424,
      "step": 199100
    },
    {
      "epoch": 0.8008450684406939,
      "grad_norm": 2.818281650543213,
      "learning_rate": 3.665263578264057e-05,
      "loss": 1.6763,
      "step": 199200
    },
    {
      "epoch": 0.8012470990975417,
      "grad_norm": 3.8374886512756348,
      "learning_rate": 3.664593526495858e-05,
      "loss": 1.6555,
      "step": 199300
    },
    {
      "epoch": 0.8016491297543894,
      "grad_norm": 2.808377504348755,
      "learning_rate": 3.6639234747276575e-05,
      "loss": 1.7248,
      "step": 199400
    },
    {
      "epoch": 0.8020511604112371,
      "grad_norm": 3.2209432125091553,
      "learning_rate": 3.663253422959458e-05,
      "loss": 1.6644,
      "step": 199500
    },
    {
      "epoch": 0.8024531910680849,
      "grad_norm": 2.713131904602051,
      "learning_rate": 3.6625833711912586e-05,
      "loss": 1.6835,
      "step": 199600
    },
    {
      "epoch": 0.8028552217249326,
      "grad_norm": 2.974637269973755,
      "learning_rate": 3.6619133194230584e-05,
      "loss": 1.7441,
      "step": 199700
    },
    {
      "epoch": 0.8032572523817804,
      "grad_norm": 2.444674253463745,
      "learning_rate": 3.661243267654859e-05,
      "loss": 1.6679,
      "step": 199800
    },
    {
      "epoch": 0.8036592830386281,
      "grad_norm": 2.5068631172180176,
      "learning_rate": 3.6605732158866594e-05,
      "loss": 1.6492,
      "step": 199900
    },
    {
      "epoch": 0.8040613136954758,
      "grad_norm": 3.617307662963867,
      "learning_rate": 3.65990316411846e-05,
      "loss": 1.6452,
      "step": 200000
    },
    {
      "epoch": 0.8044633443523236,
      "grad_norm": 3.793511152267456,
      "learning_rate": 3.6592331123502605e-05,
      "loss": 1.6569,
      "step": 200100
    },
    {
      "epoch": 0.8048653750091713,
      "grad_norm": 3.430344343185425,
      "learning_rate": 3.658563060582061e-05,
      "loss": 1.6749,
      "step": 200200
    },
    {
      "epoch": 0.8052674056660191,
      "grad_norm": 3.0293030738830566,
      "learning_rate": 3.6578930088138615e-05,
      "loss": 1.6428,
      "step": 200300
    },
    {
      "epoch": 0.8056694363228668,
      "grad_norm": 3.4228923320770264,
      "learning_rate": 3.657222957045662e-05,
      "loss": 1.6281,
      "step": 200400
    },
    {
      "epoch": 0.8060714669797145,
      "grad_norm": 2.7239701747894287,
      "learning_rate": 3.656552905277462e-05,
      "loss": 1.6478,
      "step": 200500
    },
    {
      "epoch": 0.8064734976365623,
      "grad_norm": 4.089287757873535,
      "learning_rate": 3.6558828535092623e-05,
      "loss": 1.6574,
      "step": 200600
    },
    {
      "epoch": 0.80687552829341,
      "grad_norm": 2.839244842529297,
      "learning_rate": 3.655212801741062e-05,
      "loss": 1.6669,
      "step": 200700
    },
    {
      "epoch": 0.8072775589502578,
      "grad_norm": 2.798637866973877,
      "learning_rate": 3.654542749972863e-05,
      "loss": 1.6197,
      "step": 200800
    },
    {
      "epoch": 0.8076795896071055,
      "grad_norm": 3.3314924240112305,
      "learning_rate": 3.653872698204663e-05,
      "loss": 1.6914,
      "step": 200900
    },
    {
      "epoch": 0.8080816202639532,
      "grad_norm": 3.8721327781677246,
      "learning_rate": 3.653202646436464e-05,
      "loss": 1.6321,
      "step": 201000
    },
    {
      "epoch": 0.808483650920801,
      "grad_norm": 3.313450336456299,
      "learning_rate": 3.652532594668264e-05,
      "loss": 1.5919,
      "step": 201100
    },
    {
      "epoch": 0.8088856815776487,
      "grad_norm": 3.30146861076355,
      "learning_rate": 3.651862542900065e-05,
      "loss": 1.6648,
      "step": 201200
    },
    {
      "epoch": 0.8092877122344965,
      "grad_norm": 2.6984145641326904,
      "learning_rate": 3.651192491131865e-05,
      "loss": 1.6839,
      "step": 201300
    },
    {
      "epoch": 0.8096897428913442,
      "grad_norm": 3.4663074016571045,
      "learning_rate": 3.650522439363666e-05,
      "loss": 1.6282,
      "step": 201400
    },
    {
      "epoch": 0.810091773548192,
      "grad_norm": 3.274704933166504,
      "learning_rate": 3.649852387595466e-05,
      "loss": 1.6348,
      "step": 201500
    },
    {
      "epoch": 0.8104938042050397,
      "grad_norm": 2.936730146408081,
      "learning_rate": 3.649182335827266e-05,
      "loss": 1.6303,
      "step": 201600
    },
    {
      "epoch": 0.8108958348618874,
      "grad_norm": 2.832211971282959,
      "learning_rate": 3.6485122840590666e-05,
      "loss": 1.6383,
      "step": 201700
    },
    {
      "epoch": 0.8112978655187352,
      "grad_norm": 3.413806200027466,
      "learning_rate": 3.6478422322908665e-05,
      "loss": 1.6641,
      "step": 201800
    },
    {
      "epoch": 0.8116998961755829,
      "grad_norm": 3.9638404846191406,
      "learning_rate": 3.647172180522667e-05,
      "loss": 1.6204,
      "step": 201900
    },
    {
      "epoch": 0.8121019268324307,
      "grad_norm": 3.2248244285583496,
      "learning_rate": 3.6465021287544675e-05,
      "loss": 1.6513,
      "step": 202000
    },
    {
      "epoch": 0.8125039574892784,
      "grad_norm": 2.9415669441223145,
      "learning_rate": 3.645832076986268e-05,
      "loss": 1.6692,
      "step": 202100
    },
    {
      "epoch": 0.8129059881461261,
      "grad_norm": 3.1949799060821533,
      "learning_rate": 3.6451620252180685e-05,
      "loss": 1.6369,
      "step": 202200
    },
    {
      "epoch": 0.8133080188029739,
      "grad_norm": 3.3314332962036133,
      "learning_rate": 3.644491973449869e-05,
      "loss": 1.6339,
      "step": 202300
    },
    {
      "epoch": 0.8137100494598215,
      "grad_norm": 2.924006223678589,
      "learning_rate": 3.6438219216816695e-05,
      "loss": 1.6726,
      "step": 202400
    },
    {
      "epoch": 0.8141120801166692,
      "grad_norm": 3.2566816806793213,
      "learning_rate": 3.64315186991347e-05,
      "loss": 1.6702,
      "step": 202500
    },
    {
      "epoch": 0.814514110773517,
      "grad_norm": 4.022268772125244,
      "learning_rate": 3.6424818181452706e-05,
      "loss": 1.6754,
      "step": 202600
    },
    {
      "epoch": 0.8149161414303647,
      "grad_norm": 2.8221094608306885,
      "learning_rate": 3.6418117663770704e-05,
      "loss": 1.6231,
      "step": 202700
    },
    {
      "epoch": 0.8153181720872125,
      "grad_norm": 3.1256978511810303,
      "learning_rate": 3.641141714608871e-05,
      "loss": 1.7128,
      "step": 202800
    },
    {
      "epoch": 0.8157202027440602,
      "grad_norm": 3.273746967315674,
      "learning_rate": 3.640471662840671e-05,
      "loss": 1.691,
      "step": 202900
    },
    {
      "epoch": 0.816122233400908,
      "grad_norm": 3.9970569610595703,
      "learning_rate": 3.639801611072471e-05,
      "loss": 1.6545,
      "step": 203000
    },
    {
      "epoch": 0.8165242640577557,
      "grad_norm": 3.405672311782837,
      "learning_rate": 3.639131559304272e-05,
      "loss": 1.6379,
      "step": 203100
    },
    {
      "epoch": 0.8169262947146034,
      "grad_norm": 2.824788808822632,
      "learning_rate": 3.638461507536072e-05,
      "loss": 1.6412,
      "step": 203200
    },
    {
      "epoch": 0.8173283253714512,
      "grad_norm": 3.1377806663513184,
      "learning_rate": 3.637791455767873e-05,
      "loss": 1.6724,
      "step": 203300
    },
    {
      "epoch": 0.8177303560282989,
      "grad_norm": 2.7009551525115967,
      "learning_rate": 3.637121403999673e-05,
      "loss": 1.6388,
      "step": 203400
    },
    {
      "epoch": 0.8181323866851467,
      "grad_norm": 2.7594897747039795,
      "learning_rate": 3.636451352231474e-05,
      "loss": 1.6288,
      "step": 203500
    },
    {
      "epoch": 0.8185344173419944,
      "grad_norm": 3.2325599193573,
      "learning_rate": 3.635781300463274e-05,
      "loss": 1.6184,
      "step": 203600
    },
    {
      "epoch": 0.8189364479988421,
      "grad_norm": 3.0932092666625977,
      "learning_rate": 3.635111248695074e-05,
      "loss": 1.6542,
      "step": 203700
    },
    {
      "epoch": 0.8193384786556899,
      "grad_norm": 3.055086851119995,
      "learning_rate": 3.634441196926875e-05,
      "loss": 1.6573,
      "step": 203800
    },
    {
      "epoch": 0.8197405093125376,
      "grad_norm": 3.4429690837860107,
      "learning_rate": 3.633771145158675e-05,
      "loss": 1.6419,
      "step": 203900
    },
    {
      "epoch": 0.8201425399693854,
      "grad_norm": 2.875318765640259,
      "learning_rate": 3.633101093390476e-05,
      "loss": 1.6653,
      "step": 204000
    },
    {
      "epoch": 0.8205445706262331,
      "grad_norm": 2.744915246963501,
      "learning_rate": 3.6324310416222755e-05,
      "loss": 1.6447,
      "step": 204100
    },
    {
      "epoch": 0.8209466012830808,
      "grad_norm": 3.206010580062866,
      "learning_rate": 3.631760989854076e-05,
      "loss": 1.6374,
      "step": 204200
    },
    {
      "epoch": 0.8213486319399286,
      "grad_norm": 3.0105793476104736,
      "learning_rate": 3.6310909380858766e-05,
      "loss": 1.6469,
      "step": 204300
    },
    {
      "epoch": 0.8217506625967763,
      "grad_norm": 3.522088050842285,
      "learning_rate": 3.630420886317677e-05,
      "loss": 1.6792,
      "step": 204400
    },
    {
      "epoch": 0.8221526932536241,
      "grad_norm": 3.3658998012542725,
      "learning_rate": 3.6297508345494776e-05,
      "loss": 1.6688,
      "step": 204500
    },
    {
      "epoch": 0.8225547239104718,
      "grad_norm": 3.0351572036743164,
      "learning_rate": 3.629080782781278e-05,
      "loss": 1.6462,
      "step": 204600
    },
    {
      "epoch": 0.8229567545673195,
      "grad_norm": 3.5214085578918457,
      "learning_rate": 3.628410731013078e-05,
      "loss": 1.6476,
      "step": 204700
    },
    {
      "epoch": 0.8233587852241673,
      "grad_norm": 3.7068421840667725,
      "learning_rate": 3.6277406792448785e-05,
      "loss": 1.6702,
      "step": 204800
    },
    {
      "epoch": 0.823760815881015,
      "grad_norm": 3.3192131519317627,
      "learning_rate": 3.627070627476679e-05,
      "loss": 1.6367,
      "step": 204900
    },
    {
      "epoch": 0.8241628465378628,
      "grad_norm": 3.0785391330718994,
      "learning_rate": 3.6264005757084795e-05,
      "loss": 1.6758,
      "step": 205000
    },
    {
      "epoch": 0.8245648771947105,
      "grad_norm": 3.616415023803711,
      "learning_rate": 3.62573052394028e-05,
      "loss": 1.6866,
      "step": 205100
    },
    {
      "epoch": 0.8249669078515582,
      "grad_norm": 3.4109046459198,
      "learning_rate": 3.6250604721720805e-05,
      "loss": 1.6446,
      "step": 205200
    },
    {
      "epoch": 0.825368938508406,
      "grad_norm": 3.428103446960449,
      "learning_rate": 3.6243904204038803e-05,
      "loss": 1.6712,
      "step": 205300
    },
    {
      "epoch": 0.8257709691652537,
      "grad_norm": 3.0077435970306396,
      "learning_rate": 3.623720368635681e-05,
      "loss": 1.6331,
      "step": 205400
    },
    {
      "epoch": 0.8261729998221015,
      "grad_norm": 3.1432554721832275,
      "learning_rate": 3.6230503168674814e-05,
      "loss": 1.6691,
      "step": 205500
    },
    {
      "epoch": 0.8265750304789492,
      "grad_norm": 3.6513519287109375,
      "learning_rate": 3.622380265099282e-05,
      "loss": 1.6255,
      "step": 205600
    },
    {
      "epoch": 0.8269770611357969,
      "grad_norm": 3.2144579887390137,
      "learning_rate": 3.6217102133310824e-05,
      "loss": 1.6087,
      "step": 205700
    },
    {
      "epoch": 0.8273790917926447,
      "grad_norm": 3.10162353515625,
      "learning_rate": 3.621040161562882e-05,
      "loss": 1.669,
      "step": 205800
    },
    {
      "epoch": 0.8277811224494924,
      "grad_norm": 3.8317744731903076,
      "learning_rate": 3.620370109794683e-05,
      "loss": 1.6647,
      "step": 205900
    },
    {
      "epoch": 0.8281831531063402,
      "grad_norm": 3.1778852939605713,
      "learning_rate": 3.619700058026483e-05,
      "loss": 1.6512,
      "step": 206000
    },
    {
      "epoch": 0.8285851837631879,
      "grad_norm": 3.078362464904785,
      "learning_rate": 3.619030006258284e-05,
      "loss": 1.6598,
      "step": 206100
    },
    {
      "epoch": 0.8289872144200356,
      "grad_norm": 2.7573211193084717,
      "learning_rate": 3.618359954490084e-05,
      "loss": 1.6582,
      "step": 206200
    },
    {
      "epoch": 0.8293892450768834,
      "grad_norm": 3.581397294998169,
      "learning_rate": 3.617689902721885e-05,
      "loss": 1.6697,
      "step": 206300
    },
    {
      "epoch": 0.8297912757337311,
      "grad_norm": 3.531769037246704,
      "learning_rate": 3.617019850953685e-05,
      "loss": 1.6187,
      "step": 206400
    },
    {
      "epoch": 0.8301933063905789,
      "grad_norm": 2.777103900909424,
      "learning_rate": 3.616349799185485e-05,
      "loss": 1.6501,
      "step": 206500
    },
    {
      "epoch": 0.8305953370474265,
      "grad_norm": 3.5825247764587402,
      "learning_rate": 3.6156797474172856e-05,
      "loss": 1.6046,
      "step": 206600
    },
    {
      "epoch": 0.8309973677042742,
      "grad_norm": 3.252641439437866,
      "learning_rate": 3.615009695649086e-05,
      "loss": 1.6629,
      "step": 206700
    },
    {
      "epoch": 0.831399398361122,
      "grad_norm": 3.338836431503296,
      "learning_rate": 3.614339643880886e-05,
      "loss": 1.6392,
      "step": 206800
    },
    {
      "epoch": 0.8318014290179697,
      "grad_norm": 2.7408647537231445,
      "learning_rate": 3.6136695921126865e-05,
      "loss": 1.6511,
      "step": 206900
    },
    {
      "epoch": 0.8322034596748175,
      "grad_norm": 4.089726448059082,
      "learning_rate": 3.612999540344487e-05,
      "loss": 1.6075,
      "step": 207000
    },
    {
      "epoch": 0.8326054903316652,
      "grad_norm": 3.8086838722229004,
      "learning_rate": 3.6123294885762875e-05,
      "loss": 1.7263,
      "step": 207100
    },
    {
      "epoch": 0.8330075209885129,
      "grad_norm": 2.967940092086792,
      "learning_rate": 3.611659436808088e-05,
      "loss": 1.6835,
      "step": 207200
    },
    {
      "epoch": 0.8334095516453607,
      "grad_norm": 3.2438058853149414,
      "learning_rate": 3.6109893850398886e-05,
      "loss": 1.6761,
      "step": 207300
    },
    {
      "epoch": 0.8338115823022084,
      "grad_norm": 3.2045323848724365,
      "learning_rate": 3.610319333271689e-05,
      "loss": 1.6716,
      "step": 207400
    },
    {
      "epoch": 0.8342136129590562,
      "grad_norm": 2.961550235748291,
      "learning_rate": 3.6096492815034896e-05,
      "loss": 1.6551,
      "step": 207500
    },
    {
      "epoch": 0.8346156436159039,
      "grad_norm": 2.780869960784912,
      "learning_rate": 3.60897922973529e-05,
      "loss": 1.6477,
      "step": 207600
    },
    {
      "epoch": 0.8350176742727516,
      "grad_norm": 3.5261762142181396,
      "learning_rate": 3.60830917796709e-05,
      "loss": 1.6285,
      "step": 207700
    },
    {
      "epoch": 0.8354197049295994,
      "grad_norm": 3.5355334281921387,
      "learning_rate": 3.6076391261988904e-05,
      "loss": 1.6265,
      "step": 207800
    },
    {
      "epoch": 0.8358217355864471,
      "grad_norm": 2.9967739582061768,
      "learning_rate": 3.60696907443069e-05,
      "loss": 1.6102,
      "step": 207900
    },
    {
      "epoch": 0.8362237662432949,
      "grad_norm": 3.988138437271118,
      "learning_rate": 3.606299022662491e-05,
      "loss": 1.6486,
      "step": 208000
    },
    {
      "epoch": 0.8366257969001426,
      "grad_norm": 3.1512646675109863,
      "learning_rate": 3.605628970894291e-05,
      "loss": 1.6734,
      "step": 208100
    },
    {
      "epoch": 0.8370278275569903,
      "grad_norm": 4.014041423797607,
      "learning_rate": 3.604958919126092e-05,
      "loss": 1.694,
      "step": 208200
    },
    {
      "epoch": 0.8374298582138381,
      "grad_norm": 3.064432144165039,
      "learning_rate": 3.604288867357892e-05,
      "loss": 1.5963,
      "step": 208300
    },
    {
      "epoch": 0.8378318888706858,
      "grad_norm": 3.557525396347046,
      "learning_rate": 3.603618815589693e-05,
      "loss": 1.6334,
      "step": 208400
    },
    {
      "epoch": 0.8382339195275336,
      "grad_norm": 3.1233911514282227,
      "learning_rate": 3.6029487638214934e-05,
      "loss": 1.6492,
      "step": 208500
    },
    {
      "epoch": 0.8386359501843813,
      "grad_norm": 3.5021471977233887,
      "learning_rate": 3.602278712053294e-05,
      "loss": 1.6506,
      "step": 208600
    },
    {
      "epoch": 0.839037980841229,
      "grad_norm": 3.2102739810943604,
      "learning_rate": 3.601608660285094e-05,
      "loss": 1.6508,
      "step": 208700
    },
    {
      "epoch": 0.8394400114980768,
      "grad_norm": 3.1852478981018066,
      "learning_rate": 3.600938608516894e-05,
      "loss": 1.6861,
      "step": 208800
    },
    {
      "epoch": 0.8398420421549245,
      "grad_norm": 2.6459267139434814,
      "learning_rate": 3.600268556748694e-05,
      "loss": 1.6613,
      "step": 208900
    },
    {
      "epoch": 0.8402440728117723,
      "grad_norm": 3.2220184803009033,
      "learning_rate": 3.5995985049804946e-05,
      "loss": 1.6418,
      "step": 209000
    },
    {
      "epoch": 0.84064610346862,
      "grad_norm": 2.4566023349761963,
      "learning_rate": 3.598928453212295e-05,
      "loss": 1.6617,
      "step": 209100
    },
    {
      "epoch": 0.8410481341254678,
      "grad_norm": 3.791508197784424,
      "learning_rate": 3.5982584014440956e-05,
      "loss": 1.6438,
      "step": 209200
    },
    {
      "epoch": 0.8414501647823155,
      "grad_norm": 2.9826619625091553,
      "learning_rate": 3.597588349675896e-05,
      "loss": 1.6129,
      "step": 209300
    },
    {
      "epoch": 0.8418521954391632,
      "grad_norm": 2.778651475906372,
      "learning_rate": 3.5969182979076966e-05,
      "loss": 1.6507,
      "step": 209400
    },
    {
      "epoch": 0.842254226096011,
      "grad_norm": 3.384442090988159,
      "learning_rate": 3.596248246139497e-05,
      "loss": 1.669,
      "step": 209500
    },
    {
      "epoch": 0.8426562567528587,
      "grad_norm": 2.985089063644409,
      "learning_rate": 3.5955781943712976e-05,
      "loss": 1.6301,
      "step": 209600
    },
    {
      "epoch": 0.8430582874097065,
      "grad_norm": 3.315100908279419,
      "learning_rate": 3.594908142603098e-05,
      "loss": 1.6558,
      "step": 209700
    },
    {
      "epoch": 0.8434603180665542,
      "grad_norm": 3.206592559814453,
      "learning_rate": 3.594238090834898e-05,
      "loss": 1.6945,
      "step": 209800
    },
    {
      "epoch": 0.8438623487234019,
      "grad_norm": 3.692126989364624,
      "learning_rate": 3.5935680390666985e-05,
      "loss": 1.6964,
      "step": 209900
    },
    {
      "epoch": 0.8442643793802497,
      "grad_norm": 2.886568307876587,
      "learning_rate": 3.592897987298498e-05,
      "loss": 1.6045,
      "step": 210000
    },
    {
      "epoch": 0.8446664100370974,
      "grad_norm": 2.796566963195801,
      "learning_rate": 3.592227935530299e-05,
      "loss": 1.625,
      "step": 210100
    },
    {
      "epoch": 0.8450684406939452,
      "grad_norm": 3.3591482639312744,
      "learning_rate": 3.5915578837620994e-05,
      "loss": 1.6404,
      "step": 210200
    },
    {
      "epoch": 0.8454704713507929,
      "grad_norm": 3.176685094833374,
      "learning_rate": 3.5908878319939e-05,
      "loss": 1.6775,
      "step": 210300
    },
    {
      "epoch": 0.8458725020076406,
      "grad_norm": 3.2997186183929443,
      "learning_rate": 3.5902177802257004e-05,
      "loss": 1.6616,
      "step": 210400
    },
    {
      "epoch": 0.8462745326644884,
      "grad_norm": 3.5214667320251465,
      "learning_rate": 3.589547728457501e-05,
      "loss": 1.6431,
      "step": 210500
    },
    {
      "epoch": 0.8466765633213361,
      "grad_norm": 3.1584274768829346,
      "learning_rate": 3.5888776766893014e-05,
      "loss": 1.7162,
      "step": 210600
    },
    {
      "epoch": 0.8470785939781839,
      "grad_norm": 3.086230754852295,
      "learning_rate": 3.588207624921102e-05,
      "loss": 1.611,
      "step": 210700
    },
    {
      "epoch": 0.8474806246350316,
      "grad_norm": 3.197091817855835,
      "learning_rate": 3.587537573152902e-05,
      "loss": 1.6366,
      "step": 210800
    },
    {
      "epoch": 0.8478826552918792,
      "grad_norm": 3.9121036529541016,
      "learning_rate": 3.586867521384702e-05,
      "loss": 1.6883,
      "step": 210900
    },
    {
      "epoch": 0.848284685948727,
      "grad_norm": 3.3511037826538086,
      "learning_rate": 3.586197469616503e-05,
      "loss": 1.6701,
      "step": 211000
    },
    {
      "epoch": 0.8486867166055747,
      "grad_norm": 2.9896020889282227,
      "learning_rate": 3.585527417848303e-05,
      "loss": 1.6358,
      "step": 211100
    },
    {
      "epoch": 0.8490887472624224,
      "grad_norm": 2.7601139545440674,
      "learning_rate": 3.584857366080103e-05,
      "loss": 1.6703,
      "step": 211200
    },
    {
      "epoch": 0.8494907779192702,
      "grad_norm": 3.712678909301758,
      "learning_rate": 3.5841873143119036e-05,
      "loss": 1.6312,
      "step": 211300
    },
    {
      "epoch": 0.8498928085761179,
      "grad_norm": 3.3005306720733643,
      "learning_rate": 3.583517262543704e-05,
      "loss": 1.626,
      "step": 211400
    },
    {
      "epoch": 0.8502948392329657,
      "grad_norm": 3.2928946018218994,
      "learning_rate": 3.582847210775505e-05,
      "loss": 1.6613,
      "step": 211500
    },
    {
      "epoch": 0.8506968698898134,
      "grad_norm": 3.728175401687622,
      "learning_rate": 3.582177159007305e-05,
      "loss": 1.6517,
      "step": 211600
    },
    {
      "epoch": 0.8510989005466612,
      "grad_norm": 3.108713388442993,
      "learning_rate": 3.581507107239106e-05,
      "loss": 1.6962,
      "step": 211700
    },
    {
      "epoch": 0.8515009312035089,
      "grad_norm": 2.8815886974334717,
      "learning_rate": 3.5808370554709055e-05,
      "loss": 1.6282,
      "step": 211800
    },
    {
      "epoch": 0.8519029618603566,
      "grad_norm": 3.5715677738189697,
      "learning_rate": 3.580167003702706e-05,
      "loss": 1.699,
      "step": 211900
    },
    {
      "epoch": 0.8523049925172044,
      "grad_norm": 3.2010726928710938,
      "learning_rate": 3.5794969519345066e-05,
      "loss": 1.6575,
      "step": 212000
    },
    {
      "epoch": 0.8527070231740521,
      "grad_norm": 3.4035274982452393,
      "learning_rate": 3.578826900166307e-05,
      "loss": 1.6796,
      "step": 212100
    },
    {
      "epoch": 0.8531090538308999,
      "grad_norm": 4.231422424316406,
      "learning_rate": 3.5781568483981076e-05,
      "loss": 1.6238,
      "step": 212200
    },
    {
      "epoch": 0.8535110844877476,
      "grad_norm": 3.83282732963562,
      "learning_rate": 3.577486796629908e-05,
      "loss": 1.6312,
      "step": 212300
    },
    {
      "epoch": 0.8539131151445953,
      "grad_norm": 2.9880266189575195,
      "learning_rate": 3.576816744861708e-05,
      "loss": 1.6418,
      "step": 212400
    },
    {
      "epoch": 0.8543151458014431,
      "grad_norm": 3.1380810737609863,
      "learning_rate": 3.5761466930935084e-05,
      "loss": 1.6246,
      "step": 212500
    },
    {
      "epoch": 0.8547171764582908,
      "grad_norm": 2.945470094680786,
      "learning_rate": 3.575476641325309e-05,
      "loss": 1.6189,
      "step": 212600
    },
    {
      "epoch": 0.8551192071151386,
      "grad_norm": 3.0018298625946045,
      "learning_rate": 3.5748065895571095e-05,
      "loss": 1.6963,
      "step": 212700
    },
    {
      "epoch": 0.8555212377719863,
      "grad_norm": 3.109222173690796,
      "learning_rate": 3.57413653778891e-05,
      "loss": 1.6502,
      "step": 212800
    },
    {
      "epoch": 0.855923268428834,
      "grad_norm": 3.427633762359619,
      "learning_rate": 3.57346648602071e-05,
      "loss": 1.6369,
      "step": 212900
    },
    {
      "epoch": 0.8563252990856818,
      "grad_norm": 2.881465196609497,
      "learning_rate": 3.57279643425251e-05,
      "loss": 1.68,
      "step": 213000
    },
    {
      "epoch": 0.8567273297425295,
      "grad_norm": 4.062740325927734,
      "learning_rate": 3.572126382484311e-05,
      "loss": 1.6115,
      "step": 213100
    },
    {
      "epoch": 0.8571293603993773,
      "grad_norm": 3.0185256004333496,
      "learning_rate": 3.5714563307161113e-05,
      "loss": 1.6134,
      "step": 213200
    },
    {
      "epoch": 0.857531391056225,
      "grad_norm": 4.228710651397705,
      "learning_rate": 3.570786278947912e-05,
      "loss": 1.6541,
      "step": 213300
    },
    {
      "epoch": 0.8579334217130727,
      "grad_norm": 3.196134328842163,
      "learning_rate": 3.5701162271797124e-05,
      "loss": 1.6783,
      "step": 213400
    },
    {
      "epoch": 0.8583354523699205,
      "grad_norm": 3.2375240325927734,
      "learning_rate": 3.569446175411513e-05,
      "loss": 1.6103,
      "step": 213500
    },
    {
      "epoch": 0.8587374830267682,
      "grad_norm": 3.34067440032959,
      "learning_rate": 3.568776123643313e-05,
      "loss": 1.5926,
      "step": 213600
    },
    {
      "epoch": 0.859139513683616,
      "grad_norm": 3.007687568664551,
      "learning_rate": 3.568106071875113e-05,
      "loss": 1.6497,
      "step": 213700
    },
    {
      "epoch": 0.8595415443404637,
      "grad_norm": 3.4032320976257324,
      "learning_rate": 3.567436020106914e-05,
      "loss": 1.6515,
      "step": 213800
    },
    {
      "epoch": 0.8599435749973114,
      "grad_norm": 3.1981866359710693,
      "learning_rate": 3.5667659683387136e-05,
      "loss": 1.6392,
      "step": 213900
    },
    {
      "epoch": 0.8603456056541592,
      "grad_norm": 3.40860652923584,
      "learning_rate": 3.566095916570514e-05,
      "loss": 1.6139,
      "step": 214000
    },
    {
      "epoch": 0.8607476363110069,
      "grad_norm": 2.910940647125244,
      "learning_rate": 3.5654258648023146e-05,
      "loss": 1.691,
      "step": 214100
    },
    {
      "epoch": 0.8611496669678547,
      "grad_norm": 3.233448028564453,
      "learning_rate": 3.564755813034115e-05,
      "loss": 1.6113,
      "step": 214200
    },
    {
      "epoch": 0.8615516976247024,
      "grad_norm": 3.7222838401794434,
      "learning_rate": 3.5640857612659156e-05,
      "loss": 1.6988,
      "step": 214300
    },
    {
      "epoch": 0.8619537282815501,
      "grad_norm": 3.1755313873291016,
      "learning_rate": 3.563415709497716e-05,
      "loss": 1.6523,
      "step": 214400
    },
    {
      "epoch": 0.8623557589383979,
      "grad_norm": 3.1519601345062256,
      "learning_rate": 3.5627456577295167e-05,
      "loss": 1.6618,
      "step": 214500
    },
    {
      "epoch": 0.8627577895952456,
      "grad_norm": 2.5469818115234375,
      "learning_rate": 3.562075605961317e-05,
      "loss": 1.6608,
      "step": 214600
    },
    {
      "epoch": 0.8631598202520934,
      "grad_norm": 4.431888580322266,
      "learning_rate": 3.561405554193118e-05,
      "loss": 1.595,
      "step": 214700
    },
    {
      "epoch": 0.8635618509089411,
      "grad_norm": 3.1890668869018555,
      "learning_rate": 3.5607355024249175e-05,
      "loss": 1.6547,
      "step": 214800
    },
    {
      "epoch": 0.8639638815657888,
      "grad_norm": 2.6438677310943604,
      "learning_rate": 3.560065450656718e-05,
      "loss": 1.6256,
      "step": 214900
    },
    {
      "epoch": 0.8643659122226366,
      "grad_norm": 3.736703872680664,
      "learning_rate": 3.559395398888518e-05,
      "loss": 1.6769,
      "step": 215000
    },
    {
      "epoch": 0.8647679428794843,
      "grad_norm": 3.5090601444244385,
      "learning_rate": 3.5587253471203184e-05,
      "loss": 1.6643,
      "step": 215100
    },
    {
      "epoch": 0.865169973536332,
      "grad_norm": 3.4799692630767822,
      "learning_rate": 3.558055295352119e-05,
      "loss": 1.67,
      "step": 215200
    },
    {
      "epoch": 0.8655720041931797,
      "grad_norm": 2.660891532897949,
      "learning_rate": 3.5573852435839194e-05,
      "loss": 1.6205,
      "step": 215300
    },
    {
      "epoch": 0.8659740348500274,
      "grad_norm": 3.093712568283081,
      "learning_rate": 3.55671519181572e-05,
      "loss": 1.6588,
      "step": 215400
    },
    {
      "epoch": 0.8663760655068752,
      "grad_norm": 3.5667896270751953,
      "learning_rate": 3.5560451400475204e-05,
      "loss": 1.6265,
      "step": 215500
    },
    {
      "epoch": 0.8667780961637229,
      "grad_norm": 3.4328010082244873,
      "learning_rate": 3.555375088279321e-05,
      "loss": 1.6266,
      "step": 215600
    },
    {
      "epoch": 0.8671801268205707,
      "grad_norm": 3.719567060470581,
      "learning_rate": 3.5547050365111215e-05,
      "loss": 1.6249,
      "step": 215700
    },
    {
      "epoch": 0.8675821574774184,
      "grad_norm": 2.898599863052368,
      "learning_rate": 3.554034984742921e-05,
      "loss": 1.61,
      "step": 215800
    },
    {
      "epoch": 0.8679841881342661,
      "grad_norm": 2.9882450103759766,
      "learning_rate": 3.553364932974722e-05,
      "loss": 1.6104,
      "step": 215900
    },
    {
      "epoch": 0.8683862187911139,
      "grad_norm": 3.019331932067871,
      "learning_rate": 3.5526948812065216e-05,
      "loss": 1.6559,
      "step": 216000
    },
    {
      "epoch": 0.8687882494479616,
      "grad_norm": 3.638704776763916,
      "learning_rate": 3.552024829438322e-05,
      "loss": 1.6605,
      "step": 216100
    },
    {
      "epoch": 0.8691902801048094,
      "grad_norm": 2.70078706741333,
      "learning_rate": 3.551354777670123e-05,
      "loss": 1.6256,
      "step": 216200
    },
    {
      "epoch": 0.8695923107616571,
      "grad_norm": 2.589712381362915,
      "learning_rate": 3.550684725901923e-05,
      "loss": 1.6918,
      "step": 216300
    },
    {
      "epoch": 0.8699943414185048,
      "grad_norm": 2.5446038246154785,
      "learning_rate": 3.550014674133724e-05,
      "loss": 1.6524,
      "step": 216400
    },
    {
      "epoch": 0.8703963720753526,
      "grad_norm": 3.136714458465576,
      "learning_rate": 3.549344622365524e-05,
      "loss": 1.6116,
      "step": 216500
    },
    {
      "epoch": 0.8707984027322003,
      "grad_norm": 2.962559700012207,
      "learning_rate": 3.548674570597325e-05,
      "loss": 1.6508,
      "step": 216600
    },
    {
      "epoch": 0.8712004333890481,
      "grad_norm": 3.5296597480773926,
      "learning_rate": 3.548004518829125e-05,
      "loss": 1.6577,
      "step": 216700
    },
    {
      "epoch": 0.8716024640458958,
      "grad_norm": 4.087000370025635,
      "learning_rate": 3.547334467060926e-05,
      "loss": 1.6485,
      "step": 216800
    },
    {
      "epoch": 0.8720044947027435,
      "grad_norm": 2.5014634132385254,
      "learning_rate": 3.5466644152927256e-05,
      "loss": 1.677,
      "step": 216900
    },
    {
      "epoch": 0.8724065253595913,
      "grad_norm": 2.9186906814575195,
      "learning_rate": 3.545994363524526e-05,
      "loss": 1.6391,
      "step": 217000
    },
    {
      "epoch": 0.872808556016439,
      "grad_norm": 2.694207191467285,
      "learning_rate": 3.5453243117563266e-05,
      "loss": 1.6152,
      "step": 217100
    },
    {
      "epoch": 0.8732105866732868,
      "grad_norm": 3.0562665462493896,
      "learning_rate": 3.5446542599881264e-05,
      "loss": 1.6779,
      "step": 217200
    },
    {
      "epoch": 0.8736126173301345,
      "grad_norm": 3.233863353729248,
      "learning_rate": 3.543984208219927e-05,
      "loss": 1.6465,
      "step": 217300
    },
    {
      "epoch": 0.8740146479869823,
      "grad_norm": 3.095531463623047,
      "learning_rate": 3.5433141564517275e-05,
      "loss": 1.6662,
      "step": 217400
    },
    {
      "epoch": 0.87441667864383,
      "grad_norm": 3.5233399868011475,
      "learning_rate": 3.542644104683528e-05,
      "loss": 1.6221,
      "step": 217500
    },
    {
      "epoch": 0.8748187093006777,
      "grad_norm": 2.9315907955169678,
      "learning_rate": 3.5419740529153285e-05,
      "loss": 1.6048,
      "step": 217600
    },
    {
      "epoch": 0.8752207399575255,
      "grad_norm": 2.777737855911255,
      "learning_rate": 3.541304001147129e-05,
      "loss": 1.659,
      "step": 217700
    },
    {
      "epoch": 0.8756227706143732,
      "grad_norm": 3.1069858074188232,
      "learning_rate": 3.5406339493789295e-05,
      "loss": 1.6347,
      "step": 217800
    },
    {
      "epoch": 0.876024801271221,
      "grad_norm": 3.252838134765625,
      "learning_rate": 3.5399638976107293e-05,
      "loss": 1.6054,
      "step": 217900
    },
    {
      "epoch": 0.8764268319280687,
      "grad_norm": 3.399170160293579,
      "learning_rate": 3.53929384584253e-05,
      "loss": 1.633,
      "step": 218000
    },
    {
      "epoch": 0.8768288625849164,
      "grad_norm": 3.0244393348693848,
      "learning_rate": 3.5386237940743304e-05,
      "loss": 1.6615,
      "step": 218100
    },
    {
      "epoch": 0.8772308932417642,
      "grad_norm": 3.260594129562378,
      "learning_rate": 3.537953742306131e-05,
      "loss": 1.5874,
      "step": 218200
    },
    {
      "epoch": 0.8776329238986119,
      "grad_norm": 4.15632963180542,
      "learning_rate": 3.5372836905379314e-05,
      "loss": 1.6274,
      "step": 218300
    },
    {
      "epoch": 0.8780349545554597,
      "grad_norm": 3.581845283508301,
      "learning_rate": 3.536613638769731e-05,
      "loss": 1.6597,
      "step": 218400
    },
    {
      "epoch": 0.8784369852123074,
      "grad_norm": 3.089881420135498,
      "learning_rate": 3.535943587001532e-05,
      "loss": 1.6292,
      "step": 218500
    },
    {
      "epoch": 0.8788390158691551,
      "grad_norm": 2.909468412399292,
      "learning_rate": 3.535273535233332e-05,
      "loss": 1.6688,
      "step": 218600
    },
    {
      "epoch": 0.8792410465260029,
      "grad_norm": 2.861994504928589,
      "learning_rate": 3.534603483465133e-05,
      "loss": 1.6642,
      "step": 218700
    },
    {
      "epoch": 0.8796430771828506,
      "grad_norm": 3.6997625827789307,
      "learning_rate": 3.533933431696933e-05,
      "loss": 1.668,
      "step": 218800
    },
    {
      "epoch": 0.8800451078396984,
      "grad_norm": 3.8361618518829346,
      "learning_rate": 3.533263379928734e-05,
      "loss": 1.646,
      "step": 218900
    },
    {
      "epoch": 0.8804471384965461,
      "grad_norm": 2.8857696056365967,
      "learning_rate": 3.5325933281605336e-05,
      "loss": 1.6951,
      "step": 219000
    },
    {
      "epoch": 0.8808491691533938,
      "grad_norm": 4.132707595825195,
      "learning_rate": 3.531923276392334e-05,
      "loss": 1.6412,
      "step": 219100
    },
    {
      "epoch": 0.8812511998102416,
      "grad_norm": 3.2514607906341553,
      "learning_rate": 3.5312532246241347e-05,
      "loss": 1.6618,
      "step": 219200
    },
    {
      "epoch": 0.8816532304670893,
      "grad_norm": 3.1924638748168945,
      "learning_rate": 3.530583172855935e-05,
      "loss": 1.6124,
      "step": 219300
    },
    {
      "epoch": 0.882055261123937,
      "grad_norm": 3.257488965988159,
      "learning_rate": 3.529913121087736e-05,
      "loss": 1.6783,
      "step": 219400
    },
    {
      "epoch": 0.8824572917807847,
      "grad_norm": 2.893873453140259,
      "learning_rate": 3.5292430693195355e-05,
      "loss": 1.6516,
      "step": 219500
    },
    {
      "epoch": 0.8828593224376324,
      "grad_norm": 3.841283082962036,
      "learning_rate": 3.528573017551336e-05,
      "loss": 1.6695,
      "step": 219600
    },
    {
      "epoch": 0.8832613530944802,
      "grad_norm": 2.7200942039489746,
      "learning_rate": 3.5279029657831365e-05,
      "loss": 1.6225,
      "step": 219700
    },
    {
      "epoch": 0.8836633837513279,
      "grad_norm": 2.6642465591430664,
      "learning_rate": 3.527232914014937e-05,
      "loss": 1.6421,
      "step": 219800
    },
    {
      "epoch": 0.8840654144081757,
      "grad_norm": 3.5252134799957275,
      "learning_rate": 3.5265628622467376e-05,
      "loss": 1.6399,
      "step": 219900
    },
    {
      "epoch": 0.8844674450650234,
      "grad_norm": 3.6431519985198975,
      "learning_rate": 3.5258928104785374e-05,
      "loss": 1.6915,
      "step": 220000
    },
    {
      "epoch": 0.8848694757218711,
      "grad_norm": 3.120699644088745,
      "learning_rate": 3.525222758710338e-05,
      "loss": 1.6313,
      "step": 220100
    },
    {
      "epoch": 0.8852715063787189,
      "grad_norm": 3.4827332496643066,
      "learning_rate": 3.5245527069421384e-05,
      "loss": 1.6947,
      "step": 220200
    },
    {
      "epoch": 0.8856735370355666,
      "grad_norm": 3.020845651626587,
      "learning_rate": 3.523882655173939e-05,
      "loss": 1.6348,
      "step": 220300
    },
    {
      "epoch": 0.8860755676924144,
      "grad_norm": 3.1713387966156006,
      "learning_rate": 3.5232126034057394e-05,
      "loss": 1.6669,
      "step": 220400
    },
    {
      "epoch": 0.8864775983492621,
      "grad_norm": 3.1944103240966797,
      "learning_rate": 3.52254255163754e-05,
      "loss": 1.6484,
      "step": 220500
    },
    {
      "epoch": 0.8868796290061098,
      "grad_norm": 3.811840057373047,
      "learning_rate": 3.5218724998693405e-05,
      "loss": 1.641,
      "step": 220600
    },
    {
      "epoch": 0.8872816596629576,
      "grad_norm": 3.7672219276428223,
      "learning_rate": 3.52120244810114e-05,
      "loss": 1.6325,
      "step": 220700
    },
    {
      "epoch": 0.8876836903198053,
      "grad_norm": 2.801664352416992,
      "learning_rate": 3.520532396332941e-05,
      "loss": 1.638,
      "step": 220800
    },
    {
      "epoch": 0.8880857209766531,
      "grad_norm": 3.384338855743408,
      "learning_rate": 3.519862344564741e-05,
      "loss": 1.6258,
      "step": 220900
    },
    {
      "epoch": 0.8884877516335008,
      "grad_norm": 3.175393581390381,
      "learning_rate": 3.519192292796541e-05,
      "loss": 1.6715,
      "step": 221000
    },
    {
      "epoch": 0.8888897822903485,
      "grad_norm": 3.622133255004883,
      "learning_rate": 3.518522241028342e-05,
      "loss": 1.6548,
      "step": 221100
    },
    {
      "epoch": 0.8892918129471963,
      "grad_norm": 2.9423165321350098,
      "learning_rate": 3.517852189260142e-05,
      "loss": 1.6581,
      "step": 221200
    },
    {
      "epoch": 0.889693843604044,
      "grad_norm": 3.269568681716919,
      "learning_rate": 3.517182137491943e-05,
      "loss": 1.6349,
      "step": 221300
    },
    {
      "epoch": 0.8900958742608918,
      "grad_norm": 3.7415037155151367,
      "learning_rate": 3.516512085723743e-05,
      "loss": 1.6324,
      "step": 221400
    },
    {
      "epoch": 0.8904979049177395,
      "grad_norm": 3.321580648422241,
      "learning_rate": 3.515842033955544e-05,
      "loss": 1.6472,
      "step": 221500
    },
    {
      "epoch": 0.8908999355745872,
      "grad_norm": 3.3832132816314697,
      "learning_rate": 3.515171982187344e-05,
      "loss": 1.7034,
      "step": 221600
    },
    {
      "epoch": 0.891301966231435,
      "grad_norm": 3.344109296798706,
      "learning_rate": 3.514501930419145e-05,
      "loss": 1.654,
      "step": 221700
    },
    {
      "epoch": 0.8917039968882827,
      "grad_norm": 2.6122617721557617,
      "learning_rate": 3.513831878650945e-05,
      "loss": 1.6353,
      "step": 221800
    },
    {
      "epoch": 0.8921060275451305,
      "grad_norm": 2.939650774002075,
      "learning_rate": 3.513161826882745e-05,
      "loss": 1.6775,
      "step": 221900
    },
    {
      "epoch": 0.8925080582019782,
      "grad_norm": 4.048311233520508,
      "learning_rate": 3.5124917751145456e-05,
      "loss": 1.6421,
      "step": 222000
    },
    {
      "epoch": 0.892910088858826,
      "grad_norm": 2.8655619621276855,
      "learning_rate": 3.5118217233463455e-05,
      "loss": 1.6405,
      "step": 222100
    },
    {
      "epoch": 0.8933121195156737,
      "grad_norm": 3.0965144634246826,
      "learning_rate": 3.511151671578146e-05,
      "loss": 1.6257,
      "step": 222200
    },
    {
      "epoch": 0.8937141501725214,
      "grad_norm": 3.0515177249908447,
      "learning_rate": 3.5104816198099465e-05,
      "loss": 1.6215,
      "step": 222300
    },
    {
      "epoch": 0.8941161808293692,
      "grad_norm": 2.6593706607818604,
      "learning_rate": 3.509811568041747e-05,
      "loss": 1.6283,
      "step": 222400
    },
    {
      "epoch": 0.8945182114862169,
      "grad_norm": 3.216867685317993,
      "learning_rate": 3.5091415162735475e-05,
      "loss": 1.6221,
      "step": 222500
    },
    {
      "epoch": 0.8949202421430646,
      "grad_norm": 3.0718016624450684,
      "learning_rate": 3.508471464505348e-05,
      "loss": 1.5914,
      "step": 222600
    },
    {
      "epoch": 0.8953222727999124,
      "grad_norm": 3.2939741611480713,
      "learning_rate": 3.5078014127371485e-05,
      "loss": 1.6498,
      "step": 222700
    },
    {
      "epoch": 0.8957243034567601,
      "grad_norm": 4.151980400085449,
      "learning_rate": 3.507131360968949e-05,
      "loss": 1.7052,
      "step": 222800
    },
    {
      "epoch": 0.8961263341136079,
      "grad_norm": 3.844602108001709,
      "learning_rate": 3.5064613092007496e-05,
      "loss": 1.6846,
      "step": 222900
    },
    {
      "epoch": 0.8965283647704556,
      "grad_norm": 3.6578426361083984,
      "learning_rate": 3.5057912574325494e-05,
      "loss": 1.6355,
      "step": 223000
    },
    {
      "epoch": 0.8969303954273034,
      "grad_norm": 3.687871217727661,
      "learning_rate": 3.505121205664349e-05,
      "loss": 1.669,
      "step": 223100
    },
    {
      "epoch": 0.8973324260841511,
      "grad_norm": 2.830406904220581,
      "learning_rate": 3.50445115389615e-05,
      "loss": 1.6901,
      "step": 223200
    },
    {
      "epoch": 0.8977344567409988,
      "grad_norm": 3.67562198638916,
      "learning_rate": 3.50378110212795e-05,
      "loss": 1.7085,
      "step": 223300
    },
    {
      "epoch": 0.8981364873978466,
      "grad_norm": 3.6656410694122314,
      "learning_rate": 3.503111050359751e-05,
      "loss": 1.6416,
      "step": 223400
    },
    {
      "epoch": 0.8985385180546943,
      "grad_norm": 2.70615816116333,
      "learning_rate": 3.502440998591551e-05,
      "loss": 1.6014,
      "step": 223500
    },
    {
      "epoch": 0.898940548711542,
      "grad_norm": 3.429711103439331,
      "learning_rate": 3.501770946823352e-05,
      "loss": 1.662,
      "step": 223600
    },
    {
      "epoch": 0.8993425793683897,
      "grad_norm": 3.8792169094085693,
      "learning_rate": 3.501100895055152e-05,
      "loss": 1.5976,
      "step": 223700
    },
    {
      "epoch": 0.8997446100252374,
      "grad_norm": 3.0051963329315186,
      "learning_rate": 3.500430843286953e-05,
      "loss": 1.665,
      "step": 223800
    },
    {
      "epoch": 0.9001466406820852,
      "grad_norm": 3.5505194664001465,
      "learning_rate": 3.499760791518753e-05,
      "loss": 1.6284,
      "step": 223900
    },
    {
      "epoch": 0.9005486713389329,
      "grad_norm": 3.243640422821045,
      "learning_rate": 3.499090739750553e-05,
      "loss": 1.6268,
      "step": 224000
    },
    {
      "epoch": 0.9009507019957806,
      "grad_norm": 3.484578847885132,
      "learning_rate": 3.498420687982354e-05,
      "loss": 1.6556,
      "step": 224100
    },
    {
      "epoch": 0.9013527326526284,
      "grad_norm": 2.8392107486724854,
      "learning_rate": 3.497750636214154e-05,
      "loss": 1.647,
      "step": 224200
    },
    {
      "epoch": 0.9017547633094761,
      "grad_norm": 3.204383134841919,
      "learning_rate": 3.497080584445954e-05,
      "loss": 1.6044,
      "step": 224300
    },
    {
      "epoch": 0.9021567939663239,
      "grad_norm": 2.450437545776367,
      "learning_rate": 3.4964105326777545e-05,
      "loss": 1.7006,
      "step": 224400
    },
    {
      "epoch": 0.9025588246231716,
      "grad_norm": 3.967501163482666,
      "learning_rate": 3.495740480909555e-05,
      "loss": 1.694,
      "step": 224500
    },
    {
      "epoch": 0.9029608552800193,
      "grad_norm": 3.7773890495300293,
      "learning_rate": 3.4950704291413556e-05,
      "loss": 1.6314,
      "step": 224600
    },
    {
      "epoch": 0.9033628859368671,
      "grad_norm": 2.9100077152252197,
      "learning_rate": 3.494400377373156e-05,
      "loss": 1.5893,
      "step": 224700
    },
    {
      "epoch": 0.9037649165937148,
      "grad_norm": 4.126307487487793,
      "learning_rate": 3.4937303256049566e-05,
      "loss": 1.6306,
      "step": 224800
    },
    {
      "epoch": 0.9041669472505626,
      "grad_norm": 3.2306039333343506,
      "learning_rate": 3.493060273836757e-05,
      "loss": 1.6396,
      "step": 224900
    },
    {
      "epoch": 0.9045689779074103,
      "grad_norm": 3.4105896949768066,
      "learning_rate": 3.492390222068557e-05,
      "loss": 1.6445,
      "step": 225000
    },
    {
      "epoch": 0.904971008564258,
      "grad_norm": 3.4854960441589355,
      "learning_rate": 3.4917201703003574e-05,
      "loss": 1.6285,
      "step": 225100
    },
    {
      "epoch": 0.9053730392211058,
      "grad_norm": 2.8008222579956055,
      "learning_rate": 3.491050118532158e-05,
      "loss": 1.647,
      "step": 225200
    },
    {
      "epoch": 0.9057750698779535,
      "grad_norm": 3.6914873123168945,
      "learning_rate": 3.4903800667639585e-05,
      "loss": 1.664,
      "step": 225300
    },
    {
      "epoch": 0.9061771005348013,
      "grad_norm": 3.177691698074341,
      "learning_rate": 3.489710014995759e-05,
      "loss": 1.6493,
      "step": 225400
    },
    {
      "epoch": 0.906579131191649,
      "grad_norm": 3.3799638748168945,
      "learning_rate": 3.489039963227559e-05,
      "loss": 1.6648,
      "step": 225500
    },
    {
      "epoch": 0.9069811618484968,
      "grad_norm": 3.3390204906463623,
      "learning_rate": 3.488369911459359e-05,
      "loss": 1.6195,
      "step": 225600
    },
    {
      "epoch": 0.9073831925053445,
      "grad_norm": 3.325512647628784,
      "learning_rate": 3.48769985969116e-05,
      "loss": 1.6763,
      "step": 225700
    },
    {
      "epoch": 0.9077852231621922,
      "grad_norm": 3.361422538757324,
      "learning_rate": 3.4870298079229604e-05,
      "loss": 1.6349,
      "step": 225800
    },
    {
      "epoch": 0.90818725381904,
      "grad_norm": 3.4639174938201904,
      "learning_rate": 3.486359756154761e-05,
      "loss": 1.6552,
      "step": 225900
    },
    {
      "epoch": 0.9085892844758877,
      "grad_norm": 3.2246463298797607,
      "learning_rate": 3.4856897043865614e-05,
      "loss": 1.6367,
      "step": 226000
    },
    {
      "epoch": 0.9089913151327355,
      "grad_norm": 2.895799398422241,
      "learning_rate": 3.485019652618361e-05,
      "loss": 1.6449,
      "step": 226100
    },
    {
      "epoch": 0.9093933457895832,
      "grad_norm": 3.121501922607422,
      "learning_rate": 3.484349600850162e-05,
      "loss": 1.6729,
      "step": 226200
    },
    {
      "epoch": 0.9097953764464309,
      "grad_norm": 3.366725206375122,
      "learning_rate": 3.483679549081962e-05,
      "loss": 1.6694,
      "step": 226300
    },
    {
      "epoch": 0.9101974071032787,
      "grad_norm": 2.8333466053009033,
      "learning_rate": 3.483009497313763e-05,
      "loss": 1.6794,
      "step": 226400
    },
    {
      "epoch": 0.9105994377601264,
      "grad_norm": 3.3987131118774414,
      "learning_rate": 3.482339445545563e-05,
      "loss": 1.6571,
      "step": 226500
    },
    {
      "epoch": 0.9110014684169742,
      "grad_norm": 3.5602569580078125,
      "learning_rate": 3.481669393777364e-05,
      "loss": 1.6395,
      "step": 226600
    },
    {
      "epoch": 0.9114034990738219,
      "grad_norm": 3.243527412414551,
      "learning_rate": 3.4809993420091636e-05,
      "loss": 1.6024,
      "step": 226700
    },
    {
      "epoch": 0.9118055297306696,
      "grad_norm": 2.7891650199890137,
      "learning_rate": 3.480329290240964e-05,
      "loss": 1.6197,
      "step": 226800
    },
    {
      "epoch": 0.9122075603875174,
      "grad_norm": 2.6490530967712402,
      "learning_rate": 3.4796592384727646e-05,
      "loss": 1.6592,
      "step": 226900
    },
    {
      "epoch": 0.9126095910443651,
      "grad_norm": 2.9393558502197266,
      "learning_rate": 3.478989186704565e-05,
      "loss": 1.6197,
      "step": 227000
    },
    {
      "epoch": 0.9130116217012129,
      "grad_norm": 3.8181419372558594,
      "learning_rate": 3.478319134936365e-05,
      "loss": 1.6533,
      "step": 227100
    },
    {
      "epoch": 0.9134136523580606,
      "grad_norm": 3.8382632732391357,
      "learning_rate": 3.4776490831681655e-05,
      "loss": 1.6584,
      "step": 227200
    },
    {
      "epoch": 0.9138156830149083,
      "grad_norm": 2.967804193496704,
      "learning_rate": 3.476979031399966e-05,
      "loss": 1.6442,
      "step": 227300
    },
    {
      "epoch": 0.9142177136717561,
      "grad_norm": 3.297062397003174,
      "learning_rate": 3.4763089796317665e-05,
      "loss": 1.5821,
      "step": 227400
    },
    {
      "epoch": 0.9146197443286038,
      "grad_norm": 3.0920140743255615,
      "learning_rate": 3.475638927863567e-05,
      "loss": 1.6609,
      "step": 227500
    },
    {
      "epoch": 0.9150217749854516,
      "grad_norm": 4.5767364501953125,
      "learning_rate": 3.4749688760953675e-05,
      "loss": 1.6595,
      "step": 227600
    },
    {
      "epoch": 0.9154238056422993,
      "grad_norm": 3.5514776706695557,
      "learning_rate": 3.474298824327168e-05,
      "loss": 1.6395,
      "step": 227700
    },
    {
      "epoch": 0.915825836299147,
      "grad_norm": 3.352097272872925,
      "learning_rate": 3.4736287725589686e-05,
      "loss": 1.6602,
      "step": 227800
    },
    {
      "epoch": 0.9162278669559947,
      "grad_norm": 3.9155898094177246,
      "learning_rate": 3.4729587207907684e-05,
      "loss": 1.5806,
      "step": 227900
    },
    {
      "epoch": 0.9166298976128424,
      "grad_norm": 3.5795414447784424,
      "learning_rate": 3.472288669022569e-05,
      "loss": 1.6432,
      "step": 228000
    },
    {
      "epoch": 0.9170319282696902,
      "grad_norm": 3.370266914367676,
      "learning_rate": 3.4716186172543694e-05,
      "loss": 1.6408,
      "step": 228100
    },
    {
      "epoch": 0.9174339589265379,
      "grad_norm": 2.725811719894409,
      "learning_rate": 3.470948565486169e-05,
      "loss": 1.6186,
      "step": 228200
    },
    {
      "epoch": 0.9178359895833856,
      "grad_norm": 3.565727472305298,
      "learning_rate": 3.47027851371797e-05,
      "loss": 1.5978,
      "step": 228300
    },
    {
      "epoch": 0.9182380202402334,
      "grad_norm": 3.6230876445770264,
      "learning_rate": 3.46960846194977e-05,
      "loss": 1.6677,
      "step": 228400
    },
    {
      "epoch": 0.9186400508970811,
      "grad_norm": 3.2045536041259766,
      "learning_rate": 3.468938410181571e-05,
      "loss": 1.6286,
      "step": 228500
    },
    {
      "epoch": 0.9190420815539289,
      "grad_norm": 2.7537693977355957,
      "learning_rate": 3.468268358413371e-05,
      "loss": 1.6553,
      "step": 228600
    },
    {
      "epoch": 0.9194441122107766,
      "grad_norm": 3.149611234664917,
      "learning_rate": 3.467598306645172e-05,
      "loss": 1.6598,
      "step": 228700
    },
    {
      "epoch": 0.9198461428676243,
      "grad_norm": 3.4248123168945312,
      "learning_rate": 3.4669282548769723e-05,
      "loss": 1.6441,
      "step": 228800
    },
    {
      "epoch": 0.9202481735244721,
      "grad_norm": 3.9170727729797363,
      "learning_rate": 3.466258203108773e-05,
      "loss": 1.6409,
      "step": 228900
    },
    {
      "epoch": 0.9206502041813198,
      "grad_norm": 3.7523627281188965,
      "learning_rate": 3.465588151340573e-05,
      "loss": 1.6071,
      "step": 229000
    },
    {
      "epoch": 0.9210522348381676,
      "grad_norm": 3.06654691696167,
      "learning_rate": 3.464918099572373e-05,
      "loss": 1.5749,
      "step": 229100
    },
    {
      "epoch": 0.9214542654950153,
      "grad_norm": 3.1468098163604736,
      "learning_rate": 3.464248047804173e-05,
      "loss": 1.6798,
      "step": 229200
    },
    {
      "epoch": 0.921856296151863,
      "grad_norm": 3.4218289852142334,
      "learning_rate": 3.4635779960359736e-05,
      "loss": 1.6097,
      "step": 229300
    },
    {
      "epoch": 0.9222583268087108,
      "grad_norm": 4.111168384552002,
      "learning_rate": 3.462907944267774e-05,
      "loss": 1.6242,
      "step": 229400
    },
    {
      "epoch": 0.9226603574655585,
      "grad_norm": 5.323233604431152,
      "learning_rate": 3.4622378924995746e-05,
      "loss": 1.5814,
      "step": 229500
    },
    {
      "epoch": 0.9230623881224063,
      "grad_norm": 3.047567367553711,
      "learning_rate": 3.461567840731375e-05,
      "loss": 1.648,
      "step": 229600
    },
    {
      "epoch": 0.923464418779254,
      "grad_norm": 3.572214126586914,
      "learning_rate": 3.4608977889631756e-05,
      "loss": 1.6631,
      "step": 229700
    },
    {
      "epoch": 0.9238664494361017,
      "grad_norm": 3.382897138595581,
      "learning_rate": 3.460227737194976e-05,
      "loss": 1.6829,
      "step": 229800
    },
    {
      "epoch": 0.9242684800929495,
      "grad_norm": 3.737278938293457,
      "learning_rate": 3.4595576854267766e-05,
      "loss": 1.6364,
      "step": 229900
    },
    {
      "epoch": 0.9246705107497972,
      "grad_norm": 2.989128351211548,
      "learning_rate": 3.458887633658577e-05,
      "loss": 1.6205,
      "step": 230000
    },
    {
      "epoch": 0.925072541406645,
      "grad_norm": 3.126523494720459,
      "learning_rate": 3.458217581890377e-05,
      "loss": 1.6296,
      "step": 230100
    },
    {
      "epoch": 0.9254745720634927,
      "grad_norm": 2.64277982711792,
      "learning_rate": 3.457547530122177e-05,
      "loss": 1.6078,
      "step": 230200
    },
    {
      "epoch": 0.9258766027203404,
      "grad_norm": 2.7656679153442383,
      "learning_rate": 3.456877478353977e-05,
      "loss": 1.5733,
      "step": 230300
    },
    {
      "epoch": 0.9262786333771882,
      "grad_norm": 2.8052165508270264,
      "learning_rate": 3.456207426585778e-05,
      "loss": 1.6645,
      "step": 230400
    },
    {
      "epoch": 0.9266806640340359,
      "grad_norm": 3.26863694190979,
      "learning_rate": 3.4555373748175783e-05,
      "loss": 1.6228,
      "step": 230500
    },
    {
      "epoch": 0.9270826946908837,
      "grad_norm": 3.755274772644043,
      "learning_rate": 3.454867323049379e-05,
      "loss": 1.6677,
      "step": 230600
    },
    {
      "epoch": 0.9274847253477314,
      "grad_norm": 3.347085475921631,
      "learning_rate": 3.4541972712811794e-05,
      "loss": 1.6069,
      "step": 230700
    },
    {
      "epoch": 0.9278867560045791,
      "grad_norm": 3.2548911571502686,
      "learning_rate": 3.45352721951298e-05,
      "loss": 1.5925,
      "step": 230800
    },
    {
      "epoch": 0.9282887866614269,
      "grad_norm": 3.8919692039489746,
      "learning_rate": 3.4528571677447804e-05,
      "loss": 1.645,
      "step": 230900
    },
    {
      "epoch": 0.9286908173182746,
      "grad_norm": 3.484051465988159,
      "learning_rate": 3.452187115976581e-05,
      "loss": 1.7126,
      "step": 231000
    },
    {
      "epoch": 0.9290928479751224,
      "grad_norm": 3.673246145248413,
      "learning_rate": 3.451517064208381e-05,
      "loss": 1.6336,
      "step": 231100
    },
    {
      "epoch": 0.9294948786319701,
      "grad_norm": 2.968600034713745,
      "learning_rate": 3.450847012440181e-05,
      "loss": 1.671,
      "step": 231200
    },
    {
      "epoch": 0.9298969092888179,
      "grad_norm": 3.176311731338501,
      "learning_rate": 3.450176960671982e-05,
      "loss": 1.6054,
      "step": 231300
    },
    {
      "epoch": 0.9302989399456656,
      "grad_norm": 3.134370803833008,
      "learning_rate": 3.4495069089037816e-05,
      "loss": 1.6434,
      "step": 231400
    },
    {
      "epoch": 0.9307009706025133,
      "grad_norm": 3.2878384590148926,
      "learning_rate": 3.448836857135582e-05,
      "loss": 1.6019,
      "step": 231500
    },
    {
      "epoch": 0.9311030012593611,
      "grad_norm": 3.191356897354126,
      "learning_rate": 3.4481668053673826e-05,
      "loss": 1.6213,
      "step": 231600
    },
    {
      "epoch": 0.9315050319162088,
      "grad_norm": 3.373626470565796,
      "learning_rate": 3.447496753599183e-05,
      "loss": 1.6213,
      "step": 231700
    },
    {
      "epoch": 0.9319070625730566,
      "grad_norm": 3.181920051574707,
      "learning_rate": 3.4468267018309837e-05,
      "loss": 1.6418,
      "step": 231800
    },
    {
      "epoch": 0.9323090932299043,
      "grad_norm": 3.8142173290252686,
      "learning_rate": 3.446156650062784e-05,
      "loss": 1.6693,
      "step": 231900
    },
    {
      "epoch": 0.932711123886752,
      "grad_norm": 3.8769733905792236,
      "learning_rate": 3.445486598294585e-05,
      "loss": 1.633,
      "step": 232000
    },
    {
      "epoch": 0.9331131545435998,
      "grad_norm": 3.17848539352417,
      "learning_rate": 3.444816546526385e-05,
      "loss": 1.6261,
      "step": 232100
    },
    {
      "epoch": 0.9335151852004474,
      "grad_norm": 3.4872186183929443,
      "learning_rate": 3.444146494758185e-05,
      "loss": 1.6098,
      "step": 232200
    },
    {
      "epoch": 0.9339172158572951,
      "grad_norm": 3.01135516166687,
      "learning_rate": 3.4434764429899855e-05,
      "loss": 1.645,
      "step": 232300
    },
    {
      "epoch": 0.9343192465141429,
      "grad_norm": 3.4717819690704346,
      "learning_rate": 3.442806391221786e-05,
      "loss": 1.6282,
      "step": 232400
    },
    {
      "epoch": 0.9347212771709906,
      "grad_norm": 2.7456395626068115,
      "learning_rate": 3.4421363394535866e-05,
      "loss": 1.5986,
      "step": 232500
    },
    {
      "epoch": 0.9351233078278384,
      "grad_norm": 3.3695144653320312,
      "learning_rate": 3.4414662876853864e-05,
      "loss": 1.579,
      "step": 232600
    },
    {
      "epoch": 0.9355253384846861,
      "grad_norm": 3.6608316898345947,
      "learning_rate": 3.440796235917187e-05,
      "loss": 1.6377,
      "step": 232700
    },
    {
      "epoch": 0.9359273691415338,
      "grad_norm": 3.8800153732299805,
      "learning_rate": 3.4401261841489874e-05,
      "loss": 1.6301,
      "step": 232800
    },
    {
      "epoch": 0.9363293997983816,
      "grad_norm": 3.689417600631714,
      "learning_rate": 3.439456132380788e-05,
      "loss": 1.6625,
      "step": 232900
    },
    {
      "epoch": 0.9367314304552293,
      "grad_norm": 4.093885898590088,
      "learning_rate": 3.4387860806125885e-05,
      "loss": 1.5896,
      "step": 233000
    },
    {
      "epoch": 0.9371334611120771,
      "grad_norm": 4.415507793426514,
      "learning_rate": 3.438116028844389e-05,
      "loss": 1.633,
      "step": 233100
    },
    {
      "epoch": 0.9375354917689248,
      "grad_norm": 3.4660491943359375,
      "learning_rate": 3.437445977076189e-05,
      "loss": 1.6477,
      "step": 233200
    },
    {
      "epoch": 0.9379375224257726,
      "grad_norm": 3.533698558807373,
      "learning_rate": 3.436775925307989e-05,
      "loss": 1.6243,
      "step": 233300
    },
    {
      "epoch": 0.9383395530826203,
      "grad_norm": 3.574036121368408,
      "learning_rate": 3.43610587353979e-05,
      "loss": 1.6435,
      "step": 233400
    },
    {
      "epoch": 0.938741583739468,
      "grad_norm": 3.066524028778076,
      "learning_rate": 3.4354358217715903e-05,
      "loss": 1.6603,
      "step": 233500
    },
    {
      "epoch": 0.9391436143963158,
      "grad_norm": 3.625509262084961,
      "learning_rate": 3.434765770003391e-05,
      "loss": 1.5663,
      "step": 233600
    },
    {
      "epoch": 0.9395456450531635,
      "grad_norm": 3.517953634262085,
      "learning_rate": 3.4340957182351914e-05,
      "loss": 1.6462,
      "step": 233700
    },
    {
      "epoch": 0.9399476757100113,
      "grad_norm": 2.8238351345062256,
      "learning_rate": 3.433425666466991e-05,
      "loss": 1.6,
      "step": 233800
    },
    {
      "epoch": 0.940349706366859,
      "grad_norm": 2.963111162185669,
      "learning_rate": 3.432755614698792e-05,
      "loss": 1.653,
      "step": 233900
    },
    {
      "epoch": 0.9407517370237067,
      "grad_norm": 3.7800133228302,
      "learning_rate": 3.432085562930592e-05,
      "loss": 1.5626,
      "step": 234000
    },
    {
      "epoch": 0.9411537676805545,
      "grad_norm": 3.6727659702301025,
      "learning_rate": 3.431415511162393e-05,
      "loss": 1.6808,
      "step": 234100
    },
    {
      "epoch": 0.9415557983374022,
      "grad_norm": 2.8039798736572266,
      "learning_rate": 3.4307454593941926e-05,
      "loss": 1.6305,
      "step": 234200
    },
    {
      "epoch": 0.94195782899425,
      "grad_norm": 3.2641303539276123,
      "learning_rate": 3.430075407625993e-05,
      "loss": 1.6413,
      "step": 234300
    },
    {
      "epoch": 0.9423598596510977,
      "grad_norm": 3.0741024017333984,
      "learning_rate": 3.4294053558577936e-05,
      "loss": 1.6214,
      "step": 234400
    },
    {
      "epoch": 0.9427618903079454,
      "grad_norm": 3.1099696159362793,
      "learning_rate": 3.428735304089594e-05,
      "loss": 1.6144,
      "step": 234500
    },
    {
      "epoch": 0.9431639209647932,
      "grad_norm": 3.0808377265930176,
      "learning_rate": 3.4280652523213946e-05,
      "loss": 1.6445,
      "step": 234600
    },
    {
      "epoch": 0.9435659516216409,
      "grad_norm": 3.4796738624572754,
      "learning_rate": 3.427395200553195e-05,
      "loss": 1.5967,
      "step": 234700
    },
    {
      "epoch": 0.9439679822784887,
      "grad_norm": 2.8945889472961426,
      "learning_rate": 3.4267251487849956e-05,
      "loss": 1.6378,
      "step": 234800
    },
    {
      "epoch": 0.9443700129353364,
      "grad_norm": 2.791748523712158,
      "learning_rate": 3.426055097016796e-05,
      "loss": 1.6016,
      "step": 234900
    },
    {
      "epoch": 0.9447720435921841,
      "grad_norm": 3.1816446781158447,
      "learning_rate": 3.425385045248596e-05,
      "loss": 1.6052,
      "step": 235000
    },
    {
      "epoch": 0.9451740742490319,
      "grad_norm": 3.6764914989471436,
      "learning_rate": 3.4247149934803965e-05,
      "loss": 1.6239,
      "step": 235100
    },
    {
      "epoch": 0.9455761049058796,
      "grad_norm": 3.226325511932373,
      "learning_rate": 3.424044941712197e-05,
      "loss": 1.6027,
      "step": 235200
    },
    {
      "epoch": 0.9459781355627274,
      "grad_norm": 2.9476771354675293,
      "learning_rate": 3.423374889943997e-05,
      "loss": 1.6494,
      "step": 235300
    },
    {
      "epoch": 0.9463801662195751,
      "grad_norm": 3.1277778148651123,
      "learning_rate": 3.4227048381757974e-05,
      "loss": 1.6407,
      "step": 235400
    },
    {
      "epoch": 0.9467821968764228,
      "grad_norm": 3.148576021194458,
      "learning_rate": 3.422034786407598e-05,
      "loss": 1.6233,
      "step": 235500
    },
    {
      "epoch": 0.9471842275332706,
      "grad_norm": 3.644434928894043,
      "learning_rate": 3.4213647346393984e-05,
      "loss": 1.6433,
      "step": 235600
    },
    {
      "epoch": 0.9475862581901183,
      "grad_norm": 3.3545126914978027,
      "learning_rate": 3.420694682871199e-05,
      "loss": 1.7139,
      "step": 235700
    },
    {
      "epoch": 0.9479882888469661,
      "grad_norm": 3.4358651638031006,
      "learning_rate": 3.4200246311029994e-05,
      "loss": 1.6334,
      "step": 235800
    },
    {
      "epoch": 0.9483903195038138,
      "grad_norm": 3.6963560581207275,
      "learning_rate": 3.4193545793348e-05,
      "loss": 1.6223,
      "step": 235900
    },
    {
      "epoch": 0.9487923501606615,
      "grad_norm": 3.1961543560028076,
      "learning_rate": 3.4186845275666004e-05,
      "loss": 1.6239,
      "step": 236000
    },
    {
      "epoch": 0.9491943808175093,
      "grad_norm": 2.7822530269622803,
      "learning_rate": 3.4180144757984e-05,
      "loss": 1.6466,
      "step": 236100
    },
    {
      "epoch": 0.949596411474357,
      "grad_norm": 3.5270700454711914,
      "learning_rate": 3.417344424030201e-05,
      "loss": 1.5993,
      "step": 236200
    },
    {
      "epoch": 0.9499984421312048,
      "grad_norm": 3.0594356060028076,
      "learning_rate": 3.4166743722620006e-05,
      "loss": 1.6312,
      "step": 236300
    },
    {
      "epoch": 0.9504004727880525,
      "grad_norm": 3.525033950805664,
      "learning_rate": 3.416004320493801e-05,
      "loss": 1.6697,
      "step": 236400
    },
    {
      "epoch": 0.9508025034449001,
      "grad_norm": 3.406259536743164,
      "learning_rate": 3.4153342687256017e-05,
      "loss": 1.6692,
      "step": 236500
    },
    {
      "epoch": 0.9512045341017479,
      "grad_norm": 3.1557626724243164,
      "learning_rate": 3.414664216957402e-05,
      "loss": 1.6217,
      "step": 236600
    },
    {
      "epoch": 0.9516065647585956,
      "grad_norm": 3.0913946628570557,
      "learning_rate": 3.413994165189203e-05,
      "loss": 1.5984,
      "step": 236700
    },
    {
      "epoch": 0.9520085954154434,
      "grad_norm": 2.6290717124938965,
      "learning_rate": 3.413324113421003e-05,
      "loss": 1.6134,
      "step": 236800
    },
    {
      "epoch": 0.9524106260722911,
      "grad_norm": 3.241274356842041,
      "learning_rate": 3.412654061652804e-05,
      "loss": 1.6577,
      "step": 236900
    },
    {
      "epoch": 0.9528126567291388,
      "grad_norm": 2.9475462436676025,
      "learning_rate": 3.411984009884604e-05,
      "loss": 1.64,
      "step": 237000
    },
    {
      "epoch": 0.9532146873859866,
      "grad_norm": 2.801074504852295,
      "learning_rate": 3.411313958116405e-05,
      "loss": 1.6308,
      "step": 237100
    },
    {
      "epoch": 0.9536167180428343,
      "grad_norm": 2.9553942680358887,
      "learning_rate": 3.4106439063482046e-05,
      "loss": 1.5764,
      "step": 237200
    },
    {
      "epoch": 0.9540187486996821,
      "grad_norm": 3.4960029125213623,
      "learning_rate": 3.409973854580005e-05,
      "loss": 1.6201,
      "step": 237300
    },
    {
      "epoch": 0.9544207793565298,
      "grad_norm": 2.686960220336914,
      "learning_rate": 3.409303802811805e-05,
      "loss": 1.5911,
      "step": 237400
    },
    {
      "epoch": 0.9548228100133775,
      "grad_norm": 3.060106039047241,
      "learning_rate": 3.4086337510436054e-05,
      "loss": 1.6316,
      "step": 237500
    },
    {
      "epoch": 0.9552248406702253,
      "grad_norm": 3.0693230628967285,
      "learning_rate": 3.407963699275406e-05,
      "loss": 1.6642,
      "step": 237600
    },
    {
      "epoch": 0.955626871327073,
      "grad_norm": 3.2376906871795654,
      "learning_rate": 3.4072936475072064e-05,
      "loss": 1.5986,
      "step": 237700
    },
    {
      "epoch": 0.9560289019839208,
      "grad_norm": 3.3147895336151123,
      "learning_rate": 3.406623595739007e-05,
      "loss": 1.6122,
      "step": 237800
    },
    {
      "epoch": 0.9564309326407685,
      "grad_norm": 3.1952476501464844,
      "learning_rate": 3.4059535439708075e-05,
      "loss": 1.6528,
      "step": 237900
    },
    {
      "epoch": 0.9568329632976162,
      "grad_norm": 4.324949741363525,
      "learning_rate": 3.405283492202608e-05,
      "loss": 1.6525,
      "step": 238000
    },
    {
      "epoch": 0.957234993954464,
      "grad_norm": 2.6157188415527344,
      "learning_rate": 3.4046134404344085e-05,
      "loss": 1.6205,
      "step": 238100
    },
    {
      "epoch": 0.9576370246113117,
      "grad_norm": 3.4617433547973633,
      "learning_rate": 3.403943388666208e-05,
      "loss": 1.6714,
      "step": 238200
    },
    {
      "epoch": 0.9580390552681595,
      "grad_norm": 2.850900173187256,
      "learning_rate": 3.403273336898009e-05,
      "loss": 1.6148,
      "step": 238300
    },
    {
      "epoch": 0.9584410859250072,
      "grad_norm": 3.4578051567077637,
      "learning_rate": 3.4026032851298094e-05,
      "loss": 1.5819,
      "step": 238400
    },
    {
      "epoch": 0.958843116581855,
      "grad_norm": 3.8411526679992676,
      "learning_rate": 3.401933233361609e-05,
      "loss": 1.5711,
      "step": 238500
    },
    {
      "epoch": 0.9592451472387027,
      "grad_norm": 2.886550188064575,
      "learning_rate": 3.40126318159341e-05,
      "loss": 1.5723,
      "step": 238600
    },
    {
      "epoch": 0.9596471778955504,
      "grad_norm": 3.5535922050476074,
      "learning_rate": 3.40059312982521e-05,
      "loss": 1.6838,
      "step": 238700
    },
    {
      "epoch": 0.9600492085523982,
      "grad_norm": 2.8151862621307373,
      "learning_rate": 3.399923078057011e-05,
      "loss": 1.6164,
      "step": 238800
    },
    {
      "epoch": 0.9604512392092459,
      "grad_norm": 3.7838993072509766,
      "learning_rate": 3.399253026288811e-05,
      "loss": 1.6129,
      "step": 238900
    },
    {
      "epoch": 0.9608532698660937,
      "grad_norm": 3.1538829803466797,
      "learning_rate": 3.398582974520612e-05,
      "loss": 1.6284,
      "step": 239000
    },
    {
      "epoch": 0.9612553005229414,
      "grad_norm": 3.018411874771118,
      "learning_rate": 3.397912922752412e-05,
      "loss": 1.593,
      "step": 239100
    },
    {
      "epoch": 0.9616573311797891,
      "grad_norm": 3.337301254272461,
      "learning_rate": 3.397242870984213e-05,
      "loss": 1.68,
      "step": 239200
    },
    {
      "epoch": 0.9620593618366369,
      "grad_norm": 3.7173233032226562,
      "learning_rate": 3.3965728192160126e-05,
      "loss": 1.5911,
      "step": 239300
    },
    {
      "epoch": 0.9624613924934846,
      "grad_norm": 3.368414878845215,
      "learning_rate": 3.395902767447813e-05,
      "loss": 1.6618,
      "step": 239400
    },
    {
      "epoch": 0.9628634231503324,
      "grad_norm": 3.0209388732910156,
      "learning_rate": 3.3952327156796136e-05,
      "loss": 1.6434,
      "step": 239500
    },
    {
      "epoch": 0.9632654538071801,
      "grad_norm": 3.296290159225464,
      "learning_rate": 3.394562663911414e-05,
      "loss": 1.6466,
      "step": 239600
    },
    {
      "epoch": 0.9636674844640278,
      "grad_norm": 3.055706739425659,
      "learning_rate": 3.393892612143214e-05,
      "loss": 1.6237,
      "step": 239700
    },
    {
      "epoch": 0.9640695151208756,
      "grad_norm": 3.113131523132324,
      "learning_rate": 3.3932225603750145e-05,
      "loss": 1.6525,
      "step": 239800
    },
    {
      "epoch": 0.9644715457777233,
      "grad_norm": 2.9194869995117188,
      "learning_rate": 3.392552508606815e-05,
      "loss": 1.6359,
      "step": 239900
    },
    {
      "epoch": 0.9648735764345711,
      "grad_norm": 3.1530964374542236,
      "learning_rate": 3.3918824568386155e-05,
      "loss": 1.6138,
      "step": 240000
    },
    {
      "epoch": 0.9652756070914188,
      "grad_norm": 2.9288430213928223,
      "learning_rate": 3.391212405070416e-05,
      "loss": 1.6437,
      "step": 240100
    },
    {
      "epoch": 0.9656776377482665,
      "grad_norm": 3.1273019313812256,
      "learning_rate": 3.3905423533022166e-05,
      "loss": 1.612,
      "step": 240200
    },
    {
      "epoch": 0.9660796684051143,
      "grad_norm": 3.716492176055908,
      "learning_rate": 3.3898723015340164e-05,
      "loss": 1.6215,
      "step": 240300
    },
    {
      "epoch": 0.966481699061962,
      "grad_norm": 3.252410888671875,
      "learning_rate": 3.389202249765817e-05,
      "loss": 1.6722,
      "step": 240400
    },
    {
      "epoch": 0.9668837297188098,
      "grad_norm": 2.979111909866333,
      "learning_rate": 3.3885321979976174e-05,
      "loss": 1.6733,
      "step": 240500
    },
    {
      "epoch": 0.9672857603756575,
      "grad_norm": 3.556790828704834,
      "learning_rate": 3.387862146229418e-05,
      "loss": 1.6179,
      "step": 240600
    },
    {
      "epoch": 0.9676877910325051,
      "grad_norm": 3.562743663787842,
      "learning_rate": 3.3871920944612184e-05,
      "loss": 1.5945,
      "step": 240700
    },
    {
      "epoch": 0.9680898216893529,
      "grad_norm": 3.3133232593536377,
      "learning_rate": 3.386522042693019e-05,
      "loss": 1.5933,
      "step": 240800
    },
    {
      "epoch": 0.9684918523462006,
      "grad_norm": 3.820061683654785,
      "learning_rate": 3.385851990924819e-05,
      "loss": 1.6185,
      "step": 240900
    },
    {
      "epoch": 0.9688938830030484,
      "grad_norm": 3.607722759246826,
      "learning_rate": 3.385181939156619e-05,
      "loss": 1.6025,
      "step": 241000
    },
    {
      "epoch": 0.9692959136598961,
      "grad_norm": 2.6706109046936035,
      "learning_rate": 3.38451188738842e-05,
      "loss": 1.6114,
      "step": 241100
    },
    {
      "epoch": 0.9696979443167438,
      "grad_norm": 2.9797213077545166,
      "learning_rate": 3.38384183562022e-05,
      "loss": 1.5905,
      "step": 241200
    },
    {
      "epoch": 0.9700999749735916,
      "grad_norm": 4.039651393890381,
      "learning_rate": 3.38317178385202e-05,
      "loss": 1.6546,
      "step": 241300
    },
    {
      "epoch": 0.9705020056304393,
      "grad_norm": 3.277160406112671,
      "learning_rate": 3.382501732083821e-05,
      "loss": 1.6717,
      "step": 241400
    },
    {
      "epoch": 0.970904036287287,
      "grad_norm": 3.858794927597046,
      "learning_rate": 3.381831680315621e-05,
      "loss": 1.6599,
      "step": 241500
    },
    {
      "epoch": 0.9713060669441348,
      "grad_norm": 3.185244560241699,
      "learning_rate": 3.381161628547422e-05,
      "loss": 1.5972,
      "step": 241600
    },
    {
      "epoch": 0.9717080976009825,
      "grad_norm": 3.251490354537964,
      "learning_rate": 3.380491576779222e-05,
      "loss": 1.6208,
      "step": 241700
    },
    {
      "epoch": 0.9721101282578303,
      "grad_norm": 3.9468257427215576,
      "learning_rate": 3.379821525011023e-05,
      "loss": 1.6351,
      "step": 241800
    },
    {
      "epoch": 0.972512158914678,
      "grad_norm": 2.7747766971588135,
      "learning_rate": 3.379151473242823e-05,
      "loss": 1.5969,
      "step": 241900
    },
    {
      "epoch": 0.9729141895715258,
      "grad_norm": 3.9338979721069336,
      "learning_rate": 3.378481421474624e-05,
      "loss": 1.6254,
      "step": 242000
    },
    {
      "epoch": 0.9733162202283735,
      "grad_norm": 3.258208751678467,
      "learning_rate": 3.3778113697064236e-05,
      "loss": 1.5826,
      "step": 242100
    },
    {
      "epoch": 0.9737182508852212,
      "grad_norm": 3.627192258834839,
      "learning_rate": 3.377141317938224e-05,
      "loss": 1.6443,
      "step": 242200
    },
    {
      "epoch": 0.974120281542069,
      "grad_norm": 4.32263708114624,
      "learning_rate": 3.3764712661700246e-05,
      "loss": 1.6129,
      "step": 242300
    },
    {
      "epoch": 0.9745223121989167,
      "grad_norm": 3.0304954051971436,
      "learning_rate": 3.3758012144018244e-05,
      "loss": 1.6185,
      "step": 242400
    },
    {
      "epoch": 0.9749243428557645,
      "grad_norm": 3.5453851222991943,
      "learning_rate": 3.375131162633625e-05,
      "loss": 1.6322,
      "step": 242500
    },
    {
      "epoch": 0.9753263735126122,
      "grad_norm": 3.4357216358184814,
      "learning_rate": 3.3744611108654255e-05,
      "loss": 1.6223,
      "step": 242600
    },
    {
      "epoch": 0.9757284041694599,
      "grad_norm": 3.691676616668701,
      "learning_rate": 3.373791059097226e-05,
      "loss": 1.628,
      "step": 242700
    },
    {
      "epoch": 0.9761304348263077,
      "grad_norm": 2.746793270111084,
      "learning_rate": 3.3731210073290265e-05,
      "loss": 1.6442,
      "step": 242800
    },
    {
      "epoch": 0.9765324654831554,
      "grad_norm": 2.9927494525909424,
      "learning_rate": 3.372450955560827e-05,
      "loss": 1.619,
      "step": 242900
    },
    {
      "epoch": 0.9769344961400032,
      "grad_norm": 2.724867105484009,
      "learning_rate": 3.3717809037926275e-05,
      "loss": 1.6304,
      "step": 243000
    },
    {
      "epoch": 0.9773365267968509,
      "grad_norm": 3.4482898712158203,
      "learning_rate": 3.371110852024428e-05,
      "loss": 1.6202,
      "step": 243100
    },
    {
      "epoch": 0.9777385574536986,
      "grad_norm": 3.1781818866729736,
      "learning_rate": 3.3704408002562285e-05,
      "loss": 1.6621,
      "step": 243200
    },
    {
      "epoch": 0.9781405881105464,
      "grad_norm": 3.030122756958008,
      "learning_rate": 3.3697707484880284e-05,
      "loss": 1.6146,
      "step": 243300
    },
    {
      "epoch": 0.9785426187673941,
      "grad_norm": 3.3967134952545166,
      "learning_rate": 3.369100696719828e-05,
      "loss": 1.6524,
      "step": 243400
    },
    {
      "epoch": 0.9789446494242419,
      "grad_norm": 3.072408676147461,
      "learning_rate": 3.368430644951629e-05,
      "loss": 1.6288,
      "step": 243500
    },
    {
      "epoch": 0.9793466800810896,
      "grad_norm": 2.689373254776001,
      "learning_rate": 3.367760593183429e-05,
      "loss": 1.6455,
      "step": 243600
    },
    {
      "epoch": 0.9797487107379373,
      "grad_norm": 3.9172801971435547,
      "learning_rate": 3.36709054141523e-05,
      "loss": 1.6137,
      "step": 243700
    },
    {
      "epoch": 0.9801507413947851,
      "grad_norm": 3.367349863052368,
      "learning_rate": 3.36642048964703e-05,
      "loss": 1.6251,
      "step": 243800
    },
    {
      "epoch": 0.9805527720516328,
      "grad_norm": 3.4361000061035156,
      "learning_rate": 3.365750437878831e-05,
      "loss": 1.6144,
      "step": 243900
    },
    {
      "epoch": 0.9809548027084806,
      "grad_norm": 3.110041618347168,
      "learning_rate": 3.365080386110631e-05,
      "loss": 1.6771,
      "step": 244000
    },
    {
      "epoch": 0.9813568333653283,
      "grad_norm": 3.3979079723358154,
      "learning_rate": 3.364410334342432e-05,
      "loss": 1.6233,
      "step": 244100
    },
    {
      "epoch": 0.981758864022176,
      "grad_norm": 3.0345981121063232,
      "learning_rate": 3.363740282574232e-05,
      "loss": 1.6057,
      "step": 244200
    },
    {
      "epoch": 0.9821608946790238,
      "grad_norm": 3.1382787227630615,
      "learning_rate": 3.363070230806032e-05,
      "loss": 1.6538,
      "step": 244300
    },
    {
      "epoch": 0.9825629253358715,
      "grad_norm": 3.343001127243042,
      "learning_rate": 3.362400179037833e-05,
      "loss": 1.6149,
      "step": 244400
    },
    {
      "epoch": 0.9829649559927193,
      "grad_norm": 3.419501781463623,
      "learning_rate": 3.3617301272696325e-05,
      "loss": 1.6317,
      "step": 244500
    },
    {
      "epoch": 0.983366986649567,
      "grad_norm": 3.688312292098999,
      "learning_rate": 3.361060075501433e-05,
      "loss": 1.6925,
      "step": 244600
    },
    {
      "epoch": 0.9837690173064148,
      "grad_norm": 3.565643310546875,
      "learning_rate": 3.3603900237332335e-05,
      "loss": 1.6451,
      "step": 244700
    },
    {
      "epoch": 0.9841710479632625,
      "grad_norm": 3.3945136070251465,
      "learning_rate": 3.359719971965034e-05,
      "loss": 1.649,
      "step": 244800
    },
    {
      "epoch": 0.9845730786201102,
      "grad_norm": 3.0143063068389893,
      "learning_rate": 3.3590499201968345e-05,
      "loss": 1.6116,
      "step": 244900
    },
    {
      "epoch": 0.9849751092769579,
      "grad_norm": 3.310626745223999,
      "learning_rate": 3.358379868428635e-05,
      "loss": 1.6048,
      "step": 245000
    },
    {
      "epoch": 0.9853771399338056,
      "grad_norm": 3.1388072967529297,
      "learning_rate": 3.3577098166604356e-05,
      "loss": 1.5843,
      "step": 245100
    },
    {
      "epoch": 0.9857791705906533,
      "grad_norm": 3.7537286281585693,
      "learning_rate": 3.357039764892236e-05,
      "loss": 1.6197,
      "step": 245200
    },
    {
      "epoch": 0.9861812012475011,
      "grad_norm": 3.139347553253174,
      "learning_rate": 3.356369713124036e-05,
      "loss": 1.563,
      "step": 245300
    },
    {
      "epoch": 0.9865832319043488,
      "grad_norm": 3.0485103130340576,
      "learning_rate": 3.3556996613558364e-05,
      "loss": 1.6155,
      "step": 245400
    },
    {
      "epoch": 0.9869852625611966,
      "grad_norm": 2.8959767818450928,
      "learning_rate": 3.355029609587637e-05,
      "loss": 1.6079,
      "step": 245500
    },
    {
      "epoch": 0.9873872932180443,
      "grad_norm": 3.384453773498535,
      "learning_rate": 3.3543595578194375e-05,
      "loss": 1.6457,
      "step": 245600
    },
    {
      "epoch": 0.987789323874892,
      "grad_norm": 3.4882652759552,
      "learning_rate": 3.353689506051237e-05,
      "loss": 1.5917,
      "step": 245700
    },
    {
      "epoch": 0.9881913545317398,
      "grad_norm": 3.189931869506836,
      "learning_rate": 3.353019454283038e-05,
      "loss": 1.6226,
      "step": 245800
    },
    {
      "epoch": 0.9885933851885875,
      "grad_norm": 3.3093647956848145,
      "learning_rate": 3.352349402514838e-05,
      "loss": 1.5881,
      "step": 245900
    },
    {
      "epoch": 0.9889954158454353,
      "grad_norm": 3.033456325531006,
      "learning_rate": 3.351679350746639e-05,
      "loss": 1.6353,
      "step": 246000
    },
    {
      "epoch": 0.989397446502283,
      "grad_norm": 3.269862651824951,
      "learning_rate": 3.3510092989784393e-05,
      "loss": 1.6231,
      "step": 246100
    },
    {
      "epoch": 0.9897994771591307,
      "grad_norm": 3.584602117538452,
      "learning_rate": 3.35033924721024e-05,
      "loss": 1.6045,
      "step": 246200
    },
    {
      "epoch": 0.9902015078159785,
      "grad_norm": 3.132258653640747,
      "learning_rate": 3.3496691954420404e-05,
      "loss": 1.6397,
      "step": 246300
    },
    {
      "epoch": 0.9906035384728262,
      "grad_norm": 3.0932021141052246,
      "learning_rate": 3.34899914367384e-05,
      "loss": 1.603,
      "step": 246400
    },
    {
      "epoch": 0.991005569129674,
      "grad_norm": 3.3049676418304443,
      "learning_rate": 3.348329091905641e-05,
      "loss": 1.6425,
      "step": 246500
    },
    {
      "epoch": 0.9914075997865217,
      "grad_norm": 2.9494454860687256,
      "learning_rate": 3.347659040137441e-05,
      "loss": 1.619,
      "step": 246600
    },
    {
      "epoch": 0.9918096304433694,
      "grad_norm": 3.8279976844787598,
      "learning_rate": 3.346988988369242e-05,
      "loss": 1.6213,
      "step": 246700
    },
    {
      "epoch": 0.9922116611002172,
      "grad_norm": 2.722632646560669,
      "learning_rate": 3.346318936601042e-05,
      "loss": 1.652,
      "step": 246800
    },
    {
      "epoch": 0.9926136917570649,
      "grad_norm": 3.835681676864624,
      "learning_rate": 3.345648884832842e-05,
      "loss": 1.6411,
      "step": 246900
    },
    {
      "epoch": 0.9930157224139127,
      "grad_norm": 3.046461343765259,
      "learning_rate": 3.3449788330646426e-05,
      "loss": 1.6071,
      "step": 247000
    },
    {
      "epoch": 0.9934177530707604,
      "grad_norm": 2.898115634918213,
      "learning_rate": 3.344308781296443e-05,
      "loss": 1.6424,
      "step": 247100
    },
    {
      "epoch": 0.9938197837276082,
      "grad_norm": 3.070241689682007,
      "learning_rate": 3.3436387295282436e-05,
      "loss": 1.6486,
      "step": 247200
    },
    {
      "epoch": 0.9942218143844559,
      "grad_norm": 3.2618043422698975,
      "learning_rate": 3.342968677760044e-05,
      "loss": 1.6473,
      "step": 247300
    },
    {
      "epoch": 0.9946238450413036,
      "grad_norm": 3.41019344329834,
      "learning_rate": 3.342298625991844e-05,
      "loss": 1.5878,
      "step": 247400
    },
    {
      "epoch": 0.9950258756981514,
      "grad_norm": 2.854846239089966,
      "learning_rate": 3.3416285742236445e-05,
      "loss": 1.5629,
      "step": 247500
    },
    {
      "epoch": 0.9954279063549991,
      "grad_norm": 3.142932176589966,
      "learning_rate": 3.340958522455445e-05,
      "loss": 1.6358,
      "step": 247600
    },
    {
      "epoch": 0.9958299370118469,
      "grad_norm": 3.7940351963043213,
      "learning_rate": 3.3402884706872455e-05,
      "loss": 1.6006,
      "step": 247700
    },
    {
      "epoch": 0.9962319676686946,
      "grad_norm": 3.3021767139434814,
      "learning_rate": 3.339618418919046e-05,
      "loss": 1.6405,
      "step": 247800
    },
    {
      "epoch": 0.9966339983255423,
      "grad_norm": 3.4477734565734863,
      "learning_rate": 3.3389483671508465e-05,
      "loss": 1.6073,
      "step": 247900
    },
    {
      "epoch": 0.9970360289823901,
      "grad_norm": 3.895890951156616,
      "learning_rate": 3.3382783153826464e-05,
      "loss": 1.6431,
      "step": 248000
    },
    {
      "epoch": 0.9974380596392378,
      "grad_norm": 3.114316463470459,
      "learning_rate": 3.337608263614447e-05,
      "loss": 1.6603,
      "step": 248100
    },
    {
      "epoch": 0.9978400902960856,
      "grad_norm": 3.92964768409729,
      "learning_rate": 3.3369382118462474e-05,
      "loss": 1.5725,
      "step": 248200
    },
    {
      "epoch": 0.9982421209529333,
      "grad_norm": 3.524934768676758,
      "learning_rate": 3.336268160078048e-05,
      "loss": 1.638,
      "step": 248300
    },
    {
      "epoch": 0.998644151609781,
      "grad_norm": 2.8470256328582764,
      "learning_rate": 3.3355981083098484e-05,
      "loss": 1.611,
      "step": 248400
    },
    {
      "epoch": 0.9990461822666288,
      "grad_norm": 3.5641629695892334,
      "learning_rate": 3.334928056541648e-05,
      "loss": 1.6279,
      "step": 248500
    },
    {
      "epoch": 0.9994482129234765,
      "grad_norm": 3.243811845779419,
      "learning_rate": 3.334258004773449e-05,
      "loss": 1.6251,
      "step": 248600
    },
    {
      "epoch": 0.9998502435803243,
      "grad_norm": 2.46266770362854,
      "learning_rate": 3.333587953005249e-05,
      "loss": 1.5858,
      "step": 248700
    },
    {
      "epoch": 1.0002532793138141,
      "grad_norm": 3.280758857727051,
      "learning_rate": 3.33291790123705e-05,
      "loss": 1.6143,
      "step": 248800
    },
    {
      "epoch": 1.0006553099706619,
      "grad_norm": 2.6700785160064697,
      "learning_rate": 3.33224784946885e-05,
      "loss": 1.5811,
      "step": 248900
    },
    {
      "epoch": 1.0010573406275096,
      "grad_norm": 2.8768467903137207,
      "learning_rate": 3.331577797700651e-05,
      "loss": 1.5857,
      "step": 249000
    },
    {
      "epoch": 1.0014593712843574,
      "grad_norm": 2.8775100708007812,
      "learning_rate": 3.330907745932451e-05,
      "loss": 1.5647,
      "step": 249100
    },
    {
      "epoch": 1.001861401941205,
      "grad_norm": 3.869311809539795,
      "learning_rate": 3.330237694164251e-05,
      "loss": 1.5477,
      "step": 249200
    },
    {
      "epoch": 1.0022634325980528,
      "grad_norm": 2.803959846496582,
      "learning_rate": 3.329567642396052e-05,
      "loss": 1.5967,
      "step": 249300
    },
    {
      "epoch": 1.0026654632549006,
      "grad_norm": 3.088979482650757,
      "learning_rate": 3.328897590627852e-05,
      "loss": 1.5933,
      "step": 249400
    },
    {
      "epoch": 1.0030674939117483,
      "grad_norm": 3.132369041442871,
      "learning_rate": 3.328227538859652e-05,
      "loss": 1.5726,
      "step": 249500
    },
    {
      "epoch": 1.003469524568596,
      "grad_norm": 2.924691915512085,
      "learning_rate": 3.3275574870914525e-05,
      "loss": 1.5582,
      "step": 249600
    },
    {
      "epoch": 1.0038715552254438,
      "grad_norm": 3.336395502090454,
      "learning_rate": 3.326887435323253e-05,
      "loss": 1.5862,
      "step": 249700
    },
    {
      "epoch": 1.0042735858822915,
      "grad_norm": 3.128021240234375,
      "learning_rate": 3.3262173835550536e-05,
      "loss": 1.5697,
      "step": 249800
    },
    {
      "epoch": 1.0046756165391393,
      "grad_norm": 3.1515400409698486,
      "learning_rate": 3.325547331786854e-05,
      "loss": 1.5764,
      "step": 249900
    },
    {
      "epoch": 1.005077647195987,
      "grad_norm": 3.1953091621398926,
      "learning_rate": 3.3248772800186546e-05,
      "loss": 1.5699,
      "step": 250000
    },
    {
      "epoch": 1.0054796778528348,
      "grad_norm": 3.4031593799591064,
      "learning_rate": 3.324207228250455e-05,
      "loss": 1.5587,
      "step": 250100
    },
    {
      "epoch": 1.0058817085096825,
      "grad_norm": 3.1483383178710938,
      "learning_rate": 3.3235371764822556e-05,
      "loss": 1.5866,
      "step": 250200
    },
    {
      "epoch": 1.0062837391665302,
      "grad_norm": 3.070539951324463,
      "learning_rate": 3.322867124714056e-05,
      "loss": 1.5888,
      "step": 250300
    },
    {
      "epoch": 1.006685769823378,
      "grad_norm": 3.3736400604248047,
      "learning_rate": 3.322197072945856e-05,
      "loss": 1.5942,
      "step": 250400
    },
    {
      "epoch": 1.0070878004802257,
      "grad_norm": 3.206193685531616,
      "learning_rate": 3.321527021177656e-05,
      "loss": 1.5648,
      "step": 250500
    },
    {
      "epoch": 1.0074898311370735,
      "grad_norm": 3.101052761077881,
      "learning_rate": 3.320856969409456e-05,
      "loss": 1.5188,
      "step": 250600
    },
    {
      "epoch": 1.007891861793921,
      "grad_norm": 2.9465014934539795,
      "learning_rate": 3.320186917641257e-05,
      "loss": 1.5717,
      "step": 250700
    },
    {
      "epoch": 1.0082938924507687,
      "grad_norm": 3.617459297180176,
      "learning_rate": 3.3195168658730573e-05,
      "loss": 1.5602,
      "step": 250800
    },
    {
      "epoch": 1.0086959231076165,
      "grad_norm": 3.5505259037017822,
      "learning_rate": 3.318846814104858e-05,
      "loss": 1.6027,
      "step": 250900
    },
    {
      "epoch": 1.0090979537644642,
      "grad_norm": 3.058946371078491,
      "learning_rate": 3.3181767623366584e-05,
      "loss": 1.5717,
      "step": 251000
    },
    {
      "epoch": 1.009499984421312,
      "grad_norm": 3.6891839504241943,
      "learning_rate": 3.317506710568459e-05,
      "loss": 1.6173,
      "step": 251100
    },
    {
      "epoch": 1.0099020150781597,
      "grad_norm": 3.4611148834228516,
      "learning_rate": 3.3168366588002594e-05,
      "loss": 1.5396,
      "step": 251200
    },
    {
      "epoch": 1.0103040457350074,
      "grad_norm": 2.842285633087158,
      "learning_rate": 3.31616660703206e-05,
      "loss": 1.5709,
      "step": 251300
    },
    {
      "epoch": 1.0107060763918552,
      "grad_norm": 3.725464344024658,
      "learning_rate": 3.31549655526386e-05,
      "loss": 1.5993,
      "step": 251400
    },
    {
      "epoch": 1.011108107048703,
      "grad_norm": 3.4398350715637207,
      "learning_rate": 3.31482650349566e-05,
      "loss": 1.5655,
      "step": 251500
    },
    {
      "epoch": 1.0115101377055506,
      "grad_norm": 2.9120664596557617,
      "learning_rate": 3.31415645172746e-05,
      "loss": 1.6022,
      "step": 251600
    },
    {
      "epoch": 1.0119121683623984,
      "grad_norm": 2.8355352878570557,
      "learning_rate": 3.3134863999592606e-05,
      "loss": 1.5469,
      "step": 251700
    },
    {
      "epoch": 1.0123141990192461,
      "grad_norm": 3.302199363708496,
      "learning_rate": 3.312816348191061e-05,
      "loss": 1.6055,
      "step": 251800
    },
    {
      "epoch": 1.0127162296760939,
      "grad_norm": 3.8782169818878174,
      "learning_rate": 3.3121462964228616e-05,
      "loss": 1.6158,
      "step": 251900
    },
    {
      "epoch": 1.0131182603329416,
      "grad_norm": 2.606342077255249,
      "learning_rate": 3.311476244654662e-05,
      "loss": 1.5677,
      "step": 252000
    },
    {
      "epoch": 1.0135202909897894,
      "grad_norm": 3.351419687271118,
      "learning_rate": 3.3108061928864626e-05,
      "loss": 1.5712,
      "step": 252100
    },
    {
      "epoch": 1.013922321646637,
      "grad_norm": 3.0380618572235107,
      "learning_rate": 3.310136141118263e-05,
      "loss": 1.5371,
      "step": 252200
    },
    {
      "epoch": 1.0143243523034848,
      "grad_norm": 2.8755857944488525,
      "learning_rate": 3.309466089350064e-05,
      "loss": 1.5787,
      "step": 252300
    },
    {
      "epoch": 1.0147263829603326,
      "grad_norm": 3.1764376163482666,
      "learning_rate": 3.308796037581864e-05,
      "loss": 1.6064,
      "step": 252400
    },
    {
      "epoch": 1.0151284136171803,
      "grad_norm": 3.1550066471099854,
      "learning_rate": 3.308125985813664e-05,
      "loss": 1.5619,
      "step": 252500
    },
    {
      "epoch": 1.015530444274028,
      "grad_norm": 3.2509336471557617,
      "learning_rate": 3.3074559340454645e-05,
      "loss": 1.5467,
      "step": 252600
    },
    {
      "epoch": 1.0159324749308758,
      "grad_norm": 3.148524522781372,
      "learning_rate": 3.306785882277265e-05,
      "loss": 1.5436,
      "step": 252700
    },
    {
      "epoch": 1.0163345055877235,
      "grad_norm": 2.938994884490967,
      "learning_rate": 3.306115830509065e-05,
      "loss": 1.5843,
      "step": 252800
    },
    {
      "epoch": 1.0167365362445713,
      "grad_norm": 3.259685516357422,
      "learning_rate": 3.3054457787408654e-05,
      "loss": 1.605,
      "step": 252900
    },
    {
      "epoch": 1.017138566901419,
      "grad_norm": 3.1337199211120605,
      "learning_rate": 3.304775726972666e-05,
      "loss": 1.586,
      "step": 253000
    },
    {
      "epoch": 1.0175405975582668,
      "grad_norm": 3.598062038421631,
      "learning_rate": 3.3041056752044664e-05,
      "loss": 1.544,
      "step": 253100
    },
    {
      "epoch": 1.0179426282151145,
      "grad_norm": 2.909637689590454,
      "learning_rate": 3.303435623436267e-05,
      "loss": 1.5364,
      "step": 253200
    },
    {
      "epoch": 1.0183446588719622,
      "grad_norm": 2.8048717975616455,
      "learning_rate": 3.3027655716680674e-05,
      "loss": 1.5299,
      "step": 253300
    },
    {
      "epoch": 1.01874668952881,
      "grad_norm": 2.5776216983795166,
      "learning_rate": 3.302095519899868e-05,
      "loss": 1.5595,
      "step": 253400
    },
    {
      "epoch": 1.0191487201856577,
      "grad_norm": 4.057907581329346,
      "learning_rate": 3.301425468131668e-05,
      "loss": 1.5396,
      "step": 253500
    },
    {
      "epoch": 1.0195507508425055,
      "grad_norm": 3.630690336227417,
      "learning_rate": 3.300755416363468e-05,
      "loss": 1.6128,
      "step": 253600
    },
    {
      "epoch": 1.0199527814993532,
      "grad_norm": 3.517808675765991,
      "learning_rate": 3.300085364595269e-05,
      "loss": 1.5693,
      "step": 253700
    },
    {
      "epoch": 1.020354812156201,
      "grad_norm": 3.1016838550567627,
      "learning_rate": 3.299415312827069e-05,
      "loss": 1.5762,
      "step": 253800
    },
    {
      "epoch": 1.0207568428130487,
      "grad_norm": 3.100019693374634,
      "learning_rate": 3.29874526105887e-05,
      "loss": 1.5141,
      "step": 253900
    },
    {
      "epoch": 1.0211588734698964,
      "grad_norm": 3.4008679389953613,
      "learning_rate": 3.29807520929067e-05,
      "loss": 1.5335,
      "step": 254000
    },
    {
      "epoch": 1.0215609041267442,
      "grad_norm": 2.9550623893737793,
      "learning_rate": 3.29740515752247e-05,
      "loss": 1.5805,
      "step": 254100
    },
    {
      "epoch": 1.021962934783592,
      "grad_norm": 3.574592113494873,
      "learning_rate": 3.296735105754271e-05,
      "loss": 1.5948,
      "step": 254200
    },
    {
      "epoch": 1.0223649654404396,
      "grad_norm": 3.0646374225616455,
      "learning_rate": 3.296065053986071e-05,
      "loss": 1.5472,
      "step": 254300
    },
    {
      "epoch": 1.0227669960972874,
      "grad_norm": 3.3741238117218018,
      "learning_rate": 3.295395002217872e-05,
      "loss": 1.5739,
      "step": 254400
    },
    {
      "epoch": 1.0231690267541351,
      "grad_norm": 3.3835792541503906,
      "learning_rate": 3.2947249504496716e-05,
      "loss": 1.5183,
      "step": 254500
    },
    {
      "epoch": 1.0235710574109829,
      "grad_norm": 3.190176010131836,
      "learning_rate": 3.294054898681472e-05,
      "loss": 1.5538,
      "step": 254600
    },
    {
      "epoch": 1.0239730880678306,
      "grad_norm": 3.915905237197876,
      "learning_rate": 3.2933848469132726e-05,
      "loss": 1.529,
      "step": 254700
    },
    {
      "epoch": 1.0243751187246783,
      "grad_norm": 3.661098003387451,
      "learning_rate": 3.292714795145073e-05,
      "loss": 1.6056,
      "step": 254800
    },
    {
      "epoch": 1.024777149381526,
      "grad_norm": 3.2431185245513916,
      "learning_rate": 3.2920447433768736e-05,
      "loss": 1.5467,
      "step": 254900
    },
    {
      "epoch": 1.0251791800383738,
      "grad_norm": 3.5742244720458984,
      "learning_rate": 3.291374691608674e-05,
      "loss": 1.5103,
      "step": 255000
    },
    {
      "epoch": 1.0255812106952216,
      "grad_norm": 2.377565860748291,
      "learning_rate": 3.2907046398404746e-05,
      "loss": 1.5674,
      "step": 255100
    },
    {
      "epoch": 1.0259832413520693,
      "grad_norm": 3.1369643211364746,
      "learning_rate": 3.2900345880722745e-05,
      "loss": 1.6003,
      "step": 255200
    },
    {
      "epoch": 1.026385272008917,
      "grad_norm": 3.1071698665618896,
      "learning_rate": 3.289364536304075e-05,
      "loss": 1.5854,
      "step": 255300
    },
    {
      "epoch": 1.0267873026657648,
      "grad_norm": 3.6973235607147217,
      "learning_rate": 3.2886944845358755e-05,
      "loss": 1.5842,
      "step": 255400
    },
    {
      "epoch": 1.0271893333226125,
      "grad_norm": 3.6947004795074463,
      "learning_rate": 3.288024432767676e-05,
      "loss": 1.5753,
      "step": 255500
    },
    {
      "epoch": 1.0275913639794603,
      "grad_norm": 3.014753818511963,
      "learning_rate": 3.287354380999476e-05,
      "loss": 1.6023,
      "step": 255600
    },
    {
      "epoch": 1.027993394636308,
      "grad_norm": 3.235611915588379,
      "learning_rate": 3.2866843292312764e-05,
      "loss": 1.5698,
      "step": 255700
    },
    {
      "epoch": 1.0283954252931558,
      "grad_norm": 2.3889689445495605,
      "learning_rate": 3.286014277463077e-05,
      "loss": 1.5698,
      "step": 255800
    },
    {
      "epoch": 1.0287974559500035,
      "grad_norm": 3.102172613143921,
      "learning_rate": 3.2853442256948774e-05,
      "loss": 1.5912,
      "step": 255900
    },
    {
      "epoch": 1.0291994866068512,
      "grad_norm": 2.911510944366455,
      "learning_rate": 3.284674173926678e-05,
      "loss": 1.5913,
      "step": 256000
    },
    {
      "epoch": 1.029601517263699,
      "grad_norm": 3.3402528762817383,
      "learning_rate": 3.2840041221584784e-05,
      "loss": 1.568,
      "step": 256100
    },
    {
      "epoch": 1.0300035479205467,
      "grad_norm": 3.1237008571624756,
      "learning_rate": 3.283334070390279e-05,
      "loss": 1.5683,
      "step": 256200
    },
    {
      "epoch": 1.0304055785773945,
      "grad_norm": 3.312152147293091,
      "learning_rate": 3.2826640186220794e-05,
      "loss": 1.5943,
      "step": 256300
    },
    {
      "epoch": 1.0308076092342422,
      "grad_norm": 3.9923741817474365,
      "learning_rate": 3.281993966853879e-05,
      "loss": 1.6564,
      "step": 256400
    },
    {
      "epoch": 1.03120963989109,
      "grad_norm": 3.663149356842041,
      "learning_rate": 3.28132391508568e-05,
      "loss": 1.5807,
      "step": 256500
    },
    {
      "epoch": 1.0316116705479377,
      "grad_norm": 2.973428249359131,
      "learning_rate": 3.2806538633174796e-05,
      "loss": 1.5653,
      "step": 256600
    },
    {
      "epoch": 1.0320137012047854,
      "grad_norm": 3.0942091941833496,
      "learning_rate": 3.27998381154928e-05,
      "loss": 1.5909,
      "step": 256700
    },
    {
      "epoch": 1.0324157318616332,
      "grad_norm": 2.9223392009735107,
      "learning_rate": 3.2793137597810806e-05,
      "loss": 1.5579,
      "step": 256800
    },
    {
      "epoch": 1.032817762518481,
      "grad_norm": 3.1914455890655518,
      "learning_rate": 3.278643708012881e-05,
      "loss": 1.6027,
      "step": 256900
    },
    {
      "epoch": 1.0332197931753286,
      "grad_norm": 3.43200945854187,
      "learning_rate": 3.277973656244682e-05,
      "loss": 1.5773,
      "step": 257000
    },
    {
      "epoch": 1.0336218238321764,
      "grad_norm": 4.320389747619629,
      "learning_rate": 3.277303604476482e-05,
      "loss": 1.5114,
      "step": 257100
    },
    {
      "epoch": 1.0340238544890241,
      "grad_norm": 2.7028417587280273,
      "learning_rate": 3.276633552708283e-05,
      "loss": 1.5659,
      "step": 257200
    },
    {
      "epoch": 1.0344258851458719,
      "grad_norm": 3.2840514183044434,
      "learning_rate": 3.275963500940083e-05,
      "loss": 1.54,
      "step": 257300
    },
    {
      "epoch": 1.0348279158027196,
      "grad_norm": 3.3491852283477783,
      "learning_rate": 3.275293449171884e-05,
      "loss": 1.5837,
      "step": 257400
    },
    {
      "epoch": 1.0352299464595673,
      "grad_norm": 2.6512045860290527,
      "learning_rate": 3.2746233974036836e-05,
      "loss": 1.5241,
      "step": 257500
    },
    {
      "epoch": 1.035631977116415,
      "grad_norm": 3.869666576385498,
      "learning_rate": 3.2739533456354834e-05,
      "loss": 1.5307,
      "step": 257600
    },
    {
      "epoch": 1.0360340077732628,
      "grad_norm": 3.373807907104492,
      "learning_rate": 3.273283293867284e-05,
      "loss": 1.5727,
      "step": 257700
    },
    {
      "epoch": 1.0364360384301106,
      "grad_norm": 3.686777114868164,
      "learning_rate": 3.2726132420990844e-05,
      "loss": 1.5973,
      "step": 257800
    },
    {
      "epoch": 1.0368380690869583,
      "grad_norm": 3.6154611110687256,
      "learning_rate": 3.271943190330885e-05,
      "loss": 1.5794,
      "step": 257900
    },
    {
      "epoch": 1.037240099743806,
      "grad_norm": 4.373891353607178,
      "learning_rate": 3.2712731385626854e-05,
      "loss": 1.5651,
      "step": 258000
    },
    {
      "epoch": 1.0376421304006538,
      "grad_norm": 3.2965288162231445,
      "learning_rate": 3.270603086794486e-05,
      "loss": 1.5531,
      "step": 258100
    },
    {
      "epoch": 1.0380441610575015,
      "grad_norm": 4.070982456207275,
      "learning_rate": 3.2699330350262865e-05,
      "loss": 1.5944,
      "step": 258200
    },
    {
      "epoch": 1.0384461917143493,
      "grad_norm": 3.079894781112671,
      "learning_rate": 3.269262983258087e-05,
      "loss": 1.5611,
      "step": 258300
    },
    {
      "epoch": 1.038848222371197,
      "grad_norm": 3.6127512454986572,
      "learning_rate": 3.2685929314898875e-05,
      "loss": 1.5482,
      "step": 258400
    },
    {
      "epoch": 1.0392502530280447,
      "grad_norm": 3.3221194744110107,
      "learning_rate": 3.267922879721687e-05,
      "loss": 1.5636,
      "step": 258500
    },
    {
      "epoch": 1.0396522836848925,
      "grad_norm": 3.7254178524017334,
      "learning_rate": 3.267252827953488e-05,
      "loss": 1.5823,
      "step": 258600
    },
    {
      "epoch": 1.0400543143417402,
      "grad_norm": 3.3244922161102295,
      "learning_rate": 3.266582776185288e-05,
      "loss": 1.5916,
      "step": 258700
    },
    {
      "epoch": 1.040456344998588,
      "grad_norm": 3.6235883235931396,
      "learning_rate": 3.265912724417088e-05,
      "loss": 1.5635,
      "step": 258800
    },
    {
      "epoch": 1.0408583756554357,
      "grad_norm": 3.1262457370758057,
      "learning_rate": 3.265242672648889e-05,
      "loss": 1.5828,
      "step": 258900
    },
    {
      "epoch": 1.0412604063122834,
      "grad_norm": 2.813218355178833,
      "learning_rate": 3.264572620880689e-05,
      "loss": 1.5972,
      "step": 259000
    },
    {
      "epoch": 1.0416624369691312,
      "grad_norm": 3.0712122917175293,
      "learning_rate": 3.26390256911249e-05,
      "loss": 1.5501,
      "step": 259100
    },
    {
      "epoch": 1.0420644676259787,
      "grad_norm": 3.6141135692596436,
      "learning_rate": 3.26323251734429e-05,
      "loss": 1.5717,
      "step": 259200
    },
    {
      "epoch": 1.0424664982828267,
      "grad_norm": 3.077500104904175,
      "learning_rate": 3.262562465576091e-05,
      "loss": 1.5672,
      "step": 259300
    },
    {
      "epoch": 1.0428685289396742,
      "grad_norm": 3.094130039215088,
      "learning_rate": 3.261892413807891e-05,
      "loss": 1.6005,
      "step": 259400
    },
    {
      "epoch": 1.043270559596522,
      "grad_norm": 3.1898012161254883,
      "learning_rate": 3.261222362039692e-05,
      "loss": 1.5205,
      "step": 259500
    },
    {
      "epoch": 1.0436725902533697,
      "grad_norm": 2.8914153575897217,
      "learning_rate": 3.2605523102714916e-05,
      "loss": 1.5465,
      "step": 259600
    },
    {
      "epoch": 1.0440746209102174,
      "grad_norm": 3.7117998600006104,
      "learning_rate": 3.259882258503292e-05,
      "loss": 1.5519,
      "step": 259700
    },
    {
      "epoch": 1.0444766515670652,
      "grad_norm": 3.699932098388672,
      "learning_rate": 3.2592122067350926e-05,
      "loss": 1.5395,
      "step": 259800
    },
    {
      "epoch": 1.044878682223913,
      "grad_norm": 2.749630928039551,
      "learning_rate": 3.2585421549668925e-05,
      "loss": 1.5493,
      "step": 259900
    },
    {
      "epoch": 1.0452807128807606,
      "grad_norm": 3.338804006576538,
      "learning_rate": 3.257872103198693e-05,
      "loss": 1.5775,
      "step": 260000
    },
    {
      "epoch": 1.0456827435376084,
      "grad_norm": 3.632302761077881,
      "learning_rate": 3.2572020514304935e-05,
      "loss": 1.5835,
      "step": 260100
    },
    {
      "epoch": 1.0460847741944561,
      "grad_norm": 3.105022668838501,
      "learning_rate": 3.256531999662294e-05,
      "loss": 1.5947,
      "step": 260200
    },
    {
      "epoch": 1.0464868048513039,
      "grad_norm": 3.1462619304656982,
      "learning_rate": 3.2558619478940945e-05,
      "loss": 1.5545,
      "step": 260300
    },
    {
      "epoch": 1.0468888355081516,
      "grad_norm": 3.6405370235443115,
      "learning_rate": 3.255191896125895e-05,
      "loss": 1.6282,
      "step": 260400
    },
    {
      "epoch": 1.0472908661649993,
      "grad_norm": 3.4458749294281006,
      "learning_rate": 3.2545218443576955e-05,
      "loss": 1.5985,
      "step": 260500
    },
    {
      "epoch": 1.047692896821847,
      "grad_norm": 3.14728045463562,
      "learning_rate": 3.2538517925894954e-05,
      "loss": 1.566,
      "step": 260600
    },
    {
      "epoch": 1.0480949274786948,
      "grad_norm": 3.298382520675659,
      "learning_rate": 3.253181740821296e-05,
      "loss": 1.5813,
      "step": 260700
    },
    {
      "epoch": 1.0484969581355426,
      "grad_norm": 2.8909077644348145,
      "learning_rate": 3.2525116890530964e-05,
      "loss": 1.5661,
      "step": 260800
    },
    {
      "epoch": 1.0488989887923903,
      "grad_norm": 3.0368919372558594,
      "learning_rate": 3.251841637284897e-05,
      "loss": 1.5545,
      "step": 260900
    },
    {
      "epoch": 1.049301019449238,
      "grad_norm": 3.615272283554077,
      "learning_rate": 3.2511715855166974e-05,
      "loss": 1.569,
      "step": 261000
    },
    {
      "epoch": 1.0497030501060858,
      "grad_norm": 3.244544744491577,
      "learning_rate": 3.250501533748497e-05,
      "loss": 1.5293,
      "step": 261100
    },
    {
      "epoch": 1.0501050807629335,
      "grad_norm": 3.2479019165039062,
      "learning_rate": 3.249831481980298e-05,
      "loss": 1.5364,
      "step": 261200
    },
    {
      "epoch": 1.0505071114197813,
      "grad_norm": 2.9852359294891357,
      "learning_rate": 3.249161430212098e-05,
      "loss": 1.5419,
      "step": 261300
    },
    {
      "epoch": 1.050909142076629,
      "grad_norm": 3.439547061920166,
      "learning_rate": 3.248491378443899e-05,
      "loss": 1.5566,
      "step": 261400
    },
    {
      "epoch": 1.0513111727334767,
      "grad_norm": 3.4060988426208496,
      "learning_rate": 3.247821326675699e-05,
      "loss": 1.5833,
      "step": 261500
    },
    {
      "epoch": 1.0517132033903245,
      "grad_norm": 3.0842509269714355,
      "learning_rate": 3.247151274907499e-05,
      "loss": 1.5629,
      "step": 261600
    },
    {
      "epoch": 1.0521152340471722,
      "grad_norm": 3.4957094192504883,
      "learning_rate": 3.2464812231393e-05,
      "loss": 1.5917,
      "step": 261700
    },
    {
      "epoch": 1.05251726470402,
      "grad_norm": 2.585824728012085,
      "learning_rate": 3.2458111713711e-05,
      "loss": 1.5654,
      "step": 261800
    },
    {
      "epoch": 1.0529192953608677,
      "grad_norm": 2.741589069366455,
      "learning_rate": 3.245141119602901e-05,
      "loss": 1.6071,
      "step": 261900
    },
    {
      "epoch": 1.0533213260177154,
      "grad_norm": 3.6838796138763428,
      "learning_rate": 3.244471067834701e-05,
      "loss": 1.596,
      "step": 262000
    },
    {
      "epoch": 1.0537233566745632,
      "grad_norm": 4.224480628967285,
      "learning_rate": 3.243801016066502e-05,
      "loss": 1.5563,
      "step": 262100
    },
    {
      "epoch": 1.054125387331411,
      "grad_norm": 3.5182039737701416,
      "learning_rate": 3.243130964298302e-05,
      "loss": 1.5812,
      "step": 262200
    },
    {
      "epoch": 1.0545274179882587,
      "grad_norm": 3.3094077110290527,
      "learning_rate": 3.242460912530102e-05,
      "loss": 1.5588,
      "step": 262300
    },
    {
      "epoch": 1.0549294486451064,
      "grad_norm": 3.3159005641937256,
      "learning_rate": 3.2417908607619026e-05,
      "loss": 1.5702,
      "step": 262400
    },
    {
      "epoch": 1.0553314793019541,
      "grad_norm": 3.287414312362671,
      "learning_rate": 3.241120808993703e-05,
      "loss": 1.5957,
      "step": 262500
    },
    {
      "epoch": 1.0557335099588019,
      "grad_norm": 3.761050224304199,
      "learning_rate": 3.2404507572255036e-05,
      "loss": 1.584,
      "step": 262600
    },
    {
      "epoch": 1.0561355406156496,
      "grad_norm": 3.5343692302703857,
      "learning_rate": 3.2397807054573034e-05,
      "loss": 1.56,
      "step": 262700
    },
    {
      "epoch": 1.0565375712724974,
      "grad_norm": 2.798340320587158,
      "learning_rate": 3.239110653689104e-05,
      "loss": 1.5749,
      "step": 262800
    },
    {
      "epoch": 1.056939601929345,
      "grad_norm": 3.568225145339966,
      "learning_rate": 3.2384406019209045e-05,
      "loss": 1.5448,
      "step": 262900
    },
    {
      "epoch": 1.0573416325861928,
      "grad_norm": 3.784740686416626,
      "learning_rate": 3.237770550152705e-05,
      "loss": 1.58,
      "step": 263000
    },
    {
      "epoch": 1.0577436632430406,
      "grad_norm": 3.2052085399627686,
      "learning_rate": 3.2371004983845055e-05,
      "loss": 1.6169,
      "step": 263100
    },
    {
      "epoch": 1.0581456938998883,
      "grad_norm": 3.522207021713257,
      "learning_rate": 3.236430446616306e-05,
      "loss": 1.5478,
      "step": 263200
    },
    {
      "epoch": 1.058547724556736,
      "grad_norm": 2.798149585723877,
      "learning_rate": 3.2357603948481065e-05,
      "loss": 1.5455,
      "step": 263300
    },
    {
      "epoch": 1.0589497552135838,
      "grad_norm": 3.7461090087890625,
      "learning_rate": 3.235090343079907e-05,
      "loss": 1.5803,
      "step": 263400
    },
    {
      "epoch": 1.0593517858704316,
      "grad_norm": 3.4561402797698975,
      "learning_rate": 3.234420291311707e-05,
      "loss": 1.5169,
      "step": 263500
    },
    {
      "epoch": 1.0597538165272793,
      "grad_norm": 3.473722457885742,
      "learning_rate": 3.2337502395435074e-05,
      "loss": 1.5815,
      "step": 263600
    },
    {
      "epoch": 1.060155847184127,
      "grad_norm": 4.011608600616455,
      "learning_rate": 3.233080187775307e-05,
      "loss": 1.5645,
      "step": 263700
    },
    {
      "epoch": 1.0605578778409748,
      "grad_norm": 3.162588596343994,
      "learning_rate": 3.232410136007108e-05,
      "loss": 1.5648,
      "step": 263800
    },
    {
      "epoch": 1.0609599084978225,
      "grad_norm": 3.8165206909179688,
      "learning_rate": 3.231740084238908e-05,
      "loss": 1.5573,
      "step": 263900
    },
    {
      "epoch": 1.0613619391546703,
      "grad_norm": 2.9392218589782715,
      "learning_rate": 3.231070032470709e-05,
      "loss": 1.5723,
      "step": 264000
    },
    {
      "epoch": 1.061763969811518,
      "grad_norm": 3.521131992340088,
      "learning_rate": 3.230399980702509e-05,
      "loss": 1.5604,
      "step": 264100
    },
    {
      "epoch": 1.0621660004683657,
      "grad_norm": 2.8676798343658447,
      "learning_rate": 3.22972992893431e-05,
      "loss": 1.5899,
      "step": 264200
    },
    {
      "epoch": 1.0625680311252135,
      "grad_norm": 3.5678160190582275,
      "learning_rate": 3.22905987716611e-05,
      "loss": 1.5665,
      "step": 264300
    },
    {
      "epoch": 1.0629700617820612,
      "grad_norm": 3.260568618774414,
      "learning_rate": 3.228389825397911e-05,
      "loss": 1.5988,
      "step": 264400
    },
    {
      "epoch": 1.063372092438909,
      "grad_norm": 3.1563808917999268,
      "learning_rate": 3.227719773629711e-05,
      "loss": 1.6061,
      "step": 264500
    },
    {
      "epoch": 1.0637741230957567,
      "grad_norm": 3.0412757396698,
      "learning_rate": 3.227049721861511e-05,
      "loss": 1.5734,
      "step": 264600
    },
    {
      "epoch": 1.0641761537526044,
      "grad_norm": 3.1566028594970703,
      "learning_rate": 3.2263796700933117e-05,
      "loss": 1.547,
      "step": 264700
    },
    {
      "epoch": 1.0645781844094522,
      "grad_norm": 3.1001410484313965,
      "learning_rate": 3.2257096183251115e-05,
      "loss": 1.5535,
      "step": 264800
    },
    {
      "epoch": 1.0649802150663,
      "grad_norm": 3.843533515930176,
      "learning_rate": 3.225039566556912e-05,
      "loss": 1.5621,
      "step": 264900
    },
    {
      "epoch": 1.0653822457231477,
      "grad_norm": 3.8203325271606445,
      "learning_rate": 3.2243695147887125e-05,
      "loss": 1.5526,
      "step": 265000
    },
    {
      "epoch": 1.0657842763799954,
      "grad_norm": 4.3431782722473145,
      "learning_rate": 3.223699463020513e-05,
      "loss": 1.5506,
      "step": 265100
    },
    {
      "epoch": 1.0661863070368431,
      "grad_norm": 3.3577916622161865,
      "learning_rate": 3.2230294112523135e-05,
      "loss": 1.5379,
      "step": 265200
    },
    {
      "epoch": 1.0665883376936909,
      "grad_norm": 4.021955490112305,
      "learning_rate": 3.222359359484114e-05,
      "loss": 1.5101,
      "step": 265300
    },
    {
      "epoch": 1.0669903683505386,
      "grad_norm": 3.1255648136138916,
      "learning_rate": 3.2216893077159146e-05,
      "loss": 1.6066,
      "step": 265400
    },
    {
      "epoch": 1.0673923990073864,
      "grad_norm": 2.8309125900268555,
      "learning_rate": 3.221019255947715e-05,
      "loss": 1.5874,
      "step": 265500
    },
    {
      "epoch": 1.067794429664234,
      "grad_norm": 3.3599109649658203,
      "learning_rate": 3.220349204179515e-05,
      "loss": 1.6354,
      "step": 265600
    },
    {
      "epoch": 1.0681964603210818,
      "grad_norm": 3.216630458831787,
      "learning_rate": 3.2196791524113154e-05,
      "loss": 1.5893,
      "step": 265700
    },
    {
      "epoch": 1.0685984909779296,
      "grad_norm": 3.3417603969573975,
      "learning_rate": 3.219009100643116e-05,
      "loss": 1.5993,
      "step": 265800
    },
    {
      "epoch": 1.0690005216347773,
      "grad_norm": 3.773097515106201,
      "learning_rate": 3.218339048874916e-05,
      "loss": 1.5179,
      "step": 265900
    },
    {
      "epoch": 1.069402552291625,
      "grad_norm": 3.6516199111938477,
      "learning_rate": 3.217668997106716e-05,
      "loss": 1.5786,
      "step": 266000
    },
    {
      "epoch": 1.0698045829484728,
      "grad_norm": 2.795297622680664,
      "learning_rate": 3.216998945338517e-05,
      "loss": 1.542,
      "step": 266100
    },
    {
      "epoch": 1.0702066136053205,
      "grad_norm": 2.982588052749634,
      "learning_rate": 3.216328893570317e-05,
      "loss": 1.5533,
      "step": 266200
    },
    {
      "epoch": 1.0706086442621683,
      "grad_norm": 4.357997894287109,
      "learning_rate": 3.215658841802118e-05,
      "loss": 1.6167,
      "step": 266300
    },
    {
      "epoch": 1.071010674919016,
      "grad_norm": 3.3499581813812256,
      "learning_rate": 3.214988790033918e-05,
      "loss": 1.5625,
      "step": 266400
    },
    {
      "epoch": 1.0714127055758638,
      "grad_norm": 3.797318458557129,
      "learning_rate": 3.214318738265719e-05,
      "loss": 1.5612,
      "step": 266500
    },
    {
      "epoch": 1.0718147362327115,
      "grad_norm": 3.6473560333251953,
      "learning_rate": 3.2136486864975194e-05,
      "loss": 1.5975,
      "step": 266600
    },
    {
      "epoch": 1.0722167668895592,
      "grad_norm": 3.287648916244507,
      "learning_rate": 3.212978634729319e-05,
      "loss": 1.5726,
      "step": 266700
    },
    {
      "epoch": 1.072618797546407,
      "grad_norm": 3.4772560596466064,
      "learning_rate": 3.21230858296112e-05,
      "loss": 1.5354,
      "step": 266800
    },
    {
      "epoch": 1.0730208282032547,
      "grad_norm": 3.112490177154541,
      "learning_rate": 3.21163853119292e-05,
      "loss": 1.5686,
      "step": 266900
    },
    {
      "epoch": 1.0734228588601025,
      "grad_norm": 3.068831205368042,
      "learning_rate": 3.21096847942472e-05,
      "loss": 1.5249,
      "step": 267000
    },
    {
      "epoch": 1.0738248895169502,
      "grad_norm": 3.0187807083129883,
      "learning_rate": 3.2102984276565206e-05,
      "loss": 1.5418,
      "step": 267100
    },
    {
      "epoch": 1.074226920173798,
      "grad_norm": 3.247699737548828,
      "learning_rate": 3.209628375888321e-05,
      "loss": 1.5322,
      "step": 267200
    },
    {
      "epoch": 1.0746289508306457,
      "grad_norm": 3.323871612548828,
      "learning_rate": 3.2089583241201216e-05,
      "loss": 1.573,
      "step": 267300
    },
    {
      "epoch": 1.0750309814874934,
      "grad_norm": 2.9899017810821533,
      "learning_rate": 3.208288272351922e-05,
      "loss": 1.6211,
      "step": 267400
    },
    {
      "epoch": 1.075433012144341,
      "grad_norm": 3.148103952407837,
      "learning_rate": 3.2076182205837226e-05,
      "loss": 1.5767,
      "step": 267500
    },
    {
      "epoch": 1.075835042801189,
      "grad_norm": 2.8676648139953613,
      "learning_rate": 3.206948168815523e-05,
      "loss": 1.5536,
      "step": 267600
    },
    {
      "epoch": 1.0762370734580364,
      "grad_norm": 2.9826300144195557,
      "learning_rate": 3.206278117047323e-05,
      "loss": 1.5897,
      "step": 267700
    },
    {
      "epoch": 1.0766391041148844,
      "grad_norm": 3.280238628387451,
      "learning_rate": 3.2056080652791235e-05,
      "loss": 1.5603,
      "step": 267800
    },
    {
      "epoch": 1.077041134771732,
      "grad_norm": 3.157224655151367,
      "learning_rate": 3.204938013510924e-05,
      "loss": 1.5397,
      "step": 267900
    },
    {
      "epoch": 1.0774431654285799,
      "grad_norm": 3.5481748580932617,
      "learning_rate": 3.2042679617427245e-05,
      "loss": 1.5393,
      "step": 268000
    },
    {
      "epoch": 1.0778451960854274,
      "grad_norm": 2.864877700805664,
      "learning_rate": 3.203597909974525e-05,
      "loss": 1.5401,
      "step": 268100
    },
    {
      "epoch": 1.0782472267422751,
      "grad_norm": 3.2393224239349365,
      "learning_rate": 3.202927858206325e-05,
      "loss": 1.5693,
      "step": 268200
    },
    {
      "epoch": 1.0786492573991229,
      "grad_norm": 2.8463916778564453,
      "learning_rate": 3.2022578064381254e-05,
      "loss": 1.5581,
      "step": 268300
    },
    {
      "epoch": 1.0790512880559706,
      "grad_norm": 4.740322589874268,
      "learning_rate": 3.201587754669926e-05,
      "loss": 1.5822,
      "step": 268400
    },
    {
      "epoch": 1.0794533187128184,
      "grad_norm": 3.4818522930145264,
      "learning_rate": 3.2009177029017264e-05,
      "loss": 1.6049,
      "step": 268500
    },
    {
      "epoch": 1.079855349369666,
      "grad_norm": 3.456739902496338,
      "learning_rate": 3.200247651133527e-05,
      "loss": 1.5356,
      "step": 268600
    },
    {
      "epoch": 1.0802573800265138,
      "grad_norm": 2.6695449352264404,
      "learning_rate": 3.1995775993653274e-05,
      "loss": 1.5795,
      "step": 268700
    },
    {
      "epoch": 1.0806594106833616,
      "grad_norm": 3.8157403469085693,
      "learning_rate": 3.198907547597127e-05,
      "loss": 1.5643,
      "step": 268800
    },
    {
      "epoch": 1.0810614413402093,
      "grad_norm": 3.280698537826538,
      "learning_rate": 3.198237495828928e-05,
      "loss": 1.5785,
      "step": 268900
    },
    {
      "epoch": 1.081463471997057,
      "grad_norm": 3.785961627960205,
      "learning_rate": 3.197567444060728e-05,
      "loss": 1.5849,
      "step": 269000
    },
    {
      "epoch": 1.0818655026539048,
      "grad_norm": 4.258250713348389,
      "learning_rate": 3.196897392292529e-05,
      "loss": 1.5215,
      "step": 269100
    },
    {
      "epoch": 1.0822675333107525,
      "grad_norm": 2.8338451385498047,
      "learning_rate": 3.196227340524329e-05,
      "loss": 1.537,
      "step": 269200
    },
    {
      "epoch": 1.0826695639676003,
      "grad_norm": 3.443056106567383,
      "learning_rate": 3.19555728875613e-05,
      "loss": 1.5788,
      "step": 269300
    },
    {
      "epoch": 1.083071594624448,
      "grad_norm": 3.5417795181274414,
      "learning_rate": 3.1948872369879296e-05,
      "loss": 1.5695,
      "step": 269400
    },
    {
      "epoch": 1.0834736252812958,
      "grad_norm": 2.971950054168701,
      "learning_rate": 3.19421718521973e-05,
      "loss": 1.563,
      "step": 269500
    },
    {
      "epoch": 1.0838756559381435,
      "grad_norm": 3.7821755409240723,
      "learning_rate": 3.193547133451531e-05,
      "loss": 1.5843,
      "step": 269600
    },
    {
      "epoch": 1.0842776865949912,
      "grad_norm": 3.6000235080718994,
      "learning_rate": 3.192877081683331e-05,
      "loss": 1.575,
      "step": 269700
    },
    {
      "epoch": 1.084679717251839,
      "grad_norm": 3.219322443008423,
      "learning_rate": 3.192207029915131e-05,
      "loss": 1.5658,
      "step": 269800
    },
    {
      "epoch": 1.0850817479086867,
      "grad_norm": 2.934424877166748,
      "learning_rate": 3.1915369781469315e-05,
      "loss": 1.5593,
      "step": 269900
    },
    {
      "epoch": 1.0854837785655345,
      "grad_norm": 2.966531753540039,
      "learning_rate": 3.190866926378732e-05,
      "loss": 1.5842,
      "step": 270000
    },
    {
      "epoch": 1.0858858092223822,
      "grad_norm": 2.9128592014312744,
      "learning_rate": 3.1901968746105326e-05,
      "loss": 1.5137,
      "step": 270100
    },
    {
      "epoch": 1.08628783987923,
      "grad_norm": 3.5798308849334717,
      "learning_rate": 3.189526822842333e-05,
      "loss": 1.5664,
      "step": 270200
    },
    {
      "epoch": 1.0866898705360777,
      "grad_norm": 3.924570083618164,
      "learning_rate": 3.1888567710741336e-05,
      "loss": 1.5586,
      "step": 270300
    },
    {
      "epoch": 1.0870919011929254,
      "grad_norm": 3.432170867919922,
      "learning_rate": 3.188186719305934e-05,
      "loss": 1.5444,
      "step": 270400
    },
    {
      "epoch": 1.0874939318497732,
      "grad_norm": 2.6098263263702393,
      "learning_rate": 3.1875166675377346e-05,
      "loss": 1.5826,
      "step": 270500
    },
    {
      "epoch": 1.087895962506621,
      "grad_norm": 3.9792935848236084,
      "learning_rate": 3.1868466157695344e-05,
      "loss": 1.565,
      "step": 270600
    },
    {
      "epoch": 1.0882979931634686,
      "grad_norm": 3.4243898391723633,
      "learning_rate": 3.186176564001335e-05,
      "loss": 1.5548,
      "step": 270700
    },
    {
      "epoch": 1.0887000238203164,
      "grad_norm": 3.035095691680908,
      "learning_rate": 3.185506512233135e-05,
      "loss": 1.5998,
      "step": 270800
    },
    {
      "epoch": 1.0891020544771641,
      "grad_norm": 4.086634159088135,
      "learning_rate": 3.184836460464935e-05,
      "loss": 1.6036,
      "step": 270900
    },
    {
      "epoch": 1.0895040851340119,
      "grad_norm": 3.704385995864868,
      "learning_rate": 3.184166408696736e-05,
      "loss": 1.5585,
      "step": 271000
    },
    {
      "epoch": 1.0899061157908596,
      "grad_norm": 3.2246134281158447,
      "learning_rate": 3.183496356928536e-05,
      "loss": 1.5505,
      "step": 271100
    },
    {
      "epoch": 1.0903081464477073,
      "grad_norm": 3.763653516769409,
      "learning_rate": 3.182826305160337e-05,
      "loss": 1.505,
      "step": 271200
    },
    {
      "epoch": 1.090710177104555,
      "grad_norm": 3.269301176071167,
      "learning_rate": 3.1821562533921374e-05,
      "loss": 1.5281,
      "step": 271300
    },
    {
      "epoch": 1.0911122077614028,
      "grad_norm": 3.8983302116394043,
      "learning_rate": 3.181486201623938e-05,
      "loss": 1.5828,
      "step": 271400
    },
    {
      "epoch": 1.0915142384182506,
      "grad_norm": 3.5641791820526123,
      "learning_rate": 3.1808161498557384e-05,
      "loss": 1.5405,
      "step": 271500
    },
    {
      "epoch": 1.0919162690750983,
      "grad_norm": 3.7610487937927246,
      "learning_rate": 3.180146098087539e-05,
      "loss": 1.6071,
      "step": 271600
    },
    {
      "epoch": 1.092318299731946,
      "grad_norm": 4.677760601043701,
      "learning_rate": 3.179476046319339e-05,
      "loss": 1.5916,
      "step": 271700
    },
    {
      "epoch": 1.0927203303887938,
      "grad_norm": 3.455817222595215,
      "learning_rate": 3.178805994551139e-05,
      "loss": 1.6097,
      "step": 271800
    },
    {
      "epoch": 1.0931223610456415,
      "grad_norm": 2.821439743041992,
      "learning_rate": 3.178135942782939e-05,
      "loss": 1.5523,
      "step": 271900
    },
    {
      "epoch": 1.0935243917024893,
      "grad_norm": 2.9037396907806396,
      "learning_rate": 3.1774658910147396e-05,
      "loss": 1.5803,
      "step": 272000
    },
    {
      "epoch": 1.093926422359337,
      "grad_norm": 3.3005104064941406,
      "learning_rate": 3.17679583924654e-05,
      "loss": 1.5866,
      "step": 272100
    },
    {
      "epoch": 1.0943284530161848,
      "grad_norm": 2.776911973953247,
      "learning_rate": 3.1761257874783406e-05,
      "loss": 1.5541,
      "step": 272200
    },
    {
      "epoch": 1.0947304836730325,
      "grad_norm": 2.920254945755005,
      "learning_rate": 3.175455735710141e-05,
      "loss": 1.5767,
      "step": 272300
    },
    {
      "epoch": 1.0951325143298802,
      "grad_norm": 3.7225606441497803,
      "learning_rate": 3.1747856839419416e-05,
      "loss": 1.5534,
      "step": 272400
    },
    {
      "epoch": 1.095534544986728,
      "grad_norm": 3.1369855403900146,
      "learning_rate": 3.174115632173742e-05,
      "loss": 1.5248,
      "step": 272500
    },
    {
      "epoch": 1.0959365756435757,
      "grad_norm": 3.032809257507324,
      "learning_rate": 3.173445580405543e-05,
      "loss": 1.6035,
      "step": 272600
    },
    {
      "epoch": 1.0963386063004235,
      "grad_norm": 3.0747861862182617,
      "learning_rate": 3.172775528637343e-05,
      "loss": 1.5413,
      "step": 272700
    },
    {
      "epoch": 1.0967406369572712,
      "grad_norm": 3.8710734844207764,
      "learning_rate": 3.172105476869143e-05,
      "loss": 1.5829,
      "step": 272800
    },
    {
      "epoch": 1.097142667614119,
      "grad_norm": 2.9709558486938477,
      "learning_rate": 3.1714354251009435e-05,
      "loss": 1.5708,
      "step": 272900
    },
    {
      "epoch": 1.0975446982709667,
      "grad_norm": 2.806467056274414,
      "learning_rate": 3.1707653733327434e-05,
      "loss": 1.5557,
      "step": 273000
    },
    {
      "epoch": 1.0979467289278144,
      "grad_norm": 2.488668441772461,
      "learning_rate": 3.170095321564544e-05,
      "loss": 1.5628,
      "step": 273100
    },
    {
      "epoch": 1.0983487595846622,
      "grad_norm": 3.0077576637268066,
      "learning_rate": 3.1694252697963444e-05,
      "loss": 1.5511,
      "step": 273200
    },
    {
      "epoch": 1.09875079024151,
      "grad_norm": 3.0284810066223145,
      "learning_rate": 3.168755218028145e-05,
      "loss": 1.5441,
      "step": 273300
    },
    {
      "epoch": 1.0991528208983576,
      "grad_norm": 3.024486780166626,
      "learning_rate": 3.1680851662599454e-05,
      "loss": 1.6073,
      "step": 273400
    },
    {
      "epoch": 1.0995548515552054,
      "grad_norm": 3.0001938343048096,
      "learning_rate": 3.167415114491746e-05,
      "loss": 1.5583,
      "step": 273500
    },
    {
      "epoch": 1.0999568822120531,
      "grad_norm": 3.4585604667663574,
      "learning_rate": 3.1667450627235464e-05,
      "loss": 1.512,
      "step": 273600
    },
    {
      "epoch": 1.1003589128689009,
      "grad_norm": 3.344663381576538,
      "learning_rate": 3.166075010955347e-05,
      "loss": 1.5745,
      "step": 273700
    },
    {
      "epoch": 1.1007609435257486,
      "grad_norm": 3.262831926345825,
      "learning_rate": 3.165404959187147e-05,
      "loss": 1.5375,
      "step": 273800
    },
    {
      "epoch": 1.1011629741825963,
      "grad_norm": 2.740781784057617,
      "learning_rate": 3.164734907418947e-05,
      "loss": 1.5525,
      "step": 273900
    },
    {
      "epoch": 1.101565004839444,
      "grad_norm": 2.9658665657043457,
      "learning_rate": 3.164064855650748e-05,
      "loss": 1.5803,
      "step": 274000
    },
    {
      "epoch": 1.1019670354962918,
      "grad_norm": 2.898146390914917,
      "learning_rate": 3.163394803882548e-05,
      "loss": 1.5224,
      "step": 274100
    },
    {
      "epoch": 1.1023690661531396,
      "grad_norm": 2.828512191772461,
      "learning_rate": 3.162724752114348e-05,
      "loss": 1.5553,
      "step": 274200
    },
    {
      "epoch": 1.1027710968099873,
      "grad_norm": 3.99684739112854,
      "learning_rate": 3.162054700346149e-05,
      "loss": 1.5813,
      "step": 274300
    },
    {
      "epoch": 1.103173127466835,
      "grad_norm": 3.3952431678771973,
      "learning_rate": 3.161384648577949e-05,
      "loss": 1.5558,
      "step": 274400
    },
    {
      "epoch": 1.1035751581236828,
      "grad_norm": 3.0200552940368652,
      "learning_rate": 3.16071459680975e-05,
      "loss": 1.6081,
      "step": 274500
    },
    {
      "epoch": 1.1039771887805305,
      "grad_norm": 3.8517961502075195,
      "learning_rate": 3.16004454504155e-05,
      "loss": 1.5097,
      "step": 274600
    },
    {
      "epoch": 1.1043792194373783,
      "grad_norm": 3.1840715408325195,
      "learning_rate": 3.159374493273351e-05,
      "loss": 1.544,
      "step": 274700
    },
    {
      "epoch": 1.104781250094226,
      "grad_norm": 3.645991325378418,
      "learning_rate": 3.1587044415051506e-05,
      "loss": 1.5486,
      "step": 274800
    },
    {
      "epoch": 1.1051832807510737,
      "grad_norm": 3.3171751499176025,
      "learning_rate": 3.158034389736951e-05,
      "loss": 1.5784,
      "step": 274900
    },
    {
      "epoch": 1.1055853114079215,
      "grad_norm": 3.491037130355835,
      "learning_rate": 3.1573643379687516e-05,
      "loss": 1.5433,
      "step": 275000
    },
    {
      "epoch": 1.1059873420647692,
      "grad_norm": 2.7113475799560547,
      "learning_rate": 3.156694286200552e-05,
      "loss": 1.5717,
      "step": 275100
    },
    {
      "epoch": 1.106389372721617,
      "grad_norm": 3.5812153816223145,
      "learning_rate": 3.1560242344323526e-05,
      "loss": 1.6011,
      "step": 275200
    },
    {
      "epoch": 1.1067914033784647,
      "grad_norm": 3.0591373443603516,
      "learning_rate": 3.155354182664153e-05,
      "loss": 1.5915,
      "step": 275300
    },
    {
      "epoch": 1.1071934340353125,
      "grad_norm": 3.1334149837493896,
      "learning_rate": 3.154684130895953e-05,
      "loss": 1.5693,
      "step": 275400
    },
    {
      "epoch": 1.1075954646921602,
      "grad_norm": 3.0884969234466553,
      "learning_rate": 3.1540140791277535e-05,
      "loss": 1.5863,
      "step": 275500
    },
    {
      "epoch": 1.107997495349008,
      "grad_norm": 2.9644954204559326,
      "learning_rate": 3.153344027359554e-05,
      "loss": 1.5817,
      "step": 275600
    },
    {
      "epoch": 1.1083995260058557,
      "grad_norm": 3.67582106590271,
      "learning_rate": 3.1526739755913545e-05,
      "loss": 1.5816,
      "step": 275700
    },
    {
      "epoch": 1.1088015566627034,
      "grad_norm": 3.0131988525390625,
      "learning_rate": 3.152003923823155e-05,
      "loss": 1.5533,
      "step": 275800
    },
    {
      "epoch": 1.1092035873195512,
      "grad_norm": 3.847998857498169,
      "learning_rate": 3.151333872054955e-05,
      "loss": 1.579,
      "step": 275900
    },
    {
      "epoch": 1.1096056179763987,
      "grad_norm": 3.6323986053466797,
      "learning_rate": 3.1506638202867553e-05,
      "loss": 1.5726,
      "step": 276000
    },
    {
      "epoch": 1.1100076486332466,
      "grad_norm": 3.742130756378174,
      "learning_rate": 3.149993768518556e-05,
      "loss": 1.577,
      "step": 276100
    },
    {
      "epoch": 1.1104096792900942,
      "grad_norm": 3.355410099029541,
      "learning_rate": 3.1493237167503564e-05,
      "loss": 1.5713,
      "step": 276200
    },
    {
      "epoch": 1.1108117099469421,
      "grad_norm": 3.7357399463653564,
      "learning_rate": 3.148653664982157e-05,
      "loss": 1.5677,
      "step": 276300
    },
    {
      "epoch": 1.1112137406037896,
      "grad_norm": 3.375466823577881,
      "learning_rate": 3.1479836132139574e-05,
      "loss": 1.5734,
      "step": 276400
    },
    {
      "epoch": 1.1116157712606376,
      "grad_norm": 2.7977662086486816,
      "learning_rate": 3.147313561445757e-05,
      "loss": 1.5694,
      "step": 276500
    },
    {
      "epoch": 1.1120178019174851,
      "grad_norm": 3.665823221206665,
      "learning_rate": 3.146643509677558e-05,
      "loss": 1.5369,
      "step": 276600
    },
    {
      "epoch": 1.1124198325743329,
      "grad_norm": 2.8570406436920166,
      "learning_rate": 3.145973457909358e-05,
      "loss": 1.6014,
      "step": 276700
    },
    {
      "epoch": 1.1128218632311806,
      "grad_norm": 3.170436382293701,
      "learning_rate": 3.145303406141159e-05,
      "loss": 1.5094,
      "step": 276800
    },
    {
      "epoch": 1.1132238938880283,
      "grad_norm": 3.197721242904663,
      "learning_rate": 3.1446333543729586e-05,
      "loss": 1.5189,
      "step": 276900
    },
    {
      "epoch": 1.113625924544876,
      "grad_norm": 3.59065318107605,
      "learning_rate": 3.143963302604759e-05,
      "loss": 1.5463,
      "step": 277000
    },
    {
      "epoch": 1.1140279552017238,
      "grad_norm": 3.285454273223877,
      "learning_rate": 3.1432932508365596e-05,
      "loss": 1.5784,
      "step": 277100
    },
    {
      "epoch": 1.1144299858585716,
      "grad_norm": 4.559629440307617,
      "learning_rate": 3.14262319906836e-05,
      "loss": 1.5828,
      "step": 277200
    },
    {
      "epoch": 1.1148320165154193,
      "grad_norm": 2.9949233531951904,
      "learning_rate": 3.1419531473001607e-05,
      "loss": 1.5058,
      "step": 277300
    },
    {
      "epoch": 1.115234047172267,
      "grad_norm": 2.8726742267608643,
      "learning_rate": 3.141283095531961e-05,
      "loss": 1.5971,
      "step": 277400
    },
    {
      "epoch": 1.1156360778291148,
      "grad_norm": 3.232182025909424,
      "learning_rate": 3.140613043763762e-05,
      "loss": 1.5325,
      "step": 277500
    },
    {
      "epoch": 1.1160381084859625,
      "grad_norm": 3.284677743911743,
      "learning_rate": 3.139942991995562e-05,
      "loss": 1.5481,
      "step": 277600
    },
    {
      "epoch": 1.1164401391428103,
      "grad_norm": 2.9760854244232178,
      "learning_rate": 3.139272940227362e-05,
      "loss": 1.5535,
      "step": 277700
    },
    {
      "epoch": 1.116842169799658,
      "grad_norm": 3.1013927459716797,
      "learning_rate": 3.1386028884591625e-05,
      "loss": 1.5263,
      "step": 277800
    },
    {
      "epoch": 1.1172442004565057,
      "grad_norm": 3.1211955547332764,
      "learning_rate": 3.137932836690963e-05,
      "loss": 1.5248,
      "step": 277900
    },
    {
      "epoch": 1.1176462311133535,
      "grad_norm": 4.181530952453613,
      "learning_rate": 3.137262784922763e-05,
      "loss": 1.5262,
      "step": 278000
    },
    {
      "epoch": 1.1180482617702012,
      "grad_norm": 3.2033238410949707,
      "learning_rate": 3.1365927331545634e-05,
      "loss": 1.6093,
      "step": 278100
    },
    {
      "epoch": 1.118450292427049,
      "grad_norm": 3.3096141815185547,
      "learning_rate": 3.135922681386364e-05,
      "loss": 1.6157,
      "step": 278200
    },
    {
      "epoch": 1.1188523230838967,
      "grad_norm": 3.1190712451934814,
      "learning_rate": 3.1352526296181644e-05,
      "loss": 1.5556,
      "step": 278300
    },
    {
      "epoch": 1.1192543537407444,
      "grad_norm": 3.1992392539978027,
      "learning_rate": 3.134582577849965e-05,
      "loss": 1.5906,
      "step": 278400
    },
    {
      "epoch": 1.1196563843975922,
      "grad_norm": 3.0744845867156982,
      "learning_rate": 3.1339125260817655e-05,
      "loss": 1.5338,
      "step": 278500
    },
    {
      "epoch": 1.12005841505444,
      "grad_norm": 2.806619882583618,
      "learning_rate": 3.133242474313566e-05,
      "loss": 1.585,
      "step": 278600
    },
    {
      "epoch": 1.1204604457112877,
      "grad_norm": 3.424205780029297,
      "learning_rate": 3.1325724225453665e-05,
      "loss": 1.6306,
      "step": 278700
    },
    {
      "epoch": 1.1208624763681354,
      "grad_norm": 3.1264078617095947,
      "learning_rate": 3.131902370777166e-05,
      "loss": 1.6152,
      "step": 278800
    },
    {
      "epoch": 1.1212645070249831,
      "grad_norm": 3.61234188079834,
      "learning_rate": 3.131232319008967e-05,
      "loss": 1.5643,
      "step": 278900
    },
    {
      "epoch": 1.1216665376818309,
      "grad_norm": 3.15923810005188,
      "learning_rate": 3.130562267240767e-05,
      "loss": 1.5219,
      "step": 279000
    },
    {
      "epoch": 1.1220685683386786,
      "grad_norm": 3.707582473754883,
      "learning_rate": 3.129892215472567e-05,
      "loss": 1.6055,
      "step": 279100
    },
    {
      "epoch": 1.1224705989955264,
      "grad_norm": 3.721691131591797,
      "learning_rate": 3.129222163704368e-05,
      "loss": 1.5751,
      "step": 279200
    },
    {
      "epoch": 1.122872629652374,
      "grad_norm": 2.9574599266052246,
      "learning_rate": 3.128552111936168e-05,
      "loss": 1.5423,
      "step": 279300
    },
    {
      "epoch": 1.1232746603092219,
      "grad_norm": 2.944617509841919,
      "learning_rate": 3.127882060167969e-05,
      "loss": 1.534,
      "step": 279400
    },
    {
      "epoch": 1.1236766909660696,
      "grad_norm": 3.520638942718506,
      "learning_rate": 3.127212008399769e-05,
      "loss": 1.5418,
      "step": 279500
    },
    {
      "epoch": 1.1240787216229173,
      "grad_norm": 3.5471253395080566,
      "learning_rate": 3.12654195663157e-05,
      "loss": 1.5935,
      "step": 279600
    },
    {
      "epoch": 1.124480752279765,
      "grad_norm": 3.7615277767181396,
      "learning_rate": 3.12587190486337e-05,
      "loss": 1.5939,
      "step": 279700
    },
    {
      "epoch": 1.1248827829366128,
      "grad_norm": 3.4229841232299805,
      "learning_rate": 3.125201853095171e-05,
      "loss": 1.5715,
      "step": 279800
    },
    {
      "epoch": 1.1252848135934606,
      "grad_norm": 3.3091013431549072,
      "learning_rate": 3.1245318013269706e-05,
      "loss": 1.555,
      "step": 279900
    },
    {
      "epoch": 1.1256868442503083,
      "grad_norm": 3.9387857913970947,
      "learning_rate": 3.123861749558771e-05,
      "loss": 1.5691,
      "step": 280000
    },
    {
      "epoch": 1.126088874907156,
      "grad_norm": 2.952540397644043,
      "learning_rate": 3.123191697790571e-05,
      "loss": 1.5187,
      "step": 280100
    },
    {
      "epoch": 1.1264909055640038,
      "grad_norm": 3.182500123977661,
      "learning_rate": 3.1225216460223715e-05,
      "loss": 1.5438,
      "step": 280200
    },
    {
      "epoch": 1.1268929362208515,
      "grad_norm": 3.196526288986206,
      "learning_rate": 3.121851594254172e-05,
      "loss": 1.5971,
      "step": 280300
    },
    {
      "epoch": 1.1272949668776993,
      "grad_norm": 3.6262922286987305,
      "learning_rate": 3.1211815424859725e-05,
      "loss": 1.5623,
      "step": 280400
    },
    {
      "epoch": 1.127696997534547,
      "grad_norm": 3.5055224895477295,
      "learning_rate": 3.120511490717773e-05,
      "loss": 1.5743,
      "step": 280500
    },
    {
      "epoch": 1.1280990281913947,
      "grad_norm": 3.392883777618408,
      "learning_rate": 3.1198414389495735e-05,
      "loss": 1.565,
      "step": 280600
    },
    {
      "epoch": 1.1285010588482425,
      "grad_norm": 3.5740902423858643,
      "learning_rate": 3.119171387181374e-05,
      "loss": 1.6371,
      "step": 280700
    },
    {
      "epoch": 1.1289030895050902,
      "grad_norm": 2.54172420501709,
      "learning_rate": 3.1185013354131745e-05,
      "loss": 1.5887,
      "step": 280800
    },
    {
      "epoch": 1.129305120161938,
      "grad_norm": 2.865476131439209,
      "learning_rate": 3.1178312836449744e-05,
      "loss": 1.5334,
      "step": 280900
    },
    {
      "epoch": 1.1297071508187857,
      "grad_norm": 3.168088674545288,
      "learning_rate": 3.117161231876775e-05,
      "loss": 1.5325,
      "step": 281000
    },
    {
      "epoch": 1.1301091814756334,
      "grad_norm": 3.6121137142181396,
      "learning_rate": 3.1164911801085754e-05,
      "loss": 1.5006,
      "step": 281100
    },
    {
      "epoch": 1.1305112121324812,
      "grad_norm": 3.4053332805633545,
      "learning_rate": 3.115821128340376e-05,
      "loss": 1.568,
      "step": 281200
    },
    {
      "epoch": 1.130913242789329,
      "grad_norm": 3.5083274841308594,
      "learning_rate": 3.115151076572176e-05,
      "loss": 1.556,
      "step": 281300
    },
    {
      "epoch": 1.1313152734461767,
      "grad_norm": 3.4724555015563965,
      "learning_rate": 3.114481024803976e-05,
      "loss": 1.5768,
      "step": 281400
    },
    {
      "epoch": 1.1317173041030244,
      "grad_norm": 2.968125820159912,
      "learning_rate": 3.113810973035777e-05,
      "loss": 1.5598,
      "step": 281500
    },
    {
      "epoch": 1.1321193347598721,
      "grad_norm": 3.2213075160980225,
      "learning_rate": 3.113140921267577e-05,
      "loss": 1.5393,
      "step": 281600
    },
    {
      "epoch": 1.1325213654167199,
      "grad_norm": 3.2121527194976807,
      "learning_rate": 3.112470869499378e-05,
      "loss": 1.5551,
      "step": 281700
    },
    {
      "epoch": 1.1329233960735676,
      "grad_norm": 2.357564687728882,
      "learning_rate": 3.111800817731178e-05,
      "loss": 1.5699,
      "step": 281800
    },
    {
      "epoch": 1.1333254267304154,
      "grad_norm": 2.9842193126678467,
      "learning_rate": 3.111130765962978e-05,
      "loss": 1.5706,
      "step": 281900
    },
    {
      "epoch": 1.133727457387263,
      "grad_norm": 2.9340033531188965,
      "learning_rate": 3.1104607141947787e-05,
      "loss": 1.5424,
      "step": 282000
    },
    {
      "epoch": 1.1341294880441108,
      "grad_norm": 4.101245880126953,
      "learning_rate": 3.109790662426579e-05,
      "loss": 1.5626,
      "step": 282100
    },
    {
      "epoch": 1.1345315187009586,
      "grad_norm": 3.3385567665100098,
      "learning_rate": 3.10912061065838e-05,
      "loss": 1.5846,
      "step": 282200
    },
    {
      "epoch": 1.1349335493578063,
      "grad_norm": 3.5219757556915283,
      "learning_rate": 3.10845055889018e-05,
      "loss": 1.5988,
      "step": 282300
    },
    {
      "epoch": 1.135335580014654,
      "grad_norm": 3.222170114517212,
      "learning_rate": 3.107780507121981e-05,
      "loss": 1.5834,
      "step": 282400
    },
    {
      "epoch": 1.1357376106715018,
      "grad_norm": 4.0681657791137695,
      "learning_rate": 3.1071104553537805e-05,
      "loss": 1.5799,
      "step": 282500
    },
    {
      "epoch": 1.1361396413283495,
      "grad_norm": 4.22752046585083,
      "learning_rate": 3.106440403585581e-05,
      "loss": 1.5744,
      "step": 282600
    },
    {
      "epoch": 1.1365416719851973,
      "grad_norm": 3.2676658630371094,
      "learning_rate": 3.1057703518173816e-05,
      "loss": 1.5864,
      "step": 282700
    },
    {
      "epoch": 1.136943702642045,
      "grad_norm": 3.489248037338257,
      "learning_rate": 3.105100300049182e-05,
      "loss": 1.5879,
      "step": 282800
    },
    {
      "epoch": 1.1373457332988928,
      "grad_norm": 3.2291176319122314,
      "learning_rate": 3.1044302482809826e-05,
      "loss": 1.5527,
      "step": 282900
    },
    {
      "epoch": 1.1377477639557405,
      "grad_norm": 3.099076509475708,
      "learning_rate": 3.1037601965127824e-05,
      "loss": 1.597,
      "step": 283000
    },
    {
      "epoch": 1.1381497946125883,
      "grad_norm": 3.4771971702575684,
      "learning_rate": 3.103090144744583e-05,
      "loss": 1.5773,
      "step": 283100
    },
    {
      "epoch": 1.138551825269436,
      "grad_norm": 4.1037774085998535,
      "learning_rate": 3.1024200929763834e-05,
      "loss": 1.5334,
      "step": 283200
    },
    {
      "epoch": 1.1389538559262837,
      "grad_norm": 3.2921879291534424,
      "learning_rate": 3.101750041208184e-05,
      "loss": 1.6057,
      "step": 283300
    },
    {
      "epoch": 1.1393558865831315,
      "grad_norm": 3.343235969543457,
      "learning_rate": 3.1010799894399845e-05,
      "loss": 1.5709,
      "step": 283400
    },
    {
      "epoch": 1.1397579172399792,
      "grad_norm": 3.4452648162841797,
      "learning_rate": 3.100409937671785e-05,
      "loss": 1.5578,
      "step": 283500
    },
    {
      "epoch": 1.140159947896827,
      "grad_norm": 3.139463424682617,
      "learning_rate": 3.0997398859035855e-05,
      "loss": 1.5553,
      "step": 283600
    },
    {
      "epoch": 1.1405619785536747,
      "grad_norm": 3.099040985107422,
      "learning_rate": 3.099069834135385e-05,
      "loss": 1.5637,
      "step": 283700
    },
    {
      "epoch": 1.1409640092105224,
      "grad_norm": 3.183300733566284,
      "learning_rate": 3.098399782367186e-05,
      "loss": 1.5601,
      "step": 283800
    },
    {
      "epoch": 1.1413660398673702,
      "grad_norm": 3.624850273132324,
      "learning_rate": 3.0977297305989864e-05,
      "loss": 1.6122,
      "step": 283900
    },
    {
      "epoch": 1.141768070524218,
      "grad_norm": 3.27500581741333,
      "learning_rate": 3.097059678830786e-05,
      "loss": 1.6044,
      "step": 284000
    },
    {
      "epoch": 1.1421701011810657,
      "grad_norm": 3.1220016479492188,
      "learning_rate": 3.096389627062587e-05,
      "loss": 1.5346,
      "step": 284100
    },
    {
      "epoch": 1.1425721318379134,
      "grad_norm": 3.3659825325012207,
      "learning_rate": 3.095719575294387e-05,
      "loss": 1.5601,
      "step": 284200
    },
    {
      "epoch": 1.142974162494761,
      "grad_norm": 3.665717840194702,
      "learning_rate": 3.095049523526188e-05,
      "loss": 1.5639,
      "step": 284300
    },
    {
      "epoch": 1.1433761931516089,
      "grad_norm": 3.2161688804626465,
      "learning_rate": 3.094379471757988e-05,
      "loss": 1.5839,
      "step": 284400
    },
    {
      "epoch": 1.1437782238084564,
      "grad_norm": 3.7725324630737305,
      "learning_rate": 3.093709419989789e-05,
      "loss": 1.5479,
      "step": 284500
    },
    {
      "epoch": 1.1441802544653044,
      "grad_norm": 3.098834276199341,
      "learning_rate": 3.093039368221589e-05,
      "loss": 1.5778,
      "step": 284600
    },
    {
      "epoch": 1.1445822851221519,
      "grad_norm": 3.3625478744506836,
      "learning_rate": 3.09236931645339e-05,
      "loss": 1.5498,
      "step": 284700
    },
    {
      "epoch": 1.1449843157789998,
      "grad_norm": 3.858837366104126,
      "learning_rate": 3.09169926468519e-05,
      "loss": 1.5816,
      "step": 284800
    },
    {
      "epoch": 1.1453863464358474,
      "grad_norm": 3.283822774887085,
      "learning_rate": 3.09102921291699e-05,
      "loss": 1.6028,
      "step": 284900
    },
    {
      "epoch": 1.1457883770926953,
      "grad_norm": 3.13655424118042,
      "learning_rate": 3.0903591611487906e-05,
      "loss": 1.5654,
      "step": 285000
    }
  ],
  "logging_steps": 100,
  "max_steps": 746211,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 5000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.1756311084597248e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
